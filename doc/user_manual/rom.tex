\newcommand{\zNormalizationPerformed}[1]
{
  \textcolor{red}{\\It is important to NOTE that RAVEN uses a Z-score normalization of the training data before
  constructing the \textit{#1} ROM:
\begin{equation}
  \mathit{\mathbf{X'}} = \frac{(\mathit{\mathbf{X}}-\mu )}{\sigma }
\end{equation}
 }
}

\newcommand{\zNormalizationNotPerformed}[1]
{
  \textcolor{red}{
  \\It is important to NOTE that RAVEN does not pre-normalize the training data before
  constructing the \textit{#1} ROM.}
}

\subsection{ROM}
\label{subsec:models_ROM}
A Reduced Order Model (ROM) is a mathematical model consisting of a fast
solution trained to predict a response of interest of a physical system.
%
The ``training'' process is performed by sampling the response of a physical
model with respect to variations of its parameters subject, for example, to
probabilistic behavior.
%
The results (outcomes of the physical model) of the sampling are fed into the
algorithm representing the ROM that tunes itself to replicate those results.
%
RAVEN supports several different types of ROMs, both internally developed and
imported through an external library called ``scikit-learn''~\cite{SciKitLearn}.

Currently in RAVEN, the ROMs are classified into several sub-types that, once chosen,
provide access to several different algorithms.
%
These sub-types are specified in the \xmlAttr{subType} attribute and should be
one of the following:
\begin{itemize}
  \item \xmlString{GaussPolynomialRom}, for both static and time-dependent regression
  \item \xmlString{HDMRRom}, for both static and time-dependent regression
  \item \xmlString{NDinvDistWeight}, for both static and time-dependent regression
  \item \xmlString{NDSpline}, for both static and time-dependent regression
  \item \xmlString{SciKitLearn}, for both static and time-dependent regression and classification
  \item \xmlString{MSR}, for both static and time-dependent regression
  \item \xmlString{ARMA}, for time-dependent stochastic regression (time series generator)
  \item \xmlString{PolyExponential}, for time-dependent regression
  \item \xmlString{DMD}, for time-dependent regression
\end{itemize}

\specBlock{a}{ROM}
%
\attrsIntro
%
\vspace{-5mm}
\begin{itemize}
  \itemsep0em
  \item \nameDescription
  \item \xmlAttr{subType}, \xmlDesc{required string attribute}, defines which of
  the sub-types should be used, choosing among the previously reported
  types.
  %
  This choice conditions the subsequent the required and/or optional
  \xmlNode{ROM} sub-nodes.
  %
\end{itemize}
\vspace{-5mm}

In the \xmlNode{ROM} input block, the following XML sub-nodes are required,
independent of the \xmlAttr{subType} specified:
%
\begin{itemize}
%  \item \xmlNode{Features}, \xmlDesc{comma separated string, required field},
%  specifies the names of the features of this ROM.
%  %
%  \nb These parameters will be requested for the training of this object (see
%  Section~\ref{subsec:stepRomTrainer}).
%  \item \xmlNode{Target}, \xmlDesc{comma separated string, required field},
%  contains a comma separated list of the targets of this ROM.
%  %
%  The parameters listed here represent the figures of merit this ROM is supposed
%  to predict.
%  %
%  \nb These parameters are going to be requested for the training of this object
%  (see Section~\ref{subsec:stepRomTrainer}).
  %
   \item \xmlNode{Features}, \xmlDesc{comma separated string, required field}, specifies the names of the features of this ROM.
   \nb These parameters are going to be requested for the training of this object (see Section~\ref{subsec:stepRomTrainer});
    \item \xmlNode{Target}, \xmlDesc{comma separated string, required field}, contains a comma separated list of the targets of this ROM. These parameters are the Figures of Merit (FOMs) this ROM is supposed to predict.
    \nb These parameters are going to be requested for the training of this
    object (see Section \ref{subsec:stepRomTrainer}).

\end{itemize}

In addition, if the user wants to use the alias system, the following XML block can be inputted:
\begin{itemize}
  \item \aliasSystemDescription{ROM}
\end{itemize}


The types and meaning of the remaining sub-nodes depend on the sub-type
specified in the attribute \xmlAttr{subType}.

%
Note that if an HistorySet is provided in the training step then a temporal ROM is created, i.e. a ROM that generates not a single value prediction of each element indicated in the  \xmlNode{Target} block but its full temporal profile.
\\
\textcolor{red}{\\\textbf{It is important to NOTE that RAVEN uses a Z-score normalization of the training data before constructing most of the
Reduced Order Models (e.g. most of the SciKitLearn-based ROMs):}}
\begin{equation}
  \mathit{\mathbf{X'}} = \frac{(\mathit{\mathbf{X}}-\mu )}{\sigma }
\end{equation}
\\In the following sections the specifications of each ROM type are reported, highlighting when a \textbf{Z-score normalization} is performed by RAVEN before constructing the ROM or when it is not performed.

%
%%%%% ROM Model - NDspline  %%%%%%%
\subsubsection{NDspline}
\label{subsubsec:NDspline}
The NDspline sub-type contains a single ROM type, based on an $N$-dimensional
spline interpolation/extrapolation scheme.
%
In spline interpolation, the regressor is a special type of piece-wise
polynomial called tensor spline.
%
The interpolation error can be made small even when using low degree polynomials
for the spline.
%
Spline interpolation avoids the problem of Runge's phenomenon, in which
oscillation can occur between points when interpolating using higher degree
polynomials.
%

In order to use this ROM, the \xmlNode{ROM} attribute \xmlAttr{subType} needs to
be \xmlString{NDspline} (see the example below).
%
No further XML sub-nodes are required.
%
\nb This ROM type must be trained from a regular Cartesian grid.
%
Thus, it can only be trained from the outcomes of a grid sampling strategy.

\zNormalizationPerformed{NDspline}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
    <ROM name='aUserDefinedName' subType='NDspline'>
       <Features>var1,var2,var3</Features>
       <Target>result1,result2</Target>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}

%%%%% ROM Model - GaussPolynomialRom  %%%%%%%
\subsubsection{pickledROM}
\label{subsubsec:pickledROM}
It is not uncommon for a reduced-order model (ROM) to be created and trained in one RAVEN run, then
serialized to file (\emph{pickled}), then loaded into another RAVEN run to be used as a model.  When this is
the case, a \xmlNode{ROM} with subtype \xmlString{pickledROM} is used to hold the place of the ROM that will
be loaded from file.  The notation for this ROM is much less than a typical ROM; it only requires a name and
its subtype.

Note that when loading ROMs from file, RAVEN will not perform any checks on the expected inputs or outputs of
a ROM; it is expected that a user know at least the I/O of a ROM before trying to use it as a model.
However, RAVEN does require that pickled ROMs be trained before pickling in the first place.

Initially, a pickledROM is not usable.  It cannot be trained or sampled; attempting to do so will raise an
error.  An \xmlNode{IOStep} is used to load the ROM from file, at which point the ROM will have all the same
characteristics as when it was pickled in a previous RAVEN run.

\textbf{Example:}
For this example the ROM has already been created and trained in another RAVEN run, then pickled to a file
called \texttt{rom\_pickle.pk}.  In the example, the file is identified in \xmlNode{Files}, the model is
defined in \xmlNode{Models}, and the model loaded in \xmlNode{Steps}.
{\footnotesize
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Files>
    <Input name="rompk" type="">rom_pickle.pk</Input>
  </Files>
  ...
  <Models>
    ...
    <ROM name="myRom" subType="pickledROM"/>
    ...
  </Models>
  ...
  <Steps>
    ...
    <IOStep name="loadROM">
      <Input class="Files" type="">rompk</Input>
      <Output class="Models" type="ROM">myRom</Output>
    </IOStep>
    ...
  </Steps>
  ...
</Simulation>
\end{lstlisting}
}


\subsubsection{GaussPolynomialRom}
\label{subsubsec:GaussPolynomialRom}
The GaussPolynomialRom sub-type contains a single ROM type, based on a
characteristic Gaussian polynomial fitting scheme: generalized polynomial chaos
expansion (gPC).
%
In gPC, sets of polynomials orthogonal with respect to the distribution of uncertainty
are used to represent the original model.  The method converges moments of the original
model faster than Monte Carlo for small-dimension uncertainty spaces ($N<15$).
%
In order to use this ROM, the \xmlNode{ROM} attribute \xmlAttr{subType} needs to
be \xmlString{GaussPolynomialRom} (see the example below).
%
The GaussPolynomialRom is dependent on specific sampling; thus, this ROM cannot be trained unless a
SparseGridCollocation or similar Sampler specifies this ROM in its input and is sampled in a MultiRun step.
%
In addition to the common \xmlNode{Target} and \xmlNode{Features}, this ROM requires
two more nodes and can accept multiple entries of a third optional node.
\begin{itemize}
  \item \xmlNode{IndexSet}, \xmlDesc{string, required field},
  specifies the rules by which to construct multidimensional polynomials.  The options are
  \xmlString{TensorProduct}, \xmlString{TotalDegree},\\
  \xmlString{HyperbolicCross}, and \xmlString{Custom}.
  %
  Total degree is efficient for
  uncertain inputs with a large degree of regularity, while hyperbolic cross is more efficient
  for low-regularity input spaces.
  %
  If \xmlString{Custom} is chosen, the \xmlNode{IndexPoints} is required.
  %
  \item \xmlNode{PolynomialOrder}, \xmlDesc{integer, required field},
  indicates the maximum polynomial order in any one dimension to use in the
  polynomial chaos expansion. \nb If non-equal importance weights are supplied in the optional
  \xmlNode{Interpolation} node, the actual polynomial order in dimensions with high
  importance might exceed this value; however, this value is still used to limit the
  relative overall order.
  %
  \item \xmlNode{SparseGrid},\xmlDesc{string, optional field}, allows specification of the multidimensional
    quadrature construction strategy.  Options are \xmlString{smolyak} and \xmlString{tensor}.  Default is
    \xmlString{smolyak}.
  \item \xmlNode{IndexPoints}, \xmlDesc{list of tuples, required field},
  used to specify the index set points in a \xmlString{Custom} index set.  The tuples are
  entered as comma-seprated values between parenthesis, with each tuple separated by a comma.
  Any amount of whitespace is acceptable.  For example, \xmlNode{IndexPoints}\verb'(0,1),(0,2),(1,1),(4,0)'\xmlNode{/IndexPoints}
  \nb{Using custom index sets
  does not guarantee accurate convergence.}
  %
  \item \xmlNode{Interpolation}, \xmlDesc{string, optional field},
  offers the option to specify quadrature, polynomials, and importance weights for the given
  variable name.  The ROM accepts any number of \xmlNode{Interpolation} nodes up to the
  dimensionality of the input space.  This node accepts several attributes, all of which are
  optional and default to
  the code-defined optimal choices based on the input dimension uncertainty distribution:
  \begin{itemize}
    \item \xmlAttr{quad}, \xmlDesc{string, optional field},
      specifies the quadrature type to use for collocation in this dimension.  The default options
      depend on the uncertainty distribution of the input dimension, as shown in Table
      \ref{tab:gpcCompatible}. Additionally, Clenshaw Curtis quadrature can be used for any
      distribution that doesn't include an infinite bound.
      \default{see Table \ref{tab:gpcCompatible}.}
      \nb For an uncertain distribution aside from the four listed on Table
      \ref{tab:gpcCompatible}, this ROM
      makes use of the uniform-like range of the distribution's CDF to apply quadrature that is
      suited uniform uncertainty (Legendre).  It converges more slowly than the four listed, but are
      viable choices.  Choosing polynomial type Legendre for any non-uniform distribution will
      enable this formulation automatically.
    \item \xmlAttr{poly}, \xmlDesc{string,optional field},
      specifies the interpolating polynomial family to use for the polynomial expansion in this
      dimension.  The default options depend on the quadrature type chosen, as shown in Table
      \ref{tab:gpcCompatible}.  Currently, no polynomials are available outside the
      default. \default{see Table \ref{tab:gpcCompatible}.}
    \item  \xmlAttr{weight}, \xmlDesc{float, optional field},
      delineates the importance weighting of this dimension.  A larger importance weight will
      result in increased resolution for this dimension at the cost of resolution in lower-weighted
      dimensions.  The algorithm normalizes weights at run-time.\default{1}.
  \end{itemize}
  %
\end{itemize}
\begin{table}[htb]
  \centering
  \begin{tabular}{c | c c}
    Unc. Distribution & Default Quadrature & Default Polynomials \\ \hline
    Uniform & Legendre & Legendre \\
    Normal & Hermite & Hermite \\ \hline
    Gamma & Laguerre & Laguerre \\
    Beta & Jacobi & Jacobi \\ \hline
    Other & Legendre* & Legendre*
  \end{tabular}
  \caption{GaussPolynomialRom defaults}
  \label{tab:gpcCompatible}
\end{table}
%
\nb This ROM type must be trained from a collocation quadrature set.
%
Thus, it can only be trained from the outcomes of a SparseGridCollocation sampler.
Also, this ROM must be referenced in the SparseGridCollocation sampler in order to
accurately produce the necessary sparse grid points to train this ROM.

\zNormalizationNotPerformed{GaussPolynomialRom}

\textbf{Example:}
{\footnotesize
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Samplers>
    ...
    <SparseGridCollocation name="mySG" parallel="0">
      <variable name="x1">
        <distribution>myDist1</distribution>
      </variable>
      <variable name="x2">
        <distribution>myDist2</distribution>
      </variable>
      <ROM class = 'Models' type = 'ROM' >myROM</ROM>
    </SparseGridCollocation>
    ...
  </Samplers>
  ...
  <Models>
    ...
    <ROM name='myRom' subType='GaussPolynomialRom'>
      <Target>ans</Target>
      <Features>x1,x2</Features>
      <IndexSet>TotalDegree</IndexSet>
      <PolynomialOrder>4</PolynomialOrder>
      <Interpolation quad='Legendre' poly='Legendre' weight='1'>x1</Interpolation>
      <Interpolation quad='ClenshawCurtis' poly='Jacobi' weight='2'>x2</Interpolation>
    </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
}

When Printing this ROM via a Print OutStream (see \ref{sec:printing}), the available metrics are:
\begin{itemize}
  \item \xmlString{mean}, the mean value of the ROM output within the input space it was trained,
  \item \xmlString{variance}, the variance of the ROM output within the input space it was trained,
  \item \xmlString{samples}, the number of distinct model runs required to construct the ROM,
  \item \xmlString{indices}, the Sobol sensitivity indices (in percent), Sobol total indices, and partial variances,
  \item \xmlString{polyCoeffs}, the polynomial expansion coefficients (PCE moments) of the ROM.  These are
    listed by each polynomial combination, with the polynomial order tags listed in the order of the variables
    shown in the XML print.
\end{itemize}

%%%%% ROM Model - HDMRRom  %%%%%%%
\subsubsection{HDMRRom}
\label{subsubsec:HDMRRom}
The HDMRRom sub-type contains a single ROM type, based on a Sobol decomposition scheme.
%
In Sobol decomposition, also known as high-density model reduction (HDMR, specifically Cut-HDMR),
a model is approximated as as the sum of increasing-complexity interactions.  At its lowest level (order 1), it treats the function as a sum of the reference case plus a functional of each input dimesion separately.  At order 2, it adds functionals to consider the pairing of each dimension with each other dimension.  The benefit to this approach is considering several functions of small input cardinality instead of a single function with large input cardinality.  This allows reduced order models like generalized polynomial chaos (see \ref{subsubsec:GaussPolynomialRom}) to approximate the functionals accurately with few computations runs.
%
In order to use this ROM, the \xmlNode{ROM} attribute \xmlAttr{subType} needs to
be \xmlString{HDMRRom} (see the example below).
%
The HDMRRom is dependent on specific sampling; thus, this ROM cannot be trained unless a
Sobol or similar Sampler specifies this ROM in its input and is sampled in a MultiRun step.
%
In addition to the common \xmlNode{Target} and \xmlNode{Features}, this ROM requires
the same nodes as the GaussPolynomialRom (see \ref{subsubsec:GaussPolynomialRom}.
Additionally, this ROM requires the \xmlNode{SobolOrder} node.
\begin{itemize}
  \item \xmlNode{SobolOrder}, \xmlDesc{integer, required field},
  indicates the maximum cardinality of the input space used in the subset functionals.  For example, order 1
  includes only functionals of each independent dimension separately, while order 2 considers pair-wise
  interactions.
  %
\end{itemize}
\nb This ROM type must be trained from a Sobol decomposition training set.
%
Thus, it can only be trained from the outcomes of a Sobol sampler.
Also, this ROM must be referenced in the Sobol sampler in order to
accurately produce the necessary sparse grid points to train this ROM.
Experience has shown order 2 Sobol decompositions to include the great majority of
  uncertainty in most models.

\zNormalizationNotPerformed{HDMRRom}

\textbf{Example:}
{\footnotesize
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
  <Samplers>
    ...
    <Sobol name="mySobol" parallel="0">
      <variable name="x1">
        <distribution>myDist1</distribution>
      </variable>
      <variable name="x2">
        <distribution>myDist2</distribution>
      </variable>
      <ROM class = 'Models' type = 'ROM' >myHDMR</ROM>
    </Sobol>
    ...
  </Samplers>
  ...
  <Models>
    ...
    <ROM name='myHDMR' subType='HDMRRom'>
      <Target>ans</Target>
      <Features>x1,x2</Features>
      <SobolOrder>2</SobolOrder>
      <IndexSet>TotalDegree</IndexSet>
      <PolynomialOrder>4</PolynomialOrder>
      <Interpolation quad='Legendre' poly='Legendre' weight='1'>x1</Interpolation>
      <Interpolation quad='ClenshawCurtis' poly='Jacobi' weight='2'>x2</Interpolation>
    </ROM>
    ...
  </Models>
\end{lstlisting}
}

When Printing this ROM via an OutStream (see \ref{sec:printing}), the available metrics are:
\begin{itemize}
  \item \xmlString{mean}, the mean value of the ROM output within the input space it was trained,
  \item \xmlString{variance}, the ANOVA-calculated variance of the ROM output within the input space it
    was trained.
  \item \xmlString{samples}, the number of distinct model runs required to construct the ROM,
  \item \xmlString{indices}, the Sobol sensitivity indices (in percent), Sobol total indices, and partial variances.
\end{itemize}

%%%%% ROM Model - MSR  %%%%%%%
\subsubsection{MSR}
\label{subsubsec:MSR}
The MSR sub-type contains a class of ROMs that perform a topological
decomposition of the data into approximately monotonic regions and fits weighted
linear patches to the identified monotonic regions of the input space. Query
points have estimated probabilities that they belong to each cluster. These
probabilities can eitehr be used to give a smooth, weighted prediction based on
the associated linear models, or a hard classification to a particular local
linear model which is then used for prediction. Currently, the probability
prediction can be done using kernel density estimation (KDE) or through a
one-versus-one support vector machine (SVM).
%

In order to use this ROM, the \xmlNode{ROM} attribute \xmlAttr{subType} needs to
be \xmlString{MSR} (see the associated example).
%
\subnodesIntro
%
\begin{itemize}
  \item \xmlNode{persistence}, \xmlDesc{string, optional field}, specifies how
  to define the hierarchical simplification by assigning a value to each local
  minimum and maximum according to the one of the strategy options below:
  \begin{itemize}
    \item \texttt{difference} - The function value difference between the
    extremum and its closest-valued neighboring saddle.
    \item \texttt{probability} - The probability integral computed as the
    sum of the probability of each point in a cluster divided by the count of
    the cluster.
    \item \texttt{count} - The count of points that flow to or from the
    extremum.
    % \item \xmlString{area} - The area enclosed by the manifold that flows to
    % or from the extremum.
  \end{itemize}
  \default{\texttt{difference}}
  \item \xmlNode{gradient}, \xmlDesc{string, optional field}, specifies the
  method used for estimating the gradient, available options are:
  \begin{itemize}
    \item \texttt{steepest}
    %\item \texttt{maxflow} \textit{(disabled)}
  \end{itemize}
  \default{\texttt{steepest}}
  \item \xmlNode{simplification}, \xmlDesc{float, optional field}, specifies the
  amount of noise reduction to apply before returning labels.
  \default{0}
  \item \xmlNode{graph} \xmlDesc{, string, optional field}, specifies the type
  of neighborhood graph used in the algorithm, available options are:
  \begin{itemize}
    \item \texttt{beta skeleton}
    \item \texttt{relaxed beta skeleton}
    \item \texttt{approximate knn}
    %\item \texttt{delaunay} \textit{(disabled)}
  \end{itemize}
  \default{\texttt{beta skeleton}}
  \item \xmlNode{beta}, \xmlDesc{float in the range: (0,2], optional field}, is
  only used when the \xmlNode{graph} is set to \texttt{beta skeleton} or
  \texttt{relaxed beta skeleton}.
  \default{1.0}
  \item \xmlNode{knn}, \xmlDesc{integer, optional field}, is the number of
  neighbors when using the \texttt{approximate knn} for the \xmlNode{graph}
  sub-node and used to speed up the computation of other graphs by using the
  approximate knn graph as a starting point for pruning. -1 means use a fully
  connected graph.
  \default{-1}
  % \item \xmlNode{weighted}, \xmlDesc{boolean, optional}, a flag that specifies
  % whether the regression models should be probability weighted.
  % \default{False}
  \item \xmlNode{partitionPredictor}, \xmlDesc{string, optional}, a flag that
  specifies how the predictions for query point classification should be
  performed. Available options are:
  \begin{itemize}
    \item \texttt{kde}
    \item \texttt{svm}
  \end{itemize}
  \default{kde}
  \item \xmlNode{smooth}, if this node is present, the ROM will blend the
  estimates of all of the local linear models weighted by the probability the
  query point is classified as belonging to that partition of the input space.
  \item \xmlNode{kernel}, \xmlDesc{string, optional field}, this option is only
  used when the \xmlNode{partitionPredictor} is set to \texttt{kde} and
  specifies the type of kernel to use in the kernel density estimation.
  Available options are:
  \begin{itemize}
    \item \texttt{uniform}
    \item \texttt{triangular}
    \item \texttt{gaussian}
    \item \texttt{epanechnikov}
    \item \texttt{biweight} or \texttt{quartic}
    \item \texttt{triweight}
    \item \texttt{tricube}
    \item \texttt{cosine}
    \item \texttt{logistic}
    \item \texttt{silverman}
    \item \texttt{exponential}
  \end{itemize}
  \default{gaussian}
  \item \xmlNode{bandwidth}, \xmlDesc{float or string, optional field}, this
  option is only used when the \xmlNode{partitionPredictor} is set to
  \texttt{kde} and specifies the scale of the fall-off. A higher bandwidth
  implies a smooother blending. If set to \texttt{variable}, then the bandwidth
  will be set to the distance of the $k$-nearest neighbor of the query point
  where $k$ is set by the \xmlNode{knn} parameter.
  \default{1.}
\end{itemize}

\zNormalizationNotPerformed{MSR}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
    </ROM>
    <ROM name='aUserDefinedName' subType='MSR'>
       <Features>var1,var2,var3</Features>
       <Target>result1,result2</Target>
       <!-- <weighted>true</weighted> -->
       <simplification>0.0</simplification>
       <persistence>difference</persistence>
       <gradient>steepest</gradient>
       <graph>beta skeleton</graph>
       <beta>1</beta>
       <knn>8</knn>
       <partitionPredictor>kde</partitionPredictor>
       <kernel>gaussian</kernel>
       <smooth/>
       <bandwidth>0.2</bandwidth>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}

%%%%% ROM Model - NDinvDistWeight  %%%%%%%
\subsubsection{NDinvDistWeight}
\label{subsubsec:NDinvDistWeight}
The NDinvDistWeight sub-type contains a single ROM type, based on an
$N$-dimensional inverse distance weighting formulation.
%
Inverse distance weighting (IDW) is a type of deterministic method for
multivariate interpolation with a known scattered set of points.
%
The assigned values to unknown points are calculated via a weighted average of
the values available at the known points.
%

In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be xmlString{NDinvDistWeight} (see the example
below).
%
\subnodeIntro

\begin{itemize}
  \item \xmlNode{p}, \xmlDesc{integer, required field}, must be greater than
  zero and represents the ``power parameter''.
  %
  For the choice of value for \xmlNode{p},it is necessary to consider the degree
  of smoothing desired in the interpolation/extrapolation, the density and
  distribution of samples being interpolated, and the maximum distance over
  which an individual sample is allowed to influence the surrounding ones (lower
  $p$ means greater importance for points far away).
  %
\end{itemize}

\zNormalizationPerformed{NDinvDistWeight}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
    <ROM name='aUserDefinedName' subType='NDinvDistWeight'>
      <Features>var1,var2,var3</Features>
      <Target>result1,result2</Target>
      <p>3</p>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}


%%%%% ROM Model - SciKitLearn  %%%%%%%
\subsubsection{SciKitLearn}
\label{subsubsec:SciKitLearn}
The SciKitLearn sub-type represents the container of several ROMs available in
RAVEN through the external library scikit-learn~\cite{SciKitLearn}.
%

In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be \\ \xmlString{SciKitLearn} (i.e.
\xmlAttr{subType}\textbf{\texttt{=}}\xmlString{SciKitLearn}).
%
The specifications of a \xmlString{SciKitLearn} ROM depend on the value assumed
by the following sub-node within the main \xmlNode{ROM} XML node:
\begin{itemize}
  \item \xmlNode{SKLtype}, \xmlDesc{vertical bar (\texttt{|}) separated string,
  required field}, contains a string that represents the ROM type to be used.
  %
  As mentioned, its format is:\\
  \xmlNode{SKLtype}\texttt{mainSKLclass|algorithm}\xmlNode{/SKLtype} where the
  first word (before the ``\texttt{|}'' symbol) represents the main class of
  algorithms, and the second word (after the ``\texttt{|}'' symbol) represents
  the specific algorithm.
  %
\end{itemize}
Based on the \xmlNode{SKLtype} several different algorithms are available.
%
In the following paragraphs a brief explanation and the input requirements are
reported for each of them.
%
%%%%% ROM Model - SciKitLearn: Linear Models %%%%%%%
\paragraph{Linear Models}
\label{LinearModels}
The LinearModels' algorithms implement generalized linear models.
%
They include Ridge regression, Bayesian regression, lasso, and elastic net
estimators computed with least angle regression and coordinate descent.
%
This class also implements stochastic gradient descent related algorithms.
%
In the following, all of the linear models available in RAVEN are reported.
%
\subparagraph{Linear Model: Automatic Relevance Determination Regression}
\mbox{}
\\The \textit{Automatic Relevance Determination} (ARD) regressor is a
hierarchical Bayesian approach where hyperparameters explicitly represent the
relevance of different input features.
%
These relevance hyperparameters determine the range of variation for the
parameters relating to a particular input, usually by modelling the width of a
zero-mean Gaussian prior on those parameters.
%
If the width of the Gaussian is zero, then those parameters are constrained to
be zero, and the corresponding input cannot have any effect on the predictions,
therefore making it irrelevant.
%
ARD optimizes these hyperparameters to discover which inputs are relevant.
%
\skltype{Automatic Relevance Determination regressor}{linear\_model|ARDRegression}.
\begin{itemize}
  \item \nIterDescriptionA{300}
  \item \tolDescriptionA{1.e-3}
  \item \xmlNode{alpha\_1}, \xmlDesc{float, optional field}, is a shape
  hyperparameter for the Gamma distribution prior over the $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{alpha\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_1}, \xmlDesc{float, optional field}, shape
  hyperparameter for the Gamma distribution prior over the $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{compute\_score}, \xmlDesc{boolean, optional field}, if True,
  compute the objective function at each step of the model.
  \default{False}
  %
  \item \xmlNode{threshold\_lambda}, \xmlDesc{float, optional field}, specifies
  the threshold for removing (pruning) weights with high precision from the
  computation.
  \default{ 1.e+4}
  %
  \item \fitInterceptDescription{True}
  %
  \item \normalizeDescription{False}
  %
  \item \verDescriptionA{False}
\end{itemize}

\zNormalizationNotPerformed{ARDRegression}
%%%%%%%%
\subparagraph{Linear Model: Bayesian ridge regression}
\mbox{}

The \textit{Bayesian ridge regression} estimates a probabilistic model of the
regression problem as described above.
%
The prior for the parameter $w$ is given by a spherical Gaussian:
\begin{equation}
p(w|\lambda) =\mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})
\end{equation}
The priors over $\alpha$ and $\lambda$ are chosen to be gamma distributions, the
conjugate prior for the precision of the Gaussian.
%
The resulting model is called Bayesian ridge regression, and is similar to the
classical ridge regression.
%
The parameters $w$, $\alpha$, and $\lambda$ are estimated jointly during the fit
of the model.
%
The remaining hyperparameters are the parameters of the gamma priors over
$\alpha$ and $\lambda$.
%
These are usually chosen to be non-informative.
%
The parameters are estimated by maximizing the marginal log likelihood.
%
\skltype{Bayesian ridge regressor}{linear\_model|BayesianRidge}
\begin{itemize}
  \item \nIterDescriptionA{300}
  \item \tolDescriptionA{1.e-3}
  \item \xmlNode{alpha\_1}, \xmlDesc{float, optional field}, is a shape
  hyperparameter for the Gamma distribution prior over the $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{alpha\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_1}, \xmlDesc{float, optional field}, shape
  hyperparameter for the Gamma distribution prior over the $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{compute\_score}, \xmlDesc{boolean, optional field}, if True,
  compute the objective function at each step of the model.
  \default{False}
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \verDescriptionA{False}
\end{itemize}

\zNormalizationNotPerformed{BayesianRidge}
%%%%%%%
\subparagraph{Linear Model: Elastic Net}
\mbox{}
\\The \textit{Elastic Net} is a linear regression technique with combined L1 and
L2 priors as regularizers.
%
It minimizes the objective function:
\begin{equation}
1/(2*n_{samples}) *||y - Xw||^2_2+alpha*l1\_ratio*||w||_1 + 0.5 *alpha*(1 - l1\_ratio)*||w||^2_2
\end{equation}

\skltype{Elastic Net regressor}{linear\_model|ElasticNet}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, specifies a constant
  that multiplies the penalty terms.
  %
  $alpha = 0$ is equivalent to an ordinary least square, solved by the
  \textbf{LinearRegression} object.
  \default{1.0}
  %
  \item \xmlNode{l1\_ratio}, \xmlDesc{float, optional field}, specifies the
  ElasticNet mixing parameter, with $0 <= l1\_ratio <= 1$.
  %
  For $l1\_ratio = 0$ the penalty is an L2 penalty.
  %
  For $l1\_ratio = 1$ it is an L1 penalty.
  %
  For $0 < l1\_ratio < 1$, the penalty is a combination of L1 and L2.
  %
  \default{0.5}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{1000}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{False}
  \item \positiveDescription{False}
  %
\end{itemize}
\zNormalizationNotPerformed{ElasticNet}
%%%%%%%%
\subparagraph{Linear Model: Elastic Net CV}
\mbox{}
\\The \textit{Elastic Net CV} is a linear regression similar to the Elastic Net
model but with an iterative fitting along a regularization path.
%
The best model is selected by cross-validation.
%

\skltype{Elastic Net CV regressor}{linear\_model|ElasticNetCV}
\begin{itemize}
  \item \xmlNode{l1\_ratio}, \xmlDesc{float, optional field},
  %
  Float flag between 0 and 1 passed to ElasticNet (scaling between l1 and l2
  penalties).
  %
  For $l1\_ratio = 0$ the penalty is an L2 penalty.
  %
  For $l1\_ratio = 1$ it is an L1 penalty.
  %
  For $0 < l1\_ratio < 1$, the penalty is a combination of L1 and L2 This
  parameter can be a list, in which case the different values are tested by
  cross-validation and the one giving the best prediction score is used.
  %
  Note that a good choice of list of values for $l1\_ratio$ is often to put more
  values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1,
  .5, .7, .9, .95, .99, 1].
  %
  \default{0.5}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, specifies the length of
  the path.
  %
  eps=1e-3 means that $alpha\_min / alpha\_max = 1e-3$.
  %
  \default{0.001}
  \item \xmlNode{n\_alphas}, \xmlDesc{integer, optional field}, is the number of
  alphas along the regularization path used for each $l1\_ratio$.
  %
  \default{100}
  \item \precomputeDescription{'auto'}
  \item \maxIterDescription{1000}
  \item \tolDescriptionB{1.e-4}
  %
  \item \positiveDescription{False}
  %
\end{itemize}
\zNormalizationNotPerformed{ElasticNetCV}
%%%%%%
\subparagraph{Linear Model: Least Angle Regression model}
\mbox{}
\\The \textit{Least Angle Regression model} (LARS) is a regression algorithm for
high-dimensional data.
%
The LARS algorithm provides a means of producing an estimate of which variables
to include, as well as their coefficients, when a response variable is
determined by a linear combination of a subset of potential covariates.
%

\skltype{Least Angle Regression model}{linear\_model|Lars}
\begin{itemize}
  \item \xmlNode{n\_nonzero\_coefs}, \xmlDesc{integer, optional field},
  represents the target number of non-zero coefficients.
  %
  \default{500}
  \item \fitInterceptDescription{True}
  \item \verDescriptionA{False}
  \item \precomputeDescription{'auto'}
  \item \normalizeDescription{True}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the \xmlNode{tol} parameter in some iterative optimization-based
  algorithms, this parameter does not control the tolerance of the optimization.
  %
  \default{2.2204460492503131e-16}
  \item \xmlNode{fit\_path}, \xmlDesc{boolean, optional field}, if True the
  full path is stored in the coef\_path\_attribute.
  %
  If you compute the solution for a large problem or many targets, setting
  fit\_path to False will lead to a speedup, especially with a small alpha.
  %
  \default{True}
  %
\end{itemize}
\zNormalizationNotPerformed{Lars}
%%%%%%
\subparagraph{Linear Model: Cross-validated Least Angle Regression model}
\mbox{}
\\The \textit{Cross-validated Least Angle Regression model} is a regression
algorithm for high-dimensional data.
%
It is similar to the LARS method, but the best model is selected by
cross-validation.
%
\skltype{Cross-validated Least Angle Regression model}{linear\_model|LarsCV}
\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \verDescriptionA{False}
  \item \normalizeDescription{True}
  \item \precomputeDescription{'auto'}
  \item \maxIterDescription{500}
  \item \nAlphasDescription{1000}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the
  machine-precision regularization in the computation of the Cholesky diagonal
  factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the \textit{tol} parameter in some iterative optimization-based
  algorithms, this parameter does not control the tolerance of the optimization.
  %
  \default{2.2204460492503131e-16}
\end{itemize}
\subparagraph{Linear Model trained with L1 prior as regularizer (aka the Lasso)}
\mbox{}
\\The \textit{Linear Model trained with L1 prior as regularizer (Lasso)} is a
shrinkage and selection method for linear regression.
%
It minimizes the usual sum of squared errors, with a bound on the sum of the
absolute values of the coefficients.
%
\skltype{Linear Model trained with L1 prior as regularizer
  (Lasso)}{linear\_model|Lasso}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, sets a constant
  multiplier for the L1 term.
  %
  alpha = 0 is equivalent to an ordinary least square, solved by the
  LinearRegression object.
  %
  For numerical reasons, using alpha = 0 with the Lasso object is not advised
  and you should instead use the LinearRegression object.
  %
  \default{1.0}
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \precomputeDescription{False}
  \nb For sparse input this option is always True to preserve sparsity.
  \item \maxIterDescription{1000}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{False}
  \item \positiveDescription{False}
\end{itemize}
\zNormalizationNotPerformed{LarsCV}
\subparagraph{Lasso linear model with iterative fitting along a regularization
  path}
\mbox{}

The \textit{Lasso linear model with iterative fitting along a regularization
path} is an algorithm of the Lasso family, that computes the linear regressor weights,
identifying the regularization path in an iterative fitting (see http://www.jstatsoft.org/v33/i01/paper)

\skltype{Lasso linear model with iterative fitting along a regularization path
regressor}{linear\_model|LassoCV}
\begin{itemize}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the length of
  the path.
  %
  eps=1e-3 means that alpha\_min / alpha\_max = 1e-3.
  %
  \default{1.0e-3}
  %
  \item \xmlNode{n\_alphas}, \xmlDesc{int, optional field}, sets the number of
  alphas along the regularization path.
  %
  \default{100}
  %
  \item \xmlNode{alphas}, \xmlDesc{numpy array, optional field}, lists the
  locations of the alphas used to compute the models.
  %
  \default{None}
  %
  If None, alphas are set automatically.
  \item \precomputeDescription{'auto'}
  \item \maxIterDescription{1000}
  \item \tolDescriptionB{1.e-4}
  \item \verDescriptionB{False}
  \item \positiveDescription{False}
\end{itemize}
\zNormalizationNotPerformed{LassoCV}
\subparagraph{Lasso model fit with Least Angle Regression}
\mbox{}

\textit{Lasso model fit with Least Angle Regression} (aka Lars)
It is a Linear Model trained with an L1 prior as regularizer.
In order to use the \textit{Least Angle Regression model regressor}, the user needs to set the sub-node
%
\skltype{Least Angle Regression model
regressor}{linear\_model|LassoLars}

\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, specifies a constant
  that multiplies the penalty terms.
  %
  $alpha = 0$ is equivalent to an ordinary least square, solved by the
  \textbf{LinearRegression} object.
  \default{1.0}
  %
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{False}
  \item \normalizeDescription{True}
  \item \precomputeDescription{'auto'}
  \item \maxIterDescription{500}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, sets the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  \default{2.2204460492503131e-16}
\end{itemize}
\zNormalizationNotPerformed{LassoLars}
\subparagraph{Cross-validated Lasso, using the LARS algorithm}
\mbox{}

The \textit{Cross-validated Lasso, using the LARS algorithm} is a
cross-validated Lasso, using the LARS algorithm.

\skltype{Cross-validated Lasso, using the LARS algorithm
regressor}{linear\_model|LassoLarsCV}

\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{False}
  \item \normalizeDescription{True}
  \item \precomputeDescription{'auto'}
  \item \maxIterDescription{500}
  \item \nAlphasDescription{1000}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, specifies the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  \default{2.2204460492503131e-16}
\end{itemize}
\zNormalizationNotPerformed{LassoLarsCV}
\subparagraph{Lasso model fit with Lars using BIC or AIC for model selection}
\mbox{}

The \textit{Lasso model fit with Lars using BIC or AIC for model selection} is
a Lasso model fit with Lars using BIC or AIC for model selection.
%\maljdan{redundant}
The optimization objective for Lasso is:
$(1 / (2 * n\_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
AIC is the Akaike information criterion and BIC is the Bayes information
criterion.
%
Such criteria are useful in selecting the value of the regularization parameter
by making a trade-off between the goodness of fit and the complexity of the
model.
%
A good model explains the data well while maintaining simplicity.
%
\skltype{Lasso model fit with Lars using BIC or AIC for
  model selection regressor}{linear\_model|LassoLarsIC}
\begin{itemize}
  \item \xmlNode{criterion}, \xmlDesc{`bic' | `aic' }, specifies the type of
  criterion to use.
  %
  \default{'aic'}
  %
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{False}
  \item \normalizeDescription{True}
  \item \precomputeDescription{'auto'}
  \item \maxIterDescription{500}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the tol parameter in some iterative optimization-based algorithms, this
  parameter does not control the tolerance of the optimization.
  %
  %
  \default{2.2204460492503131e-16}
\end{itemize}
\zNormalizationNotPerformed{LassoLarsIC}
\subparagraph{Ordinary least squares Linear Regression}
\mbox{}

The \textit{Ordinary least squares Linear Regression} is a method for
estimating the unknown parameters in a linear regression model, with the goal of
minimizing the differences between the observed responses in some arbitrary
dataset and the responses predicted by the linear approximation of the data.
%
\skltype{Ordinary least squares Linear
Regressor}{linear\_model|LinearRegression}

\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
\end{itemize}
\zNormalizationNotPerformed{LinearRegression}
\subparagraph{Logistic Regression}
\mbox{}
\\The \textit{Logistic Regression} implements L1 and L2 regularized logistic
regression using the liblinear library.
%
It can handle both dense and sparse input.
%
This regressor uses C-ordered arrays or CSR matrices containing 64-bit floats
for optimal performance; any other input format will be converted (and copied).
%
\skltype{Logistic Regressor}{linear\_model|LogisticRegression}
\begin{itemize}
  \item \xmlNode{penalty}, \xmlDesc{string, `l1' or `l2'}, specifies the norm
  used in the penalization.
  %
  \default{'l2'}
  %}
  \item \xmlNode{dual}, \xmlDesc{boolean}, specifies the dual or primal
  formulation.
  %
  Dual formulation is only implemented for the l2 penalty.
  %
  Prefer dual=False when n\_samples $>$ n\_features.
  %
  \default{False}
  %
  \item \xmlNode{C}, \xmlDesc{float, optional field}, is the inverse of the
  regularization strength; must be a positive float.
  %
  Like in support vector machines, smaller values specify stronger
  regularization.
  %
  \default{1.0}
  \item \xmlNode{fit\_intercept}, \xmlDesc{boolean}, specifies if a constant
  (a.k.a. bias or intercept) should be added to the decision function.
  %
  \default{True}
  \item \xmlNode{intercept\_scaling}, \xmlDesc{float, optional field}, when
  self.fit\_intercept is True, instance vector x becomes [x,
  self.intercept\_scaling], i.e. a ``synthetic'' feature with constant value
  equal to intercept\_scaling is appended to the instance vector.
  %
  The intercept becomes intercept\_scaling * synthetic feature
  weight.
  \nb The synthetic feature weight is subject to l1/l2 regularization as are all
  other features.
  %
  To lessen the effect of regularization on synthetic feature weight (and
  therefore on the intercept) intercept\_scaling has to be increased.
  \default{1.0}
  \item \xmlNode{class\_weight}, \xmlDesc{dict, or 'balanced', optional}
  Weights associated with classes in the form \{class\_label: weight\}. If not given, all classes are supposed to have weight one.
  %
  The ``balanced'' mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the
  input data as n\_samples / (n\_classes * np.bincount(y))
  %
  Note that these weights will be multiplied with sample\_weight (passed through the fit method) if sample\_weight is specified.
  %
  New in version 0.17: class\_weight=’balanced’ instead of deprecated class\_weight=’auto’.
  %
  \default{None}
  %
  \item \randomStateDescription{None}
  \item \tolDescriptionC{0.0001}
\end{itemize}
\zNormalizationPerformed{LogisticRegression}
\subparagraph{Multi-task Lasso model trained with L1/L2 mixed-norm as
  regularizer}
\mbox{}
\\The \textit{Multi-task Lasso model trained with L1/L2 mixed-norm as
  regularizer} is a regressor where the optimization objective for Lasso is:
$(1 / (2 * n\_samples)) * ||Y - XW||^2_{Fro} + alpha * ||W||_{21}$
Where:
$||W||_{21} = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
%
\skltype{Multi-task Lasso model trained with L1/L2
  mixed-norm as regularizer regressor}{linear\_model|MultiTaskLasso}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, sets the constant
  multiplier for the L1/L2 term.
  %
  \default{1.0}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{1000}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{False}
\end{itemize}
\zNormalizationNotPerformed{MultiTaskLasso}
\subparagraph{Multi-task Elastic Net model trained with L1/L2 mixed-norm as
  regularizer}
\mbox{}

The \textit{Multi-task Elastic Net model trained with L1/L2 mixed-norm as
  regularizer} is a regressor where the optimization objective for
MultiTaskElasticNet is:
$(1 / (2 * n\_samples)) * ||Y - XW||^{Fro}_2
+ alpha * l1\_ratio * ||W||_{21}
+ 0.5 * alpha * (1 - l1\_ratio) * ||W||_{Fro}^2$
Where:
$||W||_{21} = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
%
\skltype{Multi-task ElasticNet model trained with L1/L2
  mixed-norm as regularizer regressor}{linear\_model|MultiTaskElasticNet}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, represents a constant
  multiplier for the L1/L2 term.
  %
  \default{1.0}
  \item \xmlNode{l1\_ratio}, \xmlDesc{float}, represents the Elastic Net mixing
  parameter, with $0 < l1\_ratio \leq 1$.
  %
  For $l1\_ratio = 0$ the penalty is an L1/L2 penalty.
  %
  For $l1\_ratio = 1$ it is an L1 penalty.
  %
  For $0 < l1\_ratio < 1$, the penalty is a combination of L1/L2
  and L2.
  %
  \default{0.5}
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{False}
\end{itemize}
\zNormalizationNotPerformed{MultiTaskElasticNet}
\subparagraph{Orthogonal Mathching Pursuit model (OMP)}
\mbox{}

The \textit{Orthogonal Mathching Pursuit model (OMP)} is a type of sparse
approximation which involves finding the ``best matching'' projections of
multidimensional data onto an over-complete dictionary, $D$.
%
\skltype{Orthogonal Mathching Pursuit model (OMP)
regressor}{linear\_model|OrthogonalMatchingPursuit}
\begin{itemize}
  \item \xmlNode{n\_nonzero\_coefs}, \xmlDesc{int, optional field}, represents
  the desired number of non-zero entries in the solution.
  %
  If None, this value is set to 10\% of n\_features.
  %
  \default{None}
  \item \xmlNode{tol}, \xmlDesc{float, optional field}, specifies the maximum
  norm of the residual.
  %
  If not None, overrides n\_nonzero\_coefs.
  %
  \default{None}
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{True}
  \item \xmlNode{precompute}, \xmlDesc{\{True, False, `auto'\}}, specifies
  whether to use a precomputed Gram and Xy matrix to speed up calculations.
  %
  Improves performance when n\_targets or n\_samples is very large.
  %
  \nb If you already have such matrices, you can pass them directly to the
  fit method.
  %
  \default{`auto'}
\end{itemize}
\zNormalizationNotPerformed{OrthogonalMatchingPursuit}
\subparagraph{Cross-validated Orthogonal Mathching Pursuit model (OMP)}
\mbox{}

The \textit{Cross-validated Orthogonal Mathching Pursuit model (OMP)} is a
regressor similar to OMP which has good performance in sparse recovery.
%
\skltype{Cross-validated Orthogonal Mathching Pursuit model (OMP)
regressor}{linear\_model|OrthogonalMatchingPursuitCV}
\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{True}
  \item \maxIterDescription{None}
  %
  Maximum number of iterations to perform, therefore maximum features to
  include 10\% of n\_features but at least 5 if available.
  %
  \item \xmlNode{cv}, \xmlDesc{cross-validation generator, optional},
  %
  see sklearn.cross\_validation.
  %
  \default{None}
  \item \verDescriptionB{False}
\end{itemize}
\zNormalizationNotPerformed{OrthogonalMatchingPursuitCV}
\subparagraph{Passive Aggressive Classifier}
\mbox{}
\\The \textit{Passive Aggressive Classifier} is a principled approach to linear
classification that advocates minimal weight updates i.e., the least required
to correctly classify the current training instance.
%
\skltype{Passive Aggressive
Classifier}{linear\_model|PassiveAggressiveClassifier}
\begin{itemize}
  \item \xmlNode{C}, \xmlDesc{float}, specifies the maximum step size
  (regularization).
  %
  \default{1.0}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{True}
  \item \randomStateDescription{None}
  \item \verDescriptionB{0}
  \item \xmlNode{loss}, \xmlDesc{string, optional field}, the loss function to
  be used:
  \begin{itemize}
    \item hinge: equivalent to PA-I (http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)
    \item squared\_hinge: equivalent to PA-II (http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)
  \end{itemize}
  %
  \default{'hinge'}
  %
  \item \warmStartDescription{False}
\end{itemize}
\zNormalizationPerformed{PassiveAggressiveClassifier}
\subparagraph{Passive Aggressive Regressor}
\mbox{}
\\The \textit{Passive Aggressive Regressor} is similar to the Perceptron in that
it does not require a learning rate.
%
However, contrary to the Perceptron, this regressor includes a regularization
parameter, $C$.

\skltype{Passive Aggressive Regressor}{linear\_model|PassiveAggressiveRegressor}
\begin{itemize}
  \item \xmlNode{C}, \xmlDesc{float}, sets the maximum step size
  (regularization).
  %
  \default{1.0}
  %
  \item \xmlNode{epsilon}, \xmlDesc{float}, if the difference between the
  current prediction and the correct label is below this threshold, the model is
  not updated.
  %
  \default{0.1}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{True}
  \item \randomStateDescription{None}
  \item \verDescriptionB{0}
  \item \xmlNode{loss}, \xmlDesc{string, optional field}, specifies the loss
  function to be used:
  \begin{itemize}
    \item epsilon\_insensitive: equivalent to PA-I in the reference paper (http://jmlr.csail.mit.edu/papers/volume7
    /crammer06a/crammer06a.pdf).
    \item squared\_epsilon\_insensitive: equivalent to PA-II in the reference paper (http://jmlr.csail.mit.edu/papers
    /volume7/crammer06a/crammer06a.pdf).
  \end{itemize}
  %
  \default{'epsilon\_insensitive'}
  %
  \item \warmStartDescription{False}
\end{itemize}
\zNormalizationPerformed{PassiveAggressiveRegressor}
\subparagraph{Perceptron}
\mbox{}

The \textit{Perceptron} method is an algorithm for supervised classification of
an input into one of several possible non-binary outputs.
%
It is a type of linear classifier, i.e. a classification algorithm that makes
its predictions based on a linear predictor function combining a set of weights
with the feature vector.
%
The algorithm allows for online learning, in that it processes elements in the
training set one at a time.
%
\skltype{Perceptron classifier}{linear\_model|Perceptron}
\begin{itemize}
  \item \xmlNode{penalty}, \xmlDesc{None, `l2' or `l1' or `elasticnet'}, defines
  the penalty (aka regularization term) to be used.
  %
  \default{None}
  %
  \item \xmlNode{alpha}, \xmlDesc{float}, sets the constant multiplier for the
  regularization term if regularization is used.
  %
  \default{0.0001}
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{True}
  \item \randomStateDescription{0}
  \item \verDescriptionB{0}
  \item \xmlNode{eta0}, \xmlDesc{double, optional field}, defines the constant
  multiplier for the updates.
  %
  \default{1.0}
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dict, \{class\_label: weight\} or “balanced” or None, optional}
  Preset for the class\_weight fit parameter.
  %
  Weights associated with classes. If not given, all classes are supposed to have weight one.
  %
  The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class
  frequencies in the input data as n\_samples / (n\_classes * np.bincount(y))
  %
  \item \warmStartDescription{False}
\end{itemize}
\zNormalizationPerformed{PassiveAggressiveRegressor}
\subparagraph{Linear least squares with l2 regularization}
\mbox{}
\\The \textit{Linear least squares with l2 regularization} solves a regression
model where the loss function is the linear least squares function and the
regularization is given by the l2-norm.
%
Also known as Ridge Regression or Tikhonov regularization.
%
This estimator has built-in support for multivariate regression (i.e., when y
is a 2d-array of shape [n\_samples, n\_targets]).
%
\skltype{Linear least squares with l2 regularization}{linear\_model|Ridge}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, array-like},
  %
  shape = [n\_targets] Small positive values of alpha improve the
  conditioning of the problem and reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^-1$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  If an array is passed, penalties are assumed to be specific to the targets.
  %
  Hence they must correspond in number.
  %
  \default{1.0}
  %
  \item \fitInterceptDescription{True}
  \item \maxIterDescription{determined by scipy.sparse.linalg.}
  \item \normalizeDescription{False}
  \item \solverDescription
  \default{`auto'}
\end{itemize}
\zNormalizationNotPerformed{Ridge}
%TODO document copy_X
%TODO document tol
%TODO document random_state

\subparagraph{Classifier using Ridge regression}
\mbox{}

The \textit{Classifier using Ridge regression} is a classifier based on linear
least squares with l2 regularization.
\skltype{Classifier using Ridge regression}{linear\_model|RidgeClassifier}

\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float}, small positive values of alpha improve
  the conditioning of the problem and reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^-1$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  \default{1.0}
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dict, optional field}, specifies
  weights associated with classes in the form {class\_label: weight}.
  %
  If not given, all classes are assumed to have weight one.
  %
  \default{None}
  %
  \item \fitInterceptDescription{True}
  \item \maxIterDescription{determined by scipy.sparse.linalg.}
  \item \normalizeDescription{False}
  \item \solverDescription
  \default{`auto'}
  \item \xmlNode{tol}, \xmlDesc{float}, defines the required precision of the
  solution.
  \default{0.001}
\end{itemize}
\zNormalizationNotPerformed{RidgeClassifier}
%TODO document random_state
%TODO document copy_X

\subparagraph{Ridge classifier with built-in cross-validation}
\mbox{}
\\The \textit{Ridge classifier with built-in cross-validation} performs
Generalized Cross-Validation, which is a form of efficient leave-one-out
cross-validation.
%
Currently, only the n\_features $>$ n\_samples case is handled efficiently.
%
\skltype{Ridge classifier with built-in cross-validation
classifier}{linear\_model|RidgeClassifierCV}
\begin{itemize}
  \item \xmlNode{alphas}, \xmlDesc{numpy array of shape [n\_alphas]}, is an
  array of alpha values to try.
  %
  Small positive values of alpha improve the conditioning of the problem and
  reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^{-1}$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  \default{(0.1, 1.0, 10.0)}
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \xmlNode{scoring}, \xmlDesc{string, callable or None, optional}, is a
  string (see model evaluation documentation) or a scorer callable object /
  function with signature scorer(estimator, X, y).
  %
  \default{None}
  \item \xmlNode{cv}, \xmlDesc{cross-validation generator, optional},
  %
  If None, Generalized Cross-Validation (efficient leave-one-out) will be used.
  %
  \default{None}
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dic, optional field}, specifies
  weights associated with classes in the form {class\_label:weight}.
  %
  If not given, all classes are supposed to have weight one.
  %
  \default{None}
  %
\end{itemize}
\zNormalizationNotPerformed{RidgeClassifierCV}
\subparagraph{Ridge regression with built-in cross-validation}
\mbox{}

The \textit{Ridge regression with built-in cross-validation} performs
Generalized Cross-Validation, which is a form of efficient leave-one-out
cross-validation.
%
\skltype{Ridge regression with built-in cross-validation regressor}{linear\_model|RidgeCV}
\begin{itemize}
  \item \xmlNode{alphas}, \xmlDesc{numpy array of shape [n\_alphas]}, specifies
  an array of alpha values to try.
  %
  Small positive values of alpha improve the conditioning of the problem and
  reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^{-1}$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  \default{(0.1, 1.0, 10.0)}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \xmlNode{scoring}, \xmlDesc{string, callable or None, optional}, is a
  string (see model evaluation documentation) or a scorer callable object /
  function with signature scorer(estimator, X, y).
  %
  \default{None}
  %
  \item \xmlNode{cv}, \xmlDesc{cross-validation generator, optional field}, if
  None, Generalized Cross-Validation (efficient leave-one-out) will be used.
  %
  \default{None}
  %
  \item \xmlNode{gcv\_mode}, \xmlDesc{\{None, `auto,' `svd,' `eigen'\}, optional
  field}, is a flag indicating which strategy to use when performing Generalized
  Cross-Validation.
  %
  Options are:
	\begin{itemize}
    \item `auto:' use svd if n\_samples > n\_features or when X is a
    sparse matrix, otherwise use eigen
  	\item `svd:' force computation via singular value decomposition of $X$
    (does not work for sparse matrices)
	  \item `eigen:' force computation via eigendecomposition of $X^T X$
	\end{itemize}
	The `auto' mode is the default and is intended to pick the cheaper
  option of the two depending upon the shape and format of the training data.
  %
  \default{None}
  \item \xmlNode{store\_cv\_values}, \xmlDesc{boolean}, is a flag indicating if
  the cross-validation values corresponding to each alpha should be stored in
  the cv\_values\_attribute (see below).
  %
  This flag is only compatible with cv=None (i.e. using Generalized
  Cross-Validation).
  %
  \default{False}
\end{itemize}
\zNormalizationNotPerformed{RidgeCV}
\subparagraph{Linear classifiers (SVM, logistic regression, a.o.) with SGD
training}
\mbox{}

The \textit{Linear classifiers (SVM, logistic regression, a.o.) with SGD
training} implements regularized linear models with stochastic gradient
descent (SGD) learning: the gradient of the loss is estimated for each sample at
a time and the model is updated along the way with a decreasing strength
schedule (aka learning rate).
%
SGD allows minibatch (online/out-of-core) learning, see the partial\_fit method.
%
This implementation works with data represented as dense or sparse arrays of
floating point values for the features.
%
The model it fits can be controlled with the loss parameter; by default, it fits
a linear support vector machine (SVM).
%
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared Euclidean norm L2 or
the absolute norm L1 or a combination of both (Elastic Net).
%
If the parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieves
online feature selection.
%
\skltype{Linear classifiers (SVM, logistic regression, a.o.) with SGD
training}{linear\_model|SGDClassifier}
\begin{itemize}
  \item \xmlNode{loss}, \xmlDesc{str, `hinge,' `log,' `modified\_huber,'
  `squared\_hinge,' `perceptron,' or a regression loss: `squared\_loss,'
  `huber,' `epsilon\_insensitive,' or `squared\_epsilon\_insensitive'},
  %
  dictates the loss function to be used.
  %
  The available options are:
  \begin{itemize}
    \item `hinge' gives a linear SVM.
    \item `log' loss gives logistic regression, a probabilistic classifier.
    \item `modified\_huber' is another smooth loss that brings tolerance to
    outliers as well as probability estimates.
    \item `squared\_hinge' is like hinge but is quadratically penalized.
    \item `perceptron' is the linear loss used by the perceptron algorithm.
  \end{itemize}
  The other losses are designed for regression but can be useful in
  classification as well; see SGDRegressor for a description.
  %
  \default{`hinge'}
  %
  \item \xmlNode{penalty}, \xmlDesc{str, `l2' or `l1' or `elasticnet'}, defines
  the penalty (aka regularization term) to be used.
  %
  `l2' is the standard regularizer for linear SVM models.
  %
  `l1' and `elasticnet' might bring sparsity to the model (feature
  selection) not achievable with `l2.'
  %
  \default{`l2'}
  \item \xmlNode{alpha}, \xmlDesc{float}, is the constant multiplier for the
  regularization term.
  %
  \default{0.0001}
  \item \xmlNode{l1\_ratio}, \xmlDesc{float}, is the Elastic Net mixing
  parameter, with 0 <= l1\_ratio <= 1.
  %
  l1\_ratio=0 corresponds to L2 penalty, l1\_ratio=1 to L1.
  %
  \default{0.15}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{True}
  \item \randomStateSVMDescription{None}
  \item \verDescriptionB{0}
  \item \xmlNode{epsilon}, \xmlDesc{float, optional field}, varies meaning
  depending on the value of \xmlNode{loss}. If loss is `huber',
  `epsilon\_insensitive' or `squared\_epsilon\_insensitive' then this is the
  epsilon in the epsilon-insensitive loss functions. For ‘huber’,
  determines the threshold at which it becomes less important to get the
  prediction exactly right. For `epsilon\_insensitive, any differences between
  the current prediction and the correct label are ignored if they are less than
  this threshold.
  %
  \default{0.1}
  %
  \item \xmlNode{learning\_rate}, \xmlDesc{string, optional field}, specifies
  the learning rate:
  \begin{itemize}
    \item `constant:' eta = eta0
    \item `optimal:' eta = 1.0 / (t + t0)
    \item `invscaling:' eta = eta0 / pow(t, power\_t)
  \end{itemize}
  \default{`optimal'}
  %
  \item \xmlNode{eta0}, \xmlDesc{double}, specifies the initial learning rate
  for the `constant' or `invscaling' schedules.
  %
  The default value is 0.0 as eta0 is not used by the default schedule
  `optimal.'
  %
  \default{0.0}
  %
  \item \xmlNode{power\_t}, \xmlDesc{double}, represents the exponent for
  the inverse scaling learning rate.
  %
  \default{0.5}
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dict, {class\_label}}, is the preset
  for the class\_weight fit parameter.
  %
  Weights associated with classes.
  %
  If not given, all classes are assumed to have weight one.
  %
  The ``auto'' mode uses the values of y to automatically adjust weights
  inversely proportional to class frequencies.
  %
  \default{None}
  %
  \item \warmStartDescription{False}
  %
\end{itemize}
\zNormalizationPerformed{SGDClassifier}
%TODO document average

\subparagraph{Linear model fitted by minimizing a regularized empirical loss
with SGD}
\mbox{}
\\The \textit{Linear model fitted by minimizing a regularized empirical loss
with SGD} is a model where SGD stands for Stochastic Gradient Descent: the
gradient of the loss is estimated each sample at a time and the model is updated
along the way with a decreasing strength schedule (aka learning rate).
%
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm L2 or
the absolute norm L1 or a combination of both (Elastic Net).
%
If the parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieving
online feature selection.
%
This implementation works with data represented as dense numpy arrays of
floating point values for the features.
%
\skltype{Linear model fitted by minimizing a regularized empirical loss with SGD}{linear\_model|SGDRegressor}
\begin{itemize}
  \item \xmlNode{loss}, \xmlDesc{str, `squared\_loss,' `huber,'
  `epsilon\_insensitive,' or `squared\_epsilon\_insensitive'}, specifies the
  loss function to be used.
  %
  Defaults to `squared\_loss' which refers to the ordinary least squares fit.
  %
  `huber' modifies `squared\_loss' to focus less on getting outliers correct by
  switching from squared to linear loss past a distance of epsilon.
  %
  `epsilon\_insensitive' ignores errors less than epsilon and is linear past
  that; this is the loss function used in SVR.
  %
  `squared\_epsilon\_insensitive' is the same but becomes squared loss past a
  tolerance of epsilon.
  %
  \default{`squared\_loss'}
  \item \xmlNode{penalty}, \xmlDesc{str, `l2' or `l1' or `elasticnet'}, sets
  the penalty (aka regularization term) to be used.
  %
  Defaults to `l2' which is the standard regularizer for linear SVM models.
  %
  `l1' and `elasticnet' might bring sparsity to the model (feature
  selection) not achievable with `l2'.
  %
  \default{`l2'}
  %
  \item \xmlNode{alpha}, \xmlDesc{float},
  %
  Constant that multiplies the regularization term.
  %
  Defaults to 0.0001
  \item \xmlNode{l1\_ratio}, \xmlDesc{float}, is the Elastic Net mixing
  parameter, with $0 \leq l1\_ratio \leq 1$.
  %
  l1\_ratio=0 corresponds to L2 penalty, l1\_ratio=1 to L1.
  %
  \default{0.15}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{True}
  \item \randomStateDescription{None}
  \item \verDescriptionB{0}
  %
  \item \xmlNode{epsilon}, \xmlDesc{float}, sets the epsilon in the
  epsilon-insensitive loss functions; only if loss is `huber,'
  `epsilon\_insensitive,' or `squared\_epsilon\_insensitive.'
  %
  For `huber', determines the threshold at which it becomes less important
  to get the prediction exactly right.
  %
  For epsilon-insensitive, any differences between the current prediction and
  the correct label are ignored if they are less than this threshold.
  %
  \default{0.1}
  %
  \item \xmlNode{learning\_rate}, \xmlDesc{string, optional field},
  Learning rate:
  \begin{itemize}
    \item constant: eta = eta0
    \item optimal: eta = 1.0/(t+t0)
    \item invscaling: eta= eta0 / pow(t, power\_t)
  \end{itemize}
  \default{invscaling}
  \item \xmlNode{eta0}, \xmlDesc{double}, specifies the initial learning rate.
  %
  \default{0.01}
  %
  \item \xmlNode{power\_t}, \xmlDesc{double, optional field}, specifies the
  exponent for inverse scaling learning rate.
  %
  \default{0.25}
  %
  \item \warmStartDescription{False}
  %
\end{itemize}
\zNormalizationPerformed{SGDRegressor}
%TODO document average

%%%%% ROM Model - SciKitLearn: Support Vector Machines %%%%%%%
\paragraph{Support Vector Machines}
\label{SVM}
In machine learning, \textbf{Support Vector Machines} (SVMs, also support vector
networks) are supervised learning models with associated learning algorithms
that analyze data and recognize patterns, used for classification and regression
analysis.
%
Given a set of training examples, each marked as belonging to one of two
categories, an SVM training algorithm builds a model that assigns new examples
into one category or the other, making it a non-probabilistic binary linear
classifier.
%
An SVM model is a representation of the examples as points in space, mapped so
that the examples of the separate categories are divided by a clear gap that is
as wide as possible.
%
New examples are then mapped into that same space and predicted to belong to a
category based on which side of the gap they fall on.
%
In addition to performing linear classification, SVMs can efficiently perform a
non-linear classification using what is called the kernel trick, implicitly
mapping their inputs into high-dimensional feature spaces.
%
\zNormalizationPerformed{SVM-based}

In the following, all the SVM models available in RAVEN are reported.

\subparagraph{Linear Support Vector Classifier}
\mbox{}
\\The \textit{Linear Support Vector Classifier} is similar to SVC with parameter
kernel=`linear', but implemented in terms of liblinear rather than libsvm,
so it has more flexibility in the choice of penalties and loss functions and
should scale better (to large numbers of samples).
%
This class supports both dense and sparse input and the multiclass support is
handled according to a one-vs-the-rest scheme.
%
\skltype{Linear Support Vector Classifier}{svm|LinearSVC}
\begin{itemize}
  \item \CSVMDescription{1.0}
  \item \xmlNode{loss}, \xmlDesc{string, `hinge' or `squared\_hinge'}, specifies the loss
  function.
  %
  `hinge' is the hinge loss (standard SVM) while `squared\_hinge' is the squared hinge
  loss.
  %
  \default{`squared\_hinge'}
  %
  \item \xmlNode{penalty}, \xmlDesc{string, `l1' or `l2'}, specifies the norm
  used in the penalization.
  %
  The `l2' penalty is the standard used in SVC.
  %
  The `l1' leads to coef\_vectors that are sparse.
  %
  \default{`l2'}
  %
  \item \xmlNode{dual}, \xmlDesc{boolean}, selects the algorithm to either solve
  the dual or primal optimization problem.
  %
  Prefer dual=False when n\_samples $>$ n\_features.
  %
  \default{True}
  %
  \item \tolSVMDescription{1e-4}
  %
  \item \xmlNode{multi\_class}, \xmlDesc{string, `ovr' or `crammer\_singer'},
  %
  Determines the multi-class strategy if y contains more than two classes.
  %
  ovr trains n\_classes one-vs-rest classifiers, while
  crammer\_singer optimizes a joint objective over all classes.
  %
  While crammer\_singer is interesting from a theoretical perspective as it is
  consistent, it is seldom used in practice and rarely leads to better accuracy
  and is more expensive to compute.
  %
  If crammer\_singer is chosen, the options loss, penalty and dual
  will be ignored.
  %
  \default{`ovr'}
  %
  \item \fitInterceptDescription{True}
  %
  \item \xmlNode{intercept\_scaling}, \xmlDesc{float, optional field}, when
  True, the instance vector x becomes [x,self.intercept\_scaling], i.e. a
  ``synthetic'' feature with constant value equals to intercept\_scaling is
  appended to the instance vector.
  %
  The intercept becomes intercept\_scaling * synthetic feature
  weight.
  \nb The synthetic feature weight is subject to l1/l2 regularization as are all
  other features.
  %
  To lessen the effect of regularization on the synthetic feature weight (and
  therefore on the intercept) intercept\_scaling has to be increased.
  %
  \default{1}
  %
  \item \classWeightDescription{None}
  \item \verDescriptionB{0}
  %
  \nb This setting takes advantage of a per-process runtime setting in liblinear
  that, if enabled, may not work properly in a multithreaded context.
  %
  \item \randomStateSVMDescription{None}
\end{itemize}

\subparagraph{C-Support Vector Classification}
\mbox{}
\\The \textit{C-Support Vector Classification} is a based on libsvm.
%
The fit time complexity is more than quadratic with the number of samples which
makes it hard to scale to datasets with more than a couple of 10000 samples.
%
The multiclass support is handled according to a one-vs-one scheme.
%
\skltype{C-Support Vector Classifier}{svm|SVC}
\begin{itemize}
  \item \CSVMDescription{1.0}
  \item \kernelDescription{`rbf'}
  \item \degreeDescription{3.0}
  \item \gammaDescription{`auto'}
  \item \coefZeroDescription{0.0}
  \item \probabilityDescription{False}
  \item \shrinkingDescription{True}
  \item \tolSVMDescription{1e-3}
  \item \cacheSizeDescription{}
  \item \classWeightDescription{None}
  \item \verSVMDescription{False}
  \item \maxIterDescription{-1}
    %TODO: Should decision_function_shape be documented?
  \item \randomStateSVMDescription{None}
  %
\end{itemize}

\subparagraph{Nu-Support Vector Classification}
\mbox{}

The \textit{Nu-Support Vector Classification} is similar to SVC but uses a
parameter to control the number of support vectors.
%
The implementation is based on libsvm.
%
\skltype{Nu-Support Vector Classifier}{svm|NuSVC}
\begin{itemize}
  \item \xmlNode{nu}, \xmlDesc{float, optional field}, is an upper bound on the
  fraction of training errors and a lower bound of the fraction of support
  vectors.
  %
  Should be in the interval (0, 1].
  %
  \default{0.5}
  %
  \item \kernelDescription{`rbf'}
  \item \degreeDescription{3}
  \item \gammaDescription{`auto'}
  \item \coefZeroDescription{0.0}
  \item \probabilityDescription{False}
  \item \shrinkingDescription{True}
  \item \tolSVMDescription{1e-3}
  \item \cacheSizeDescription{}
  \item \verSVMDescription{False}
  \item \maxIterDescription{-1}
    %TODO document decision_function_shape
  \item \randomStateSVMDescription{None}
  %
\end{itemize}

\subparagraph{Support Vector Regression}
\mbox{}
\\The \textit{Support Vector Regression} is an epsilon-Support Vector
Regression.
%
The free parameters in this model are C and epsilon.
%
The implementations is a based on libsvm.
%
\skltype{Support Vector Regressor}{svm|SVR}
\begin{itemize}
  \item \CSVMDescription{1.0}
  \item \xmlNode{epsilon}, \xmlDesc{float, optional field}, specifies the
  epsilon-tube within which no penalty is associated in the training loss
  function with points predicted within a distance epsilon from the actual
  value.
  %
  \default{0.1}
  %
  \item \kernelDescription{`rbf'}
  \item \degreeDescription{3.0}
  \item \gammaDescription{`auto'}
  \item \coefZeroDescription{0.0}
  \item \shrinkingDescription{True}
  \item \tolSVMDescription{1e-3}
  \item \cacheSizeDescription{}
  \item \verSVMDescription{False}
  \item \maxIterDescription{-1}
  %
\end{itemize}
 %%%%% ROM Model - SciKitLearn: MultiClass %%%%%%%
\paragraph{Multi Class}
\label{Multiclass}
Multiclass classification means a classification task with more than two
classes; e.g., classify a set of images of fruits which may be oranges, apples,
or pears.
%
Multiclass classification makes the assumption that each sample is assigned to
one and only one label: a fruit can be either an apple or a pear but not both at
the same time.

\zNormalizationNotPerformed{multi-class-based}

%
In the following, all the multi-class models available in RAVEN are reported.
%
%%%%%%%%%
\subparagraph{One-vs-the-rest (OvR) multiclass/multilabel strategy}
\mbox{}

The \textit{One-vs-the-rest (OvR) multiclass/multilabel strategy}, also known
as one-vs-all, consists in fitting one classifier per class.
%
For each classifier, the class is fitted against all the other classes.
%
In addition to its computational efficiency (only n\_classes classifiers are
needed), one advantage of this approach is its interpretability.
%
Since each class is represented by one and one classifier only, it is possible
to gain knowledge about the class by inspecting its corresponding classifier.
%
This is the most commonly used strategy and is a fair default choice.

\skltype{One-vs-the-rest (OvR) multiclass/multilabel classifier}{multiClass|OneVsRestClassifier}
\begin{itemize}
  \item \estimatorDescription{}
\end{itemize}
%Should n_jobs be documented?

%%%%%%%%%%%%
\subparagraph{One-vs-one multiclass strategy}
\mbox{}

The \textit{One-vs-one multiclass strategy} consists in fitting one classifier
per class pair.
%
At prediction time, the class which received the most votes is selected.
%
Since it requires to fit n\_classes * (n\_classes - 1) / 2 classifiers, this
method is usually slower than one-vs-the-rest, due to its O(n\_classes$^2$)
complexity.
%
However, this method may be advantageous for algorithms such as kernel
algorithms which do not scale well with n\_samples.
%
This is because each individual learning problem only involves a small subset of
the data whereas, with one-vs-the-rest, the complete dataset is used n\_classes
times.

\skltype{One-vs-one multiclass classifier}{multiClass|OneVsOneClassifier}
\begin{itemize}
  \item \estimatorDescription{}
\end{itemize}
%Should n_jobs be documented?

%%%%%%%%%%%%%
\subparagraph{Error-Correcting Output-Code multiclass strategy}
\mbox{}
\\The \textit{Error-Correcting Output-Code multiclass strategy} consists in
representing each class with a binary code (an array of 0s and 1s).
%
At fitting time, one binary classifier per bit in the code book is fitted.
%
At prediction time, the classifiers are used to project new points in the class
space and the class closest to the points is chosen.
%
The main advantage of these strategies is that the number of classifiers used
can be controlled by the user, either for compressing the model ($0 < code\_
size < 1$) or for making the model more robust to errors ($code\_ size > 1$).

\skltype{Error-Correcting Output-Code multiclass classifier}{multiClass|OutputCodeClassifier}
\begin{itemize}
  \item \estimatorDescription{}
  \item \xmlNode{code\_size}, \xmlDesc{float, required field}, represents the
  percentage of the number of classes to be used to create the code book.
  %
  A number between 0 and 1 will require fewer classifiers than one-vs-the-rest.
  %
  A number greater than 1 will require more classifiers than one-vs-the-rest.
  %
\end{itemize}
%Should random_state and n_jobs be documented?

%%%%%%%%%%%%%
%\subparagraph{fit a one-vs-the-rest strategy}
%pass
%\subparagraph{Make predictions using the one-vs-the-rest strategy}
%pass
%\subparagraph{ Fit a one-vs-one strategy}
%pass
%\subparagraph{Make predictions using the one-vs-one strategy}
%pass
%\subparagraph{Fit an error-correcting output-code strategy}
%pass
%\subparagraph{Make predictions using the error-correcting output-code strategy}
%pass

 %%%%% ROM Model - SciKitLearn: naiveBayes %%%%%%%
\paragraph{Naive Bayes}
\label{naiveBayes}
Naive Bayes methods are a set of supervised learning algorithms based on
applying Bayes' theorem with the ``naive'' assumption of independence between
every pair of features.
%
Given a class variable y and a dependent feature vector x\_1 through x\_n,
Bayes' theorem states the following relationship:
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}
{P(x_1, \dots, x_n)}
\end{equation}
Using the naive independence assumption that
\begin{equation}
P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),
\end{equation}
for all i, this relationship is simplified to
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
{P(x_1, \dots, x_n)}
\end{equation}
Since $P(x_1, \dots, x_n)$ is constant given the input, we can use the following
classification rule:
\begin{equation}
P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)
\Downarrow
\end{equation}
\begin{equation}
\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),
\end{equation}
and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and
$P(x_i \mid y)$; the former is then the relative frequency of class $y$ in the
training set.
%
The different naive Bayes classifiers differ mainly by the assumptions they make
regarding the distribution of $P(x_i \mid y)$.

In spite of their apparently over-simplified assumptions, naive Bayes
classifiers have worked quite well in many real-world situations, famously
document classification and spam filtering.
%
They require a small amount of training data to estimate the necessary
parameters.
%
(For theoretical reasons why naive Bayes works well, and on which types of data
it does, see the references below.)
Naive Bayes learners and classifiers can be extremely fast compared to more
sophisticated methods.
%
The decoupling of the class conditional feature distributions means that each
distribution can be independently estimated as a one dimensional distribution.
%
This in turn helps to alleviate problems stemming from the curse of
dimensionality.

On the flip side, although naive Bayes is known as a decent classifier, it is
known to be a bad estimator, so the probability outputs from predict\_proba are
not to be taken too seriously.
%
In the following, all the Naive Bayes available in RAVEN are reported.
%
%%%%%%%
\subparagraph{Gaussian Naive Bayes}
\mbox{}
\\The \textit{Gaussian Naive Bayes strategy} implements the Gaussian Naive Bayes
algorithm for classification.
%
The likelihood of the features is assumed to be Gaussian:
\begin{equation}
P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i -
  \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}
The parameters $\sigma_y$ and $\mu_y$ are estimated using maximum likelihood.

In order to use the \textit{Gaussian Naive Bayes strategy}, the user needs to
set the sub-node:

\xmlNode{SKLtype}\texttt{naiveBayes|GaussianNB}\xmlNode{/SKLtype}.

There are no additional sub-nodes available for this method.
%

\zNormalizationPerformed{GaussianNB}
%%%%%%%%%%%%
\subparagraph{Multinomial Naive Bayes}
\mbox{}
\\The \textit{Multinomial Naive Bayes} implements the naive Bayes algorithm for
multinomially distributed data, and is one of the two classic naive Bayes
variants used in text classification (where the data is typically represented
as word vector counts, although tf-idf vectors are also known to work well in
practice).
%
The distribution is parametrized by vectors $\theta_y =
(\theta_{y1},\ldots,\theta_{yn})$ for each class $y$, where $n$ is the number of
features (in text classification, the size of the vocabulary) and $\theta_{yi}$
is the probability $P(x_i \mid y)$ of feature $i$ appearing in a sample
belonging to class $y$.
%
The parameters $\theta_y$ are estimated by a smoothed version of maximum
likelihood, i.e. relative frequency counting:
\begin{equation}
\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}
\end{equation}
where $N_{yi} = \sum_{x \in T} x_i$ is the number of times feature $i$ appears
in a sample of class y in the training set T, and
$N_{y} = \sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class
$y$.
%
The smoothing priors $\alpha \ge 0$ account for features not present in the
learning samples and prevents zero probabilities in further computations.
%
Setting $\alpha = 1$ is called Laplace smoothing, while $\alpha < 1$ is called
Lidstone smoothing.
%
\skltype{Multinomial Naive Bayes strategy}{naiveBayes|MultinomialNB}
\begin{itemize}
  \item \alphaBayesDescription{1.0}
  \item \fitPriorDescription{True}
  \item \classPriorDescription{None}
\end{itemize}
\zNormalizationNotPerformed{MultinomialNB}
%%%%%%%%%%%%
\subparagraph{Bernoulli Naive Bayes}
\mbox{}
\\The \textit{Bernoulli Naive Bayes} implements the naive Bayes training and
classification algorithms for data that is distributed according to multivariate
Bernoulli distributions; i.e., there may be multiple features but each one is
assumed to be a binary-valued (Bernoulli, boolean) variable.
%
Therefore, this class requires samples to be represented as binary-valued
feature vectors; if handed any other kind of data, a \textit{Bernoulli Naive
Bayes} instance may binarize its input (depending on the binarize parameter).
%
The decision rule for Bernoulli naive Bayes is based on
\begin{equation}
P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)
\end{equation}
which differs from multinomial NB's rule in that it explicitly penalizes the
non-occurrence of a feature $i$ that is an indicator for class $y$, where the
multinomial variant would simply ignore a non-occurring feature.
%
In the case of text classification, word occurrence vectors (rather than word
count vectors) may be used to train and use this classifier.
%
\textit{Bernoulli Naive Bayes} might perform better on some datasets, especially
those with shorter documents.
%
It is advisable to evaluate both models, if time permits.
%
\skltype{Bernoulli Naive Bayes strategy}{naiveBayes|BernoulliNB}
\begin{itemize}
  \item \alphaBayesDescription{1.0}
  \item \xmlNode{binarize}, \xmlDesc{float, optional field},
  %
  Threshold for binarizing (mapping to booleans) of sample features.
  %
  If None, input is presumed to already consist of binary vectors.
  %
  \default{0.0}
  \item \fitPriorDescription{True}
  \item \classPriorDescription{None}
  %
\end{itemize}
\zNormalizationPerformed{BernoulliNB}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Neighbors %%%%%%%
\paragraph{Neighbors}
\label{Neighbors}

The \textit{Neighbors} class provides functionality for unsupervised and
supervised neighbor-based learning methods.
%
The unsupervised nearest neighbors method is the foundation of many other
learning methods, notably manifold learning and spectral clustering.
%
Supervised neighbors-based learning comes in two flavors: classification for
data with discrete labels, and regression for data with continuous labels.

The principle behind nearest neighbor methods is to find a predefined number of
training samples closest in distance to the new point, and predict the label
from these.
%
The number of samples can be a user-defined constant (k-nearest neighbor
learning), or vary based on the local density of points (radius-based neighbor
learning).
%
The distance can, in general, be any metric measure: standard Euclidean distance
is the most common choice.
%
Neighbor-based methods are known as non-generalizing machine learning methods,
since they simply ``remember'' all of its training data (possibly transformed
into a fast indexing structure such as a Ball Tree or KD Tree.).

\zNormalizationPerformed{Neighbors-based}

In the following, all the Neighbors' models available in RAVEN are reported.
%
%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Classifier}
\mbox{}
\\The \textit{K Neighbors Classifier} is a type of instance-based learning or
non-generalizing learning: it does not attempt to construct a general internal
model, but simply stores instances of the training data.
%
Classification is computed from a simple majority vote of the nearest neighbors
of each point: a query point is assigned the data class which has the most
representatives within the nearest neighbors of the point.
%
It implements learning based on the $k$ nearest neighbors of each query point,
where $k$ is an integer value specified by the user.

\skltype{K Neighbors Classifier}{neighbors|KNeighborsClassifier}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \weightsDescription{uniform}
  \item \algorithmDescription{auto}
  \item \leafSizeDescription{30}
  \item \metricDescription{minkowski}
  \item \pDescription{2}
    %TODO document metric_params
    %TODO document n_jobs?
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Classifier}
\mbox{}
\\The \textit{Radius Neighbors Classifier} is a type of instance-based learning
or non-generalizing learning: it does not attempt to construct a general
internal model, but simply stores instances of the training data.
%
Classification is computed from a simple majority vote of the nearest neighbors
of each point: a query point is assigned the data class which has the most
representatives within the nearest neighbors of the point.
%
It implements learning based on the number of neighbors within a fixed radius
$r$ of each training point, where $r$ is a floating-point value specified by the
user.

\skltype{Radius Neighbors Classifier}{neighbors|RadiusNeighbors}
\begin{itemize}
  \item \radiusDescription{1.0}
  \item \weightsDescription{uniform}
  \item \algorithmDescription{auto}
  \item \leafSizeDescription{30}
  \item \metricDescription{minkowski}
  \item \pDescription{2}
  \item \outlierLabelDescription{None}
    %TODO document metric_params
\end{itemize}

%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Regressor}
\mbox{}

The \textit{K Neighbors Regressor} can be used in cases where the data labels
are continuous rather than discrete variables.
%
The label assigned to a query point is computed based on the mean of the labels
of its nearest neighbors.
%
It implements learning based on the $k$ nearest neighbors of each query point,
where $k$ is an integer value specified by the user.

\skltype{K Neighbors Regressor}{neighbors|KNeighborsRegressor}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \weightsDescription{uniform}
  \item \algorithmDescription{auto}
  \item \leafSizeDescription{30}
  \item \metricDescription{minkowski}
  \item \pDescription{2}
    %TODO document metric_params
    %TODO document n_jobs?
\end{itemize}

%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Regressor}
\mbox{}

The \textit{Radius Neighbors Regressor} can be used in cases where the data
labels are continuous rather than discrete variables.
%
The label assigned to a query point is computed based on the mean of the labels
of its nearest neighbors.
%
It implements learning based on the neighbors within a fixed radius $r$ of the
query point, where $r$ is a floating-point value specified by the user.

\skltype{Radius Neighbors Regressor}{neighbors|RadiusNeighborsRegressor}
\begin{itemize}
  \item \radiusDescription{1.0}
  \item \weightsDescription{uniform}
  \item \algorithmDescription{auto}
  \item \leafSizeDescription{30}
  \item \metricDescription{minkowski}
  \item \pDescription{2}
    %TODO document metric_params
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Nearest Centroid Classifier}
\mbox{}

The \textit{Nearest Centroid classifier} is a simple algorithm that represents
each class by the centroid of its members.
%
It also has no parameters to choose, making it a good baseline classifier.
%
It does, however, suffer on non-convex classes, as well as when classes have
drastically different variances, as equal variance in all dimensions is assumed.

\skltype{Nearest Centroid Classifier}{neighbors|NearestCentroid}
\begin{itemize}
  \item \xmlNode{shrink\_threshold}, \xmlDesc{float, optional field}, defines
  the threshold for shrinking centroids to remove features.
  %
  \default{None}
  %
  %TODO document metric
\end{itemize}
%\subparagraph{Ball Tree}
%pass
%\subparagraph{K-D Tree}
%pass


The \textit{Quadratic Discriminant Analysis} is a classifier with a quadratic
decision boundary, generated by fitting class conditional densities to the data
and using Bayes' rule.
%
The model fits a Gaussian density to each class.

\skltype{Quadratic Discriminant Analysis Classifier}{qda|QDA}
\begin{itemize}
  \item \xmlNode{priors}, \xmlDesc{array-like (n\_classes), optional field},
  specifies the priors on the classes.
  %
  \default{None}
  \item \xmlNode{reg\_param}, \xmlDesc{float, optional field}, regularizes the
  covariance estimate as (1-reg\_param)*Sigma +
  reg\_param*Identity(n\_features).
  %
  \default{0.0}
  %
\end{itemize}
\zNormalizationNotPerformed{QDA}
 %%%%% ROM Model - SciKitLearn: Tree %%%%%%%
\paragraph{Tree}
\label{tree}

Decision Trees (DTs) are a non-parametric supervised learning method used for
classification and regression.
%
The goal is to create a model that predicts the value of a target variable by
learning simple decision rules inferred from the data features.
%
\begin{itemize}
  \item Some advantages of decision trees are:
  \item Simple to understand and to interpret.
  %
  Trees can be visualized.
  %
  \item Requires little data preparation.
  %
  Other techniques often require data normalization, dummy variables need to be
  created and blank values to be removed.
  %
  Note however, that this module does not support missing values.
  %
  \item The cost of using the tree (i.e., predicting data) is logarithmic in the
  number of data points used to train the tree.
  %
  \item Able to handle both numerical and categorical data.
  %
  Other techniques are usually specialized in analyzing datasets that have only
  one type of variable.
  %
  \item Able to handle multi-output problems.
  %
  \item Uses a white box model.
  %
  If a given situation is observable in a model, the explanation for the
  condition is easily explained by boolean logic.
  %
  By contrast, in a black box model (e.g., in an artificial neural network),
  results may be more difficult to interpret.
  %
  \item Possible to validate a model using statistical tests.
  %
  That makes it possible to account for the reliability of the model.
  %
  \item Performs well even if its assumptions are somewhat violated by the true
  model from which the data were generated.
  %
\end{itemize}
The disadvantages of decision trees include:
\begin{itemize}
  \item Decision-tree learners can create over-complex trees that do not
  generalise the data well.
  %
  This is called overfitting.
  %
  Mechanisms such as pruning (not currently supported), setting the minimum
  number of samples required at a leaf node or setting the maximum depth of the
  tree are necessary to avoid this problem.
  %
  \item Decision trees can be unstable because small variations in the data
  might result in a completely different tree being generated.
  %
  This problem is mitigated by using decision trees within an ensemble.
  %
  \item The problem of learning an optimal decision tree is known to be
  NP-complete under several aspects of optimality and even for simple concepts.
  %
  Consequently, practical decision-tree learning algorithms are based on
  heuristic algorithms such as the greedy algorithm where locally optimal
  decisions are made at each node.
  %
  Such algorithms cannot guarantee to return the globally optimal decision tree.
  %
  This can be mitigated by training multiple trees in an ensemble learner, where
  the features and samples are randomly sampled with replacement.
  %
  \item There are concepts that are hard to learn because decision trees do not
  express them easily, such as XOR, parity or multiplexer problems.
  %
  \item Decision tree learners create biased trees if some classes dominate.
  %
  It is therefore recommended to balance the dataset prior to fitting with the
  decision tree.
  %
\end{itemize}

\zNormalizationPerformed{tree-based}

In the following, all the tree-based algorithms available in RAVEN are reported.

%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Classifier}
\mbox{}
\\The \textit{Decision Tree Classifier} is a classifier that is based on the
decision tree logic.

\skltype{Decision Tree Classifier}{tree|DecisionTreeClassifier}
\begin{itemize}
  \item \criterionDescription{gini}
  \item \splitterDescription{best}
  \item \maxFeaturesDescription{None}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
    %TODO document min_weight_fraction_leaf
  \item \maxLeafNodesDescription{None}
    %TODO document class_weight
    %TODO document random_state
    %TODO document presort
\end{itemize}

%%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Regressor}
\mbox{}
\\The \textit{Decision Tree Regressor} is a Regressor that is based on the
decision tree logic.
%
\skltype{Decision Tree Regressor}{tree|DecisionTreeRegressor}
\begin{itemize}
  \item \criterionDescriptionDT{mse}
  \item \splitterDescription{best}
  \item \maxFeaturesDescription{None}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
    %TODO document min_weight_fraction_leaf
  \item \maxLeafNodesDescription{None}
    %TODO document random_state
    %TODO document presort
\end{itemize}

%%%%%%%%%%%%%%%%
\subparagraph{Extra Tree Classifier}
\mbox{}
\\The \textit{Extra Tree Classifier} is an extremely randomized tree classifier.
%
Extra-trees differ from classic decision trees in the way they are built.
%
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the max\_features randomly selected
features and the best split among those is chosen.
%
When max\_features is set 1, this amounts to building a totally random decision
tree.

\skltype{Extra Tree Classifier}{tree|ExtraTreeClassifier}

\begin{itemize}
  \item \criterionDescription{gini}
  \item \splitterDescription{random}
  \item \maxFeaturesDescription{auto}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
    %TODO document min_weight_fraction_leaf
  \item \maxLeafNodesDescription{None}
    %TODO document random_state
    %TODO document class_weight
  %
\end{itemize}

%%%%%%%%%%%%
\subparagraph{Extra Tree Regressor}
\mbox{}

The \textit{Extra Tree Regressor} is an extremely randomized tree regressor.
%
Extra-trees differ from classic decision trees in the way they are built.
%
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the max\_features randomly selected
features and the best split among those is chosen.
%
When max\_features is set 1, this amounts to building a totally random decision
tree.

\skltype{Extra Tree Regressor}{tree|ExtraTreeRegressor}

\begin{itemize}
  \item \criterionDescriptionDT{mse}
  \item \splitterDescription{random}
  \item \maxFeaturesDescription{auto}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
    %TODO document min_weight_fraction_leaf
  \item \maxLeafNodesDescription{None}
    %TODO document random_state
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Gaussian Process %%%%%%%
\paragraph{Gaussian Process}
\label{GP}
Gaussian Processes for Machine Learning (GPML) is a generic supervised learning
method primarily designed to solve regression problems.
%
The advantages of Gaussian Processes for Machine Learning are:
\begin{itemize}
  \item The prediction interpolates the observations (at least for regular
  correlation models).
  \item The prediction is probabilistic (Gaussian) so that one can compute
  empirical confidence intervals and exceedance probabilities that might be used
  to refit (online fitting, adaptive fitting) the prediction in some region of
  interest.
  \item Versatile: different linear regression models and correlation models can
  be specified.
  %
  Common models are provided, but it is also possible to specify custom models
  provided they are stationary.
  %
\end{itemize}
The disadvantages of Gaussian Processes for Machine Learning include:
\begin{itemize}
  \item It is not sparse.
  %
  It uses the whole samples/features information to perform the prediction.
  \item It loses efficiency in high dimensional spaces – namely when the
  number of features exceeds a few dozens.
  %
  It might indeed give poor performance and it loses computational efficiency.
  \item Classification is only a post-processing, meaning that one first needs
  to solve a regression problem by providing the complete scalar float precision
  output $y$ of the experiment one is attempting to model.
  %
\end{itemize}

\skltype{Gaussian Process Regressor}{GaussianProcess|GaussianProcess}

\begin{itemize}
  \item \xmlNode{regr}, \xmlDesc{string, optional field}, is a regression
  function returning an array of outputs of the linear regression functional
  basis.
  %
  The number of observations n\_samples should be greater than the size p of
  this basis.
  %
  Available built-in regression models are `constant,' `linear,' and
  `quadratic.'
  %
  \default{constant}
  \item \xmlNode{corr}, \xmlDesc{string, optional field}, is a stationary
  autocorrelation function returning the autocorrelation between two points $x$
  and $x'$.
  %
  Default assumes a squared-exponential autocorrelation model.
  %
  Built-in correlation models are `absolute\_exponential,'
  `squared\_exponential,' `generalized\_exponential,' `cubic,' and `linear.'
  %
  \default{squared\_exponential}
  \item \xmlNode{beta0}, \xmlDesc{float, array-like, optional field}, specifies
  the regression weight vector to perform Ordinary Kriging (OK).
  %
  \default{None}
  \item \xmlNode{storage\_mode}, \xmlDesc{string, optional field}, specifies
  whether the Cholesky decomposition of the correlation matrix should be stored
  in the class (storage\_mode = `full') or not (storage\_mode = `light').
  %
  \default{full}
  \item \verDescriptionA{False}
  \item \xmlNode{theta0}, \xmlDesc{float, array-like, optional field}, is an
  array with shape (n\_features, ) or (1, ).
  %
  This represents the parameters in the autocorrelation model.
  %
  If thetaL and thetaU are also specified, theta0 is considered as the starting
  point for the maximum likelihood estimation of the best set of parameters.
  %
  \default{[1e-1]}
  \item \xmlNode{thetaL}, \xmlDesc{float, array-like, optional field}, is an
  array with shape matching that defined by \xmlNode{theta0}.
  %
  Lower bound on the autocorrelation parameters for maximum likelihood
  estimation.
  %
  \default{None}
  \item \xmlNode{thetaU}, \xmlDesc{float, array-like, optional field}, is an
  array with shape matching that defined by \xmlNode{theta0}.
  %
  Upper bound on the autocorrelation parameters for maximum likelihood
  estimation.
  %
  \default{None}
  \item \xmlNode{normalize}, \xmlDesc{boolean, optional field}, if True, the
  input $X$ and observations $y$ are centered and reduced w.r.t. means and
  standard deviations estimated from the n\_samples observations provided.
  %
  \default{True}
  \item \xmlNode{nugget}, \xmlDesc{float, optional field},specifies a nugget
  effect to allow smooth predictions from noisy data.
  %
  The nugget is added to the diagonal of the assumed training covariance.
  %
  In this way it acts as a Tikhonov regularization in the problem.
  %
  In the special case of the squared exponential correlation function, the
  nugget mathematically represents the variance of the input values.
  %
  \default{10 * MACHINE\_EPSILON}
  \item \xmlNode{optimizer}, \xmlDesc{string, optional field}, specifies the
  optimization algorithm to be used.
  %
  Available optimizers are: 'fmin\_cobyla', 'Welch'.
  %
  \default{fmin\_cobyla}
  \item \xmlNode{random\_start}, \xmlDesc{integer, optional field}, sets the
  number of times the Maximum Likelihood Estimation should be performed from
  a random starting point.
  %
  The first MLE always uses the specified starting point (theta0), the next
  starting points are picked at random according to an exponential distribution
  (log-uniform on [thetaL, thetaU]).
  %
  \default{1}
  \item \xmlNode{random\_state}, \xmlDesc{integer, optional field}, is the seed
  of the internal random number generator.
  %
  \default{None}
  %
\end{itemize}

\zNormalizationNotPerformed{GaussianProcess}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
   <ROM name='aUserDefinedName' subType='SciKitLearn'>
     <Features>var1,var2,var3</Features>
     <Target>result1</Target>
     <SKLtype>linear_model|LinearRegression</SKLtype>
     <fit_intercept>True</fit_intercept>
     <normalize>False</normalize>
   </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Neural Network Models %%%%%%%
\paragraph{Neural Network Models}
\label{DNN}
It has been more than 70 years since Warren McCulloch and Water Pitts modeled the first
artificial neural network (ANN) that mimicked the way brains work. These days, deep learning
based on ANN is showing outstanding results for solving a wide variety of robotic tasks in
the areas of perception, planning, localization, and control.
%
\textbf{Multi-layer Perceptron (MLP)} is a supervised learning algorithm that can learn
a non-linear function approximator for either classifcation or regression. It is different
from logistic regression, in that between the input and output layer, there can be one
or more non-linear layers, called hidden layers.
%
The advantages of Multi-layer Perceptron are:
\begin{itemize}
  \item Capability to learn non-linear models
  \item Capability to learn models in real-time (online learning)
\end{itemize}
The disadvantages of Multi-layer Perceptron include:
\begin{itemize}
  \item MLP with hidden layers have a non-convex loss function where there exists more than
    one local minimum. Therefore different random weight initializations can lead to different
    validation accuracy.
  \item MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers
    and iterations.
  \item MLP is sensitive to feature scaling
\end{itemize}

\zNormalizationPerformed{Multi-layer Perceptron}

In the following, Multi-layer perceptron classification and regression algorithms available in RAVEN are reported.

%%%%%%%%%%%%%%%
\subparagraph{MLPClassifier}
\mbox{}
\\The \textit{MLPClassifier} implements a multi-layer perceptron algorithm that trains using \textbf{Backpropagation}
More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation.
For classification, it minimizes the Cross-Entropy loss function, and it supports multi-class classification.

\skltype{MLPClassifier}{neural\_network|MLPClassifier}
\begin{itemize}
  \item \hiddenLayerSizesMLPDescription{(100,)}
  \item \activationMLPDescription{`relu'}
  \item \solverMLPDescription{`adam'}
  \item \alphaMLPDescription{0.0001}
  \item \batchSizeMLPDescription{`auto'}
  \item \learningRateMLPDescription{`constant'}
  \item \learningRateInitMLPDescription{0.001}
  \item \powerTMLPDescription{0.5}
  \item \maxIterMLPDescription{200}
  \item \shuffleMLPDescription{True}
  \item \randomStateMLPDescription{None}
  \item \tolMLPDescription{1e-4}
  \item \verboseMLPDescription{False}
  \item \warmStartMLPDescription{False}
  \item \momentumMLPDescription{0.9}
  \item \nesterovsMomentumMLPDescription{True}
  \item \earlyStoppingMLPDescription{False}
  \item \validationFractionMLPDescription{0.1}
  \item \betaAMLPDescription{0.9}
  \item \betaBMLPDescription{0.999}
  \item \epsilonMLPDescription{1e-8}
\end{itemize}

%%%%%%%%%%%%%%%
\subparagraph{MLPRegressor}
\mbox{}
\\The \textit{MLPRegressor} implements a multi-layer perceptron algorithm that trains using \textbf{Backpropagation} with
no activation function in the output layer, which can also be seen as using the identity function as activation function.
Therefore, it uses the square error as the loss function, and the output is a set of continuous values.
\textit{MLPRegressor} also supports multi-output regression, in which a sample can have more than one target.

\skltype{MLPRegressor}{neural\_network|MLPRegressor}
\begin{itemize}
  \item \hiddenLayerSizesMLPDescription{(100,)}
  \item \activationMLPDescription{`relu'}
  \item \solverMLPDescription{`adam'}
  \item \alphaMLPDescription{0.0001}
  \item \batchSizeMLPDescription{`auto'}
  \item \learningRateMLPDescription{`constant'}
  \item \learningRateInitMLPDescription{0.001}
  \item \powerTMLPDescription{0.5}
  \item \maxIterMLPDescription{200}
  \item \shuffleMLPDescription{True}
  \item \randomStateMLPDescription{None}
  \item \tolMLPDescription{1e-4}
  \item \verboseMLPDescription{False}
  \item \warmStartMLPDescription{False}
  \item \momentumMLPDescription{0.9}
  \item \nesterovsMomentumMLPDescription{True}
  \item \earlyStoppingMLPDescription{False}
  \item \validationFractionMLPDescription{0.1}
  \item \betaAMLPDescription{0.9}
  \item \betaBMLPDescription{0.999}
  \item \epsilonMLPDescription{1e-8}
\end{itemize}

%%%% ROM Model - ARMA  %%%%%%%
\subsubsection{ARMA}
\label{subsubsec:arma}
The ARMA sub-type contains a single ROM type, based on an autoregressive moving average time series model with
Fourier signal processing, sometimes referred to as a FARMA.
%
ARMA is a type of time dependent model that characterizes the autocorrelation between time series data. The mathematic description of ARMA is given as
\begin{equation*}
x_t = \sum_{i=1}^p\phi_ix_{t-i}+\alpha_t+\sum_{j=1}^q\theta_j\alpha_{t-j},
\end{equation*}
where $x$ is a vector of dimension $n$, and $\phi_i$ and $\theta_j$ are both $n$ by $n$ matrices. When $q=0$, the above is
autoregressive (AR); when $p=0$, the above is moving average (MA).
While plans for optimizing $p$ and $q$ are under consideration, currently setting $Pmin$ and $Pmax$ to the
same value is required, and similarly for $Qmin$ and $Qmax$.
%The user is allowed to provide upper and lower limits for $p$
%and $q$ (see below), and the training process will choose the optimal $p$ and $q$ that fall into the user-specified range.
When
training an ARMA, the input needs to be a synchronized HistorySet. For unsynchronized data, use PostProcessor methods to
synchronize the data before training an ARMA.

The ARMA model implemented allows an option to use Fourier series to detrend the time series before fitting to ARMA model to
train. The Fourier trend will be stored in the trained ARMA model for data generation. The following equation
describes the detrending
process.
\begin{equation*}
x_t = y_t - \sum_m\left\{\sum_{k=1}^{K_m}a_k\sin(2\pi kf_mt)+\sum_{k=1}^{K_m}b_k\cos(2\pi kf_mt)\right\},
\end{equation*}
where $K_m$ and $f_m$ are user-defined parameters.

By default, each target in the training will be considered independent and have an unique ARMA for each
target.  Correlated targets can be specified through the \xmlNode{correlate} node, at which point
the correlated targets will be trained together using a vector ARMA (or VARMA). Due to limitations in
the VARMA, in order to seed samples the VARMA must be trained with the node \xmlNode{seed}, which acts
independently from the global random seed used by other RAVEN entities.

Both the ARMA and VARMA make use of the \texttt{statsmodels} python package.

%
In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be \xmlString{ARMA} (see the example
below).
%
\subnodeIntro

\begin{itemize}
  \item \xmlNode{pivotParameter}, \xmlDesc{string, required field}, defines the pivot variable (e.g., time) that is non-decreasing in
  the input HistorySet.
  \item \xmlNode{Features}, \xmlDesc{comma separated string, required field}, defines the features (e.g., scaling). Note that only
  one feature is allowed for \xmlString{ARMA} and in current implementation this is used for evaluation only.
  \item \xmlNode{Target}, \xmlDesc{comma separated string, required field}, defines the variable(s) of the
    time series.  Should include the pivot parameter (or Index).
  \item \xmlNode{correlate}, \xmlDesc{comma separated string, optional field}, indicates the listed variables
    should be considered as influencing each other, and trained together instead of independently.  This node
    can only be listed once, so all variables that are desired for correlation should be included.  \nb The
    correlated VARMA takes notably longer to train than the independent ARMAs for the same number of targets.
  \item \xmlNode{seed}, \xmlDesc{integer, optional field}, provides seed for ONLY the VARMA sampling.  Has no
    effect on ARMA single-target sampling or other RAVEN entities.  Must be provided before training; it cannot
    be changed once trained.
  \default{True}
  \item \xmlNode{reseedCopies}, \xmlDesc{boolean, optional field}, if True then whenever the ARMA is copied, a
    random reseeding will be performed to ensure different histories.
  \default{True}
  \item \xmlNode{Pmax}, \xmlDesc{integer, optional field}, defines the maximum value of $p$.
  \default{3}
  \item \xmlNode{Pmin}, \xmlDesc{integer, optional field}, defines the minimum value of $p$.
  \default{0}
  \item \xmlNode{Qmax}, \xmlDesc{integer, optional field}, defines the maximum value of $q$.
  \default{3}
  \item \xmlNode{Qmin}, \xmlDesc{integer, optional field}, defines the minimum value of $q$.
  \default{0}
  \item \xmlNode{Fourier}, \xmlDesc{integers, optional field}, must be positive integers. This defines the based period (with unit of second) that would be used for Fourier detrending, i.e., this field defines $1/f_m$ in the above equation. When this filed is not specified, the ARMA considers no Fourier detrend.
  \item \xmlNode{FourierOrder}, \xmlDesc{integers, optional field}, must be positive integers. The number of integers specified in this field should be exactly same as the number of base periods specified in the node \xmlNode{Fourier}. This field defines $K_m$ in the above equation.
  \item \xmlNode{outTruncation}, \xmlDesc{string, optional field}, defines whether and how the output time series is truncated. Currently available options are: positive, negative.
  \default{None}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
    <ROM name='aUserDefinedName' subType='ARMA'>
      <pivotParameter>Time</pivotParameter>
      <Features>scaling</Features>
      <Target>Speed1,Speed2</Target>
      <Pmax>5</Pmax>
      <Pmin>1</Pmin>
      <Qmax>4</Qmax>
      <Qmin>1</Qmin>
      <Fourier>604800,86400</Fourier>
      <FourierOrder>2, 4</FourierOrder>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}

%%%% ROM Model - PolyExponential  %%%%%%%
\subsubsection{PolyExponential}
\label{subsubsec:polyexponential}
The PolyExponential sub-type contains a single ROM type, aimed to construct a time-dependent (or any other monotonic
variable) surrogate model based on polynomial sum of exponential term. This surrogate have the form:
%
\begin{equation}
  SM(X,z) = \sum_{i=1}^{N} P_{i}(X) \times \exp ( - Q_{i}(X) \times z )
\end{equation}
where:
\begin{itemize}
  \item $\mathbf{z}$ is the independent  monotonic variable (e.g. time)
  \item $\mathbf{X}$  is the vector of the other independent (parametric) variables  (Features)
  \item $\mathbf{P_{i}}(X)$ is a polynomial of rank M function of the parametric space X
  \item  $\mathbf{Q_{i}}(X)$ is a polynomial of rank M function of the parametric space X
  \item  $\mathbf{N}$ is the number of requested exponential terms.
\end{itemize}
It is crucial to notice that this model is quite suitable for FOMs whose drivers are characterized by an exponential-like behavior.
In addition, it is important to notice that the exponential terms' coefficients are computed running a genetic-algorithm optimization
problem, which is quite slow in case of increasing number of ``numberExpTerms''.
%
In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be set equal to \xmlString{PolyExponential} (see the example
below).
%
\subnodeIntro

\begin{itemize}
  \item \xmlNode{pivotParameter}, \xmlDesc{string, optional field}, defines the pivot variable (e.g., time) that represents the
  independent monotonic variable
  \default{time}
  \item \xmlNode{Features}, \xmlDesc{comma separated string, required field}, defines the features (i.e. input parameters) of this
  model
  \item \xmlNode{Target}, \xmlDesc{comma separated string, required field}, defines output FOMs that are going to be predicted
  \item \xmlNode{numberExpTerms}, \xmlDesc{integer, optional field}, the number of exponential terms to be used ($N$ above)
   \default{3}
  \item \xmlNode{coeffRegressor}, \xmlDesc{string, optional field}, defines which regressor to use for interpolating the
   exponential coefficient. Available are ``spline'',``poly'' and ``nearest''.
    \default{spline}
  \item \xmlNode{polyOrder}, \xmlDesc{integer, optional field}, the polynomial order to be used for interpolating the exponential
  coefficients. Only valid in case of  \xmlNode{coeffRegressor} set to ``poly''.
   \default{2}
  \item \xmlNode{tol}, \xmlDesc{float, optional field}, relative tolerance of the optimization problem (differential evolution optimizer)
   \default{1e-3}
  \item \xmlNode{maxNumberIter}, \xmlDesc{integer, optional field}, maximum number of iterations (generations) for the
  optimization problem  (differential evolution optimizer)
   \default{5000}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
   <ROM name='PolyExp' subType='PolyExponential'>
     <Target>time,decay_heat, xe135_dens</Target>
     <Features>enrichment,bu</Features>
     <pivotParameter>time</pivotParameter>
     <numberExpTerms>5</numberExpTerms>
     <max_iter>1000000</max_iter>
     <tol>0.000001</tol>
  </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
Once the ROM is trained (\textbf{Step} \xmlNode{RomTrainer}), its coefficients can be exported into an XML file
via an \xmlNode{OutStream} of type \xmlAttr{Print}. The following variable/parameters can be exported (i.e. \xmlNode{what} node
in \xmlNode{OutStream} of type \xmlAttr{Print}):
\begin{itemize}
  \item \xmlNode{expTerms}, see XML input specifications above, inquired pre-pending the keyword ``output|'' (e.g. output| expTerms)
  \item \xmlNode{coeffRegressor}, see XML input specifications above
  \item \xmlNode{polyOrder}, see XML input specifications above
  \item \xmlNode{features}, see XML input specifications above
  \item \xmlNode{timeScale}, XML node containing the array of the training time steps values
  \item \xmlNode{coefficients}, XML node containing the exponential terms' coefficients for each realization
\end{itemize}


 See the following example:
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <OutStreams>
    ...
    <Print name = 'dumpAllCoefficients'>
      <type>xml</type>
      <source>PolyExp</source>
      <!--
        here the <what> node is omitted. All the available params/coefficients
        are going to be printed out
      -->
    </Print>
    <Print name = 'dumpSomeCoefficients'>
      <type>xml</type>
      <source>PolyExp</source>
      <what>coefficients,timeScale</what>
    </Print>
    ...
  </OutStreams>
  ...
</Simulation>
\end{lstlisting}

%%%% ROM Model - DMD  %%%%%%%
\subsubsection{DMD}
\label{subsubsec:polyexponential}
The DMD sub-type contains a single ROM type, aimed to construct a time-dependent (or any other monotonic
variable) surrogate model based on Dynamic Mode Decomposition (ref. \cite{Schmid2010DMD} and \cite{Vega2017HODMD}).
This surrogate is aimed to perform a ``dimensionality reduction regression'', where, given time series (or any monotonic-dependent
variable) of data, a set of modes each of which is associated with a fixed oscillation frequency and decay/growth rate is computed
in order to represent the data-set.
%
In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be set equal to \xmlString{DMD} (see the example
below).
%
\subnodeIntro

\begin{itemize}
  \item \xmlNode{dmdType}, \xmlDesc{string, optional field}, the type of Dynamic Mode Decomposition to apply. Available are:
   \begin{itemize}
     \item \textit{dmd}, for classical DMD
     \item \textit{hodmd}, for high order DMD.
   \end{itemize}
   \default{dmd}
  \item \xmlNode{pivotParameter}, \xmlDesc{string, optional field}, defines the pivot variable (e.g., time) that represents the
  independent monotonic variable
  \default{time}
  \item \xmlNode{Features}, \xmlDesc{comma separated string, required field}, defines the features (i.e. input parameters) of this
  model
  \item \xmlNode{Target}, \xmlDesc{comma separated string, required field}, defines output FOMs that are going to be predicted
  \item \xmlNode{rankSVD}, \xmlDesc{integer, optional field}, defines the truncation rank to be used for the SVD.
     Available options are:
     \begin{itemize}
     \item \textit{-1}, no truncation is performed
     \item \textit{0}, optimal rank is internally computed
     \item \textit{>1}, this rank is going to be used for the truncation
   \end{itemize}
   \default{-1}
  \item \xmlNode{energyRankSVD}, \xmlDesc{float, optional field},  energy level ($0.0 < float < 1.0$) used to compute the rank such
    as computed rank is the number of the biggest singular values needed to reach the energy identified by
    \xmlNode{energyRankSVD}. This node has always priority over  \xmlNode{rankSVD}
    \default{None}
  \item \xmlNode{rankTLSQ}, \xmlDesc{integer, optional field}, $int > 0$ that defines the truncation rank to be used for the total
   least square problem. If not inputted, no truncation is applied
   \default{None}
   \item \xmlNode{exactModes}, \xmlDesc{bool, optional field}, True if the exact modes need to be computed (eigenvalues and
   eigenvectors),   otherwise the projected ones (using the left-singular matrix after SVD).
  \default{True}
  \item \xmlNode{optimized}, \xmlDesc{float, optional field}, True if the amplitudes need to be computed minimizing the error
   between the modes and all the time-steps or False, if only the 1st timestep only needs to be considered
   \default{True}

\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
   <ROM name='DMD' subType='DMD'>
      <Target>time,totals_watts, xe135_dens</Target>
      <Features>enrichment,bu</Features>
      <dmdType>dmd</dmdType>
      <pivotParameter>time</pivotParameter>
      <rankSVD>0</rankSVD>
      <rankTLSQ>5</rankTLSQ>
      <exactModes>False</exactModes>
      <optimized>True</optimized>
    </ROM
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
Once the ROM  is trained (\textbf{Step} \xmlNode{RomTrainer}), its parameters/coefficients can be exported into an XML file
via an \xmlNode{OutStream} of type \xmlAttr{Print}. The following variable/parameters can be exported (i.e. \xmlNode{what} node
in \xmlNode{OutStream} of type \xmlAttr{Print}):
\begin{itemize}
  \item \xmlNode{rankSVD}, see XML input specifications above
  \item \xmlNode{energyRankSVD}, see XML input specifications above
  \item \xmlNode{rankTLSQ}, see XML input specifications above
  \item \xmlNode{exactModes}, see XML input specifications above
  \item \xmlNode{optimized}, see XML input specifications above
  \item \xmlNode{features}, see XML input specifications above
  \item \xmlNode{timeScale}, XML node containing the array of the training time steps values
  \item \xmlNode{dmdTimeScale}, XML node containing the array of time scale in the DMD space (can be used as mapping
  between the  \xmlNode{timeScale} and \xmlNode{dmdTimeScale})
  \item \xmlNode{eigs}, XML node containing the eigenvalues (imaginary and real part)
  \item \xmlNode{amplitudes}, XML node containing the amplitudes (imaginary and real part)
  \item \xmlNode{modes}, XML node containing the dynamic modes (imaginary and real part)
\end{itemize}


 See the following example:
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <OutStreams>
    ...
    <Print name = 'dumpAllCoefficients'>
      <type>xml</type>
      <source>DMD</source>
      <!--
        here the <what> node is omitted. All the available params/coefficients
        are going to be printed out
      -->
    </Print>
    <Print name = 'dumpSomeCoefficients'>
      <type>xml</type>
      <source>PolyExp</source>
      <what>eigs,amplitudes,modes</what>
    </Print>
    ...
  </OutStreams>
  ...
</Simulation>
\end{lstlisting}
