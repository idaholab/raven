\section{RunInfo}
\label{sec:RunInfo}
In the \textbf{RunInfo} block, the user specifies how the overall computation
should be run.
%
%There are several settings which can be inputted, in order to define how to
%drive the calculation and set up, when needed, particular settings for the
%machine the code needs to run on (queue system if not PBS, etc.).
This block accepts several input settings that define how to drive the
calculation and set up, when needed, particular settings for the machine the
code needs to run on (e.g. queueing system, if not PBS, etc.).
%
In the following subsections, we explain all the keywords and how to use them in
detail.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% RUN INFO CALCULATION FLOW %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RunInfo: Input of Calculation Flow}
\label{subsec:runinfoCalcFlow}
This sub-section contains the information regarding the XML nodes used to define
the settings of the calculation flow that is being performed through RAVEN:

\begin{itemize}
%%%%%% WORKING DIR
\item \xmlNode{WorkingDir}, \xmlDesc{string, required field},
  specifies the absolute or relative (with respect to the location
  where the xml file is located) path to a directory that will store
  all the results of the calculations and where RAVEN looks for the
  files specified in the block \xmlNode{Files}.  If
  \texttt{runRelative='True'} is used as an attribute, then it will be
  relative to where raven is run.
%
\default{None}

%%%%%% Files -> moved to own node
%\item \xmlNode{Files}, \xmlDesc{comma separated string, optional field}, lists
%the paths to any files required by the code. If no file is needed (either as input or output),
%this node could be omitted.
%string from the \emph{WorkingDir}.

%%%%%% REMOTE RUN COMMAND
\item \xmlNode{RemoteRunCommand}, \xmlDesc{string, optional field},
  specifies the absolute or relative (with respect to the framework
  directory) path to a command that can be used on a remote machine to
  execute a command.  The command is passed in as the environmental
  variable COMMAND.
%
  \default{raven\_qsub\_command.sh}

  %%%%%% NODE PARAMETER

\item \xmlNode{NodeParameter}, \xmlDesc{string, optional field},
  specifies the flag used to specify a node file for the MPIExec command.  This will be followed by a file with the nodes that a single batch will run on.
  %
  \default{-f}

  %%%%%% MPI EXEC
\item \xmlNode{MPIExec}, \xmlDesc{string, optional field}, specifies the command used to run mpi.  This will be followed by the \xmlNode{NodeParameter} and then the node file and then the code command.
  %
  \default{mpiexec}

%%%%%% BATCH SIZE
\item \xmlNode{batchSize}, \xmlDesc{integer, optional field},
  specifies the number of parallel runs executed simultaneously (e.g.,
  the number of driven code instances, e.g. RELAP5-3D, that RAVEN will
  spawn at the same time).  Each parallel run will use
  \texttt{NumThreads} $*$ \texttt{NumMPI} cores.
%
\default{1}

%%%%%% Sequence
\item \xmlNode{Sequence}, \xmlDesc{comma separated string, required field}, is
an ordered list of the step names that RAVEN will run (see
Section~\ref{sec:steps}).

%%%%%% JOBNAME
\item \xmlNode{JobName}, \xmlDesc{string, optional field}, specifies
the name to use for the job when submitting to a pbs queue.  Acceptable characters
include alphanumerics as well as ``-'' and ``\_''.  If more than 15
characters are provided, RAVEN will truncate it using a hyphen between the first
10 and last 4 character, i.e., ``1234567890abcdefgh'' will be truncated to
``1234567890-efgh''.
%
\default{raven\_qsub}

%%%%%% printInput
\item \xmlNode{printInput}, \xmlDesc{string, optional field}, if provided, indicates RAVEN should print out a
  duplicate of the input file.  If the provided text is \xmlString{false}, or the node is not provided, then
  no duplicate will be printed.  If the node is provided but no name specified, it will use the default name.
  Otherwise, the file will be written in the working directory as \texttt{name\_provided.xml}.
  \default{duplicated\_input.xml}
%

%%%%%% NumThreads
\item \xmlNode{NumThreads}, \xmlDesc{integer, optional field}, can be used to
specify the number of threads RAVEN should associate when running the driven
software.
%
For example, if RAVEN is driving a code named ``FOO,'' and this code has
multi-threading support, this block is used to specify how many threads each
instance of FOO should use (e.g. ``\texttt{FOO --n-threads=N}'' where \texttt{N}
 is the number of threads).
%
\default{1 (or None when the driven code does not have multi-threading
support)}

%%%%%% NumMPI
\item \xmlNode{NumMPI}, \xmlDesc{integer, optional field}, can be used to
specify the number of MPI CPUs RAVEN should associate when running the driven
software.
%
For example, if RAVEN is driving a code named ``FOO,'' and this code has MPI
support, this block specifies how many MPI CPUs each instance of FOO should use
(e.g. ``\texttt{mpiexec FOO -np N}'' where \texttt{N} is the number of CPUs).
%
\default{1 (or None when the driven code does not have MPI support)}

%%%%%% totalNumCoresUsed
\item \xmlNode{totalNumCoresUsed}, \xmlDesc{integer, optional field}, is the
global number of CPUs RAVEN is going to use for performing the calculation.
%
When the driven code has MPI and/or multi-threading support and the user
specifies \texttt{NumThreads} $> 1$  and \texttt{NumMPI} $> 1$, then
\texttt{totalNumCoresUsed} is set according to the following formula:\\
\texttt{totalNumCoresUsed} = \texttt{NumThreads} $*$ \texttt{NumMPI} $*$
\texttt{batchSize}.
%
\default{1}

%%%%%% internalParallel
\item \xmlNode{internalParallel}, \xmlDesc{boolean, optional field}, is a boolean
flag that controls the type of parallel implementation needs to be used for Internal
Objects (e.g. ROMs, External Models, PostProcessors, etc.).
If this flag is set to:
\begin{itemize}
 \item  \textbf{\texttt{False}}, the internal parallelism is employed using multi-threading (i.e. 1 processor, multiple threads equal to the \xmlNode{batchSize}).
\\\nb This ``parallelism mode'' runs multiple instances of the Model in a single processor. If the evaluation of the model is memory intensive (i.e. it uses a lot of memory) or computational intensive (i.e. a lot of computation operations evolving in a $CPUt  \approx >  0.1\frac{sec}{evaluation}$) the single processor might get over-loaded determining a degradation of performance. In such cases, the internal parallelism needs to be used (see the following);
 \item  \textbf{\texttt{True}}, the internal parallelism is employed using a internally-developed multi-processor approach (i.e. \xmlNode{batchSize} processors, 1 single thread). This approach works for both Shared Memory Systems (e.g. PC, laptops, workstations, etc.) and Distributed Memory Machines (e.g. High Performance Computing Systems, etc.). 
 \\\nb This ``parallelism mode'' runs multiple instances of the Model in multiple processors. Since the parallelism is employed
 in Python, some overhead is present. This ``mode'' needs to be used when: 
 \begin{itemize}
  \item the Model evaluation is memory intensive (i.e. the multi-threading approach will cause the over-load of a single processor);
  \item the Model evaluation is computation intensive (i.e. $CPUt  \approx >  0.1\frac{sec}{evaluation}$).
 \end{itemize}
\end{itemize}
%
\default{False}



%%%%%% precommand
\item \xmlNode{precommand}, \xmlDesc{string, optional field}, specifies
a command that needs to be inserted before the actual command that is used to
run the external model (e.g., \texttt{mpiexec -n 8 precommand
./externalModel.exe (...)}).
Note that the precommand as well as the postcommand are ONLY applied to
execution commands flagged as ``parallel'' within the code interface.
%
\default{None}

%%%%%% postcommand
\item \xmlNode{postcommand}, \xmlDesc{string, optional field},
specifies a command that needs to be appended after the actual command that is
used to run the external model (e.g., \texttt{mpiexec -n 8  ./externalModel.exe
(...) postcommand}).
Note that the postcommand as well as the precommand are ONLY applied to
execution commands flagged as ``parallel'' within the code interface.
%
\default{None}

%%%%%% clusterParameters
\item \xmlNode{clusterParameters}, \xmlDesc{string, optional field},
  specifies extra parameters to be used with the cluster submission
  command.  For example, if qsub is used to submit a command, then these
  parameters will be used as extra parameters with the qsub command.
  This can be repeated multiple times as needed and they will all be passed
  to the cluster submission command.
%
\default{None}

%%%%%% MaxLogFileSize
\item \xmlNode{MaxLogFileSize}, \xmlDesc{integer, optional field}.
specifies the maximum size of the log file in bytes. Every time RAVEN drives a
code/software, it creates a logfile of the code's screen output.
%
\default{$\infty$}

\TODO{\nb This flag is not implemtend yet.}

%%%%%% deleteOutExtension
\item \xmlNode{deleteOutExtension}, \xmlDesc{comma separated string, optional
field}, specifies, if a run of an external model has not failed, which output
files should be deleted by their extension (e.g.,
\xmlNode{deleteOutExtension}\texttt{txt,pdf}\xmlNode{/deleteOutExtension} will
delete all generated txt and pdf files).
\nb This flag is only active for Models of type ``Code''.
%
\default{None}

%%%%%% delSucLogFiles
\item \xmlNode{delSucLogFiles}, \xmlDesc{boolean, optional field}, when True and
the run of an external model has not failed (return code = 0), deletes
the associated log files.
\nb This flag is only active for Models of type ``Code''.
%
\default{False}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% RUN INFO QUEUE MODES %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RunInfo: Input of Queue Modes}
\label{subsec:runinfoModes}
In this sub-section, all of the keywords (XML nodes) for setting the queue
system are reported.
\begin{itemize}
%%%%%% MODE
\item \xmlNode{mode}, \xmlDesc{string, optional field}, can specify which kind
of protocol the parallel enviroment should use.
%
RAVEN currently supports one pre-defined ``mode'':
  \begin{itemize}
    \item \textbf{mpi}: this ``mode'' uses \texttt{mpiexec} to distribute the
      running program; more information regarding this protocol can be found
      in~\cite{MPI}.
      Mode ``MPI''  can either generate a \texttt{qsub} command or can execute
      on selected nodes.
      In order to make the ``mpi'' mode generate a \texttt{qsub} command, an
      additional keyword (xml sub-node) needs to be specified:
         \begin{itemize}
         \item If RAVEN is executed in the HEAD node of an HPC system using
           \cite{PBS}, the user needs to input a sub-node, \xmlNode{runQSUB},
           right after the specification of the mpi mode (i.e.\\
             \xmlNode{mode}\texttt{mpi}\xmlNode{runQSUB/}\xmlNode{/mode}).
             If the keyword is provided, RAVEN generates a \texttt{qsub}
             command, instantiates itself, and submits itself to the queue
             system.
           \item If the user decides to execute RAVEN from an ``interactive
             node'' (a certain number of nodes that have been reserved in
             interactive PBS mode), RAVEN, using the ``mpi'' system, is going to
             utilize the reserved resources (CPUs and nodes) to distribute the
             jobs, but, will not generate a \texttt{qsub} command.
         \end{itemize}
     When the user decides to run in ``mpi'' mode without making RAVEN generate
     a \texttt{qsub} command, different options are available:
      \begin{itemize}
           \item If the user decides to run on the local machine (either in
             local desktop/workstation or a remote machine), no additional
             keywords are needed (i.e.\\
             \xmlNode{mode}\texttt{mpi}\xmlNode{/mode}).
           \item If the user is running on multiple nodes, the node ids have
             to be specified:
           \begin{itemize}
              \item the node ids can be specified in an external text file
                (node ids separated by blank space).
                This file needs to be provided in the XML node \xmlNode{mode},
                introducing a sub-node named \xmlNode{nodefile} (e.g.\\
\xmlNode{mode}\texttt{mpi}\xmlNode{nodefile}\texttt{/tmp/nodes}\xmlNode{/nodefile}\xmlNode{/mode}).
              \item the node ids can be contained in an enviromental variable
                (node ids separated by blank space).
                This variable needs to be provided in the \xmlNode{mode} XML
                node, introducing a sub-node named \xmlNode{nodefileenv} (e.g.\\
\xmlNode{mode}\texttt{mpi}\xmlNode{nodefileenv}\texttt{NODEFILE}\xmlNode{/nodefileenv}\xmlNode{/mode>}).
                \item If none of the above options are used, RAVEN will attempt
                  to find the nodes' information in the enviroment variable
                  \texttt{PBS\_NODEFILE}.
           \end{itemize}
         \item The cores needed can be specified manually with the
           \xmlNode{coresneeded}.  This is directly used in the
           \texttt{qsub} command select statement.
         \item The max memory needed can be specified with the
           \xmlNode{memory} XML node.  This will be used in the
           \texttt{qsub} command select statement.
         \item The placement can be specified with the \xmlNode{place}
           XML node.  This will be used in the \texttt{qsub} place
           statement.
         \item There is a ``mpilegacy'' mode.  This probably will be removed in the future.  In this mode exec can be forced to run on one shared memory node with the \xmlNode{NoSplitNode}.  If this is present, the splitting apart of the batches will put each batch on one shared memory node.  Without \xmlNode{NoSplitNode}, they can be split across nodes.  There is an option \xmlAttr{maxOnNode} which puts at most \xmlAttr{maxOnNode} number of mpi processes on one node.  \xmlNode{NoSplitNode} can cause processes to not be placed, so \xmlNode{NoSplitNode} should not be used unless needed.  If limiting the number of mpi processes on one node is desired without forcing them to only run on one node, \xmlNode{LimitNode} can be used.  Both \xmlNode{NoSplitNode} and  \xmlNode{LimitNode} can have a \xmlAttr{noOverlap} which prevents multiple batches from running on a single node.
         \end{itemize}
         In addition, this flag activates the remote (PBS) execution of internal Models (e.g. ROMs,
         ExternalModels, PostProcessors, etc.). If this node is not present, the internal Models
           are run using a multi-threading approach (i.e. master processor, multiple parallel threads)
   \end{itemize}

%Both methods can submit a qsub command or can be run from an already submitted interactive qsub command:
%     \begin{itemize}
%          \item Mode ``MPI'' needs an additional keyword (XML sub-node) in order to understand when it needs to generate the ``qsub'' commnad:
%         \begin{itemize}
%           \item If RAVEN is executed in the HEAD node of an HPC system, the user needs to input a sub-node, $<runQSUB/>$, right after the specification of the mpi mode (i.e. $<mode>mpi<runQSUB/></mode>$). If the keyword is provided, RAVEN generates the ``qsub'' command, instantiates and submits itself to the queue system;
%           \item If the user decides to execute RAVEN from an ``interactive node'' (a certain number of nodes that have been reserved in interactive PBS mode), RAVEN, using the ``mpi'' system, is going to utilize the reserved resources (cpus and nodes) to distribute the jobs, but, obviously, it's not going to generate the ``qsub'' command.
%         \end{itemize}
%     \end{itemize}
%     In the mode ``mpi''  Mode ``MPI'' can be used without any PBS support.

%%%%%% CUSTOM MODE
\item \xmlNode{CustomMode}, \xmlDesc{xml node, optional field}, is an
xml node where ``advanced'' users can implement newer ``modes.''
%
Please refer to sub-section~\ref{subsec:runinfoadvanced} for advanced users.

%%%%%% QUEUE SOFTWARE
\item \xmlNode{queueingSoftware}, \xmlDesc{string, optional field}.
RAVEN has support for the PBS queueing system. If the platform provides a
different queueing system, the user can specify its name here (e.g., PBS
PROFESSIONAL, etc.).
%
\default{PBS PROFESSIONAL}

%%%%%% EXPECTED TIME
\item \xmlNode{expectedTime}, \xmlDesc{colum separated string, optional field
(mpi or custom mode)}, specifies the time the whole calculation is expected to
last.
%
The syntax of this node is \textit{hours:minutes:seconds} (e.g.
\texttt{40:10:30} equals $40$ hours, $10$ minutes, $30$ seconds). After this
period of time, the HPC system will automatically stop the simulation (even if
the simulation is not completed). It is preferable to rationally overestimate
the needed time.
%
\default{10:00:00} (10 hours.)
\end{itemize}

\subsection{RunInfo: Example Cluster Usage}
\label{subsec:runinfoclusterexample}

For this example, we have a PBSPro cluster, and there are thousands of
node, and each node has 4 processors that share memory.  There are a
couple different ways this can be used.  One way is to use interactive
mode and have a RunInfo block:

\begin{lstlisting}[style=XML]
<RunInfo>
    <WorkingDir>.</WorkingDir>
    <Sequence>FirstMRun</Sequence>
    <batchSize>3</batchSize>
    <NumThreads>4</NumThreads>
    <mode>mpi</mode>
    <NumMPI>2</NumMPI>
</RunInfo>
\end{lstlisting}

Then the commands can be used:

\begin{lstlisting}[language=bash]
#Note: select=NumMPI*batchSize, ncpus=NumThreads
qsub -l select=6:ncpus=4:mpiprocs=1 -l walltime=10:00:00 -I
#wait for processes to be allocated and interactive shell to start

#Switch to the correct directory
cd $PBS_O_WORKDIR

#Load the module with the raven libraries
module load raven-devel-gcc

#Start Raven
python  ../../framework/Driver.py test_mpi.xml
\end{lstlisting}

Alternatively, RAVEN can be asked to submit the qsub directory.  With
this, the RunInfo is:

\begin{lstlisting}[style=XML]
<RunInfo>
    <WorkingDir>.</WorkingDir>
    <Sequence>FirstMQRun</Sequence>
    <batchSize>3</batchSize>
    <NumThreads>4</NumThreads>
    <mode>
      mpi
      <runQSUB/>
    </mode>
    <NumMPI>2</NumMPI>
    <expectedTime>10:00:00</expectedTime>
</RunInfo>
\end{lstlisting}

In this case, the command run from the cluster submit node:

\begin{lstlisting}[language=bash]
python ../../framework/Driver.py test_mpiqsub_local.xml
\end{lstlisting}

%\begin{itemize}
%\item $<WorkingDir>$\textbf{\textit{, string, required field.}} in this block the user needs to specify the absolute or relative (with respect to the location where RAVEN is run from) path to a directory that is going to be used to store all the results of the calculations and where RAVEN looks for the files specified in the block $<Files>$. \textit{Default = None};
%
%
%
%\item $<CustomMode>$\textbf{\textit{, XML node, optional field.}} In this XML node, the ``advanced'' users can implement a newer ``mode''. Please refer to sub-section~\ref{subsec:runinfoadvanced} for advanced users.
%
%
%
%\item $<NumNode>$\textbf{\textit{, integer, optional field.}}  this XML node is used to specify the number of nodes RAVEN should request when running in High Performance Computing (HPC) systems. \textit{Default = None};
%
%\item $<batchSize>$\textbf{\textit{, integer, required field.}}. This parameter specifies the number of parallel runs need to be run simultaneously (e.g., the number of driven code instances, e.g. RELAP5-3D, that RAVEN will spoon at the same time). \textit{Default = 1};
%
%\item $<NumThreads>$\textbf{\textit{, integer, optional field.}} this section can be used to specify the number of threads RAVEN should associate when running the driven software. For example, if RAVEN is driving a code named "FOO", and this code has multi-threading support, in here the user specify how many threads each instance of FOO should use (e.g. FOO --n-threads=$NumThreads$). \textit{Default = 1 (or None when the driven code does not have multi-threading support)};
%
%\item $<totalNumCoresUsed>$\textbf{\textit{, integer, optional field.}}  global number of cpus RAVEN is going to use for performing the calculation. When the driven code has MPI and/or  Multi-threading support and the user decides to input $NumThreads > 1$  and $NumMPI > 1$, the totalNumCoresUsed = NumThreads*NumMPI*batchSize. \textit{Default = 1};
%
%\item $<NumMPI>$\textbf{\textit{, integer, optional field.}}  this section can be used to specify the number of MPI cpus RAVEN should associate when running the driven software. For example, if RAVEN is driving a code named "FOO", and this code has MPI support, in here the user specifies how many mpi cpus each instance of FOO should use (e.g. mpiexec FOO -np $NumMPI$). \textit{Default = 1 (or None when the driven code does not have MPI support)};
%
%\item $<precommand>$\textbf{\textit{, string, optional field.}} in here the user can specifies a command that needs to be inserted before the actual command that is used to run the external model (e.g., mpiexec -n 8 $precommand$ ./externalModel.exe (...)). \textit{Default = None};
%
%\item $<postcommand>$\textbf{\textit{, string, optional field.}} in here the user can specifies a command that needs to be appended after the actual command that is used to run the external model (e.g., mpiexec -n 8  ./externalModel.exe (...) $postcommand$). \textit{Default = None};
%
%\item $<MaxLogFileSize>$\textbf{\textit{, integer, optional field.}}  every time RAVEN drives a code/software, it creates a logfile of the code screen output. In this block, the user can input the maximum size of log file in bytes. \textit{Defautl = Inf}. NB. This flag is not implemtend yet;
%
%\item $<deleteOutExtension>$\textbf{\textit{, comma separated string, optional field.}} if a run of an external model has not failed delete the outut files with the listed extension (e.g., $<deleteOutExtension>txt,pdf</deleteOutExtension>$). \textit{Default = None}.
%
%\item $<delSucLogFiles>$\textbf{\textit{, boolean, optional field.}} if a run of an external model has not failed (return code = 0), delete the associated log files. \textit{Default = False};
%
%\item $<Files>$\textbf{\textit{, comma separated string, required field.}} these are the paths to the files required by the code, string from the $WorkingDir$;
%
%\item $<Sequence>$\textbf{\textit{, comma separated string, required field.}} ordered list of the step names that RAVEN will run (see Section~\ref{sec:steps});
%
%\item $<DefaultInputFile>$\textbf{\textit{, string, optional field.}} In this block the user can change the default XML input file RAVEN is going to look for if none has been provided as command-line argument. \textit{Default = ``test.XML''}.
%
%\end{itemize}
% source: Simulation.py

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% RUN INFO ADVANCED USERS %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RunInfo: Advanced Users}
\label{subsec:runinfoadvanced}
This sub-section addresses some customizations of the running environment that
are possible in RAVEN.
%
Firstly, all the keywords reported in the previous sections can be pre-defined
by the user in an auxiliary XML input file.
%
Every time RAVEN gets instantiated (i.e. the code is run), it looks for an
optional file, named ``\texttt{default\_runinfo.XML}'' contained in the
``\texttt{\textbackslash home\textbackslash
username\textbackslash.raven\textbackslash}'' directory (i.e.
``\texttt{\textbackslash home\textbackslash
username\textbackslash.raven\textbackslash default\_runinfo.XML}'').
%
This file (same syntax as the RunInfo block defined in the general input file)
will be used for defining default values for the data in the RunInfo block. In
addition to the keywords defined in the previous sections, in the
\xmlNode{RunInfo} node, an additional keyword can be defined:
\begin{itemize}
%%%%%% DefaultInputFile
\item \xmlNode{DefaultInputFile}, \xmlDesc{string, optional field}. In
this block, the user can change the default xml input file RAVEN is going to
look for if none have been provided as a command-line argument.
%
\default{``test.xml''}.
\end{itemize}
As already mentioned, this file is read to define default data for the RunInfo
block.
%
This means that all the keywords defined here will be overridden by any values
specified in the actual RAVEN input file.
%
\\ In section ~\ref{subsec:runinfoModes}, it is explained how RAVEN can
handle the queue and parallel systems.
%
If the currently available ``modes'' are not suitable for the user's system
(workstation, HPC system, etc.), it is possible to define a custom ``mode''
modifying the \xmlNode{RunInfo} block as follows:
\begin{lstlisting}[style=XML]
<RunInfo>
    ...
    <CustomMode file="newMode.py" class="NewMode">
       aNewMode
    </CustomMode>
    <mode>aNewMode</mode>
    ...
</RunInfo>
\end{lstlisting}

The file field can use \%BASE\_WORKING\_DIR\% and \%FRAMEWORK\_DIR\%
to specify the location of the file with respect to the base working
directory or the framework directory.

The python file should define a class that inherits from
\texttt{Simulation.SimulationMode} of the RAVEN framework and overrides the
necessary functions. Generally, \texttt{modifySimulation} will be overridden to
change the precommand or postcommand parts which will be added before and after
the executable command.
%
An example Python class is given below with the functions that can and should be
overridden:

\begin{lstlisting}[language=python]
import Simulation
class NewMode(Simulation.SimulationMode):
  def remoteRunCommand(self, runInfoDict):
    # If it returns a dictionary, then run the command in args
    # Example: {"args":["ssh","remotehost","raven_framework"]}
    # Note that this command needs to be able to tell when it
    # is running remotely, and then return None at that point
    return None

  def modifyInfo(self, runInfoDict):
    # modifyInfo is called after the runInfoDict has been
    # setup and allows the mode to change any parameters that
    # need changing. This typically modifies the precommand and
    # the postcommand that are put before/after the command.
    # In order to change them, return a dictionary with new values.
    # Those new values will be used.
    return {}

  def XMLread(self,XMLNode):
    # XMLread is called with the mode node, and can be used to
    # get extra parameters needed for the simulation mode.
    pass
\end{lstlisting}

RAVEN's Job Handler module controls the creation and execution of
individual code runs. Essentially, the SimulationMode class may be
used when it is necessary to customize that behavior. First, it allows
providing a remote command for running RAVEN. This first method can be
used if for example RAVEN needs to be run on a different machine such
as a head node of a computer cluster. In such a case, a
remoteRunCommand function can be created that causes RAVEN to be
instantiated on the cluster head node (in cases where that is
different than the computer where the user is currently working).
Secondly, (and usually easier when this is sufficient) the
SimulationMode class allows modifying the various run info parameters
before the code is run.

For modification of the run info parameters, generally the two most
important are precommand and postcommand. They are placed in front and
back before running the code.  So for example if precommand is
`mpiexec -n 3' and postcommand is `--number-threads=4' and the code
command is `runIt' then the full command would be: `mpiexec -n 3 runIt
--number-threads=4' The precommand and postcommand are used for any
run type that is `parallel', but not for `serial' codes.  They can be
modified by overriding the \verb'modifyInfo' method and returning a
new dictionary with new values.  The \verb'runInfoDict' in the
simulation is passed in.

To help with these commands, there are several variables that are
substituted in before running the command.  These are:

\begin{description}
\item[\%INDEX\%]  Contains the zero-based index in list of running jobs.  Note that this is stable for the life of the job.  After the job finishes, this is reused.  An example use would be if there were four cpus and the batch size was four, the \%INDEX\% could be used to determine which cpu to run on.
\item[\%INDEX1\%] Contains the one-based index in the list of running jobs, same as \%INDEX\%+1
\item[\%CURRENT\_ID\%]  zero-based id for the job handler.  This starts as 0, and increases for each job the job handler starts.
\item[\%CURRENT\_ID1\%] one-based id for the job handler, same as \%CURRENT\_ID\%+1
\item[\%SCRIPT\_DIR\%] Expands to the full path of the script directory (raven/scripts)
\item[\%FRAMEWORK\_DIR\%]  Expands to the full path of the framework directory (raven/framework)
\item[\%WORKING\_DIR\%]  Expands to the working directory where the input is
\item[\%BASE\_WORKING\_DIR\%]  Expands to the base working directory given in RunInfo.  This will likely be a parent of WORKING\_DIR
\item[\%METHOD\%]  Expands to the environmental variable \$METHOD
\item[\%NUM\_CPUS\%]  Expands to the number of cpus to use per single batch.  This is NumThreads in the XML file.

\end{description}

The final joining of the commands and substituting the variables is
done in the JobHandler class.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% RUN INFO EXAMPLES %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RunInfo: Examples}
\label{subsec:runinfoexamples}
Here we present a few examples using different components of the RunInfo node:
\begin{lstlisting}[style=XML]
<RunInfo>
    <WorkingDir>externalModel</WorkingDir>
    <Sequence>MonteCarlo</Sequence>
    <batchSize>100</batchSize>
    <NumThreads>4</NumThreads>
    <mode>mpi</mode>
    <NumMPI>2</NumMPI>
</RunInfo>

<Files>
    <Input name='lorentzAttractor.py' type=''>lorentzAttractor.py</Input>
</Files>
\end{lstlisting}
This examples specifies the working directory (\texttt{WorkingDir}) where the
necessary file (\texttt{Files}) is located and to run a series of 100
(\texttt{batchSize}) Monte-Carlo calculations (\texttt{Sequence}).
%
MPI mode (\texttt{mode}) is used along with 4 threads (\texttt{NumThreads}) and
2 MPI processes per run (\texttt{NumMPI}).
