\section{Models \\ \vspace{2 mm} {\small }}
\label{sec:models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% If you are confused by the input of this document, please make sure you see
% these defined commands first. There is no point writing the same thing over
% and over and over and over and over again, so these will help us reduce typos,
% by just editing a template sentence or paragraph.

% These should be organized according to whic section they are most often used
% e.g. kernelDescription should go under a heading for SVMs.
% This will take a bit of work, but if later things are added it will make
% finding the appropriate parameters easier.

\renewcommand{\nameDescription}
{
  \xmlAttr{name},
  \xmlDesc{required string attribute}, user-defined identifier of this model.
  \nb As with other objects, this identifier can be used to reference this
  specific entity from other input blocks in the XML.
}
\renewcommand{\specBlock}[2]{
  The specifications of this model must be defined within #1 \xmlNode{#2} XML
  block.
}
%
\renewcommand{\subnodeIntro}
{
  This model can be initialized with the following child:
}
\renewcommand{\subnodesIntro}
{
  This model can be initialized with the following children:
}

\newcommand{\ppType}[2]
{
  In order to use the \textit{#1} PP, the user needs to set the
  \xmlAttr{subType} of a \xmlNode{PostProcessor} node:

  \xmlNode{PostProcessor \xmlAttr{subType}=\xmlString{#2}/}.
  
  In addition, several sub-nodes are available:
}

\newcommand{\skltype}[2]
{
  In order to use the \textit{#1}, the user needs to set the
  sub-node:

  \xmlNode{SKLtype}\texttt{#2}\xmlNode{/SKLtype}.
  
  In addition to this XML node, several others are available:
}

\newcommand{\nIterDescriptionA}[1]
{
  \xmlNode{n\_iter}, \xmlDesc{integer, optional field}, is the maximum number of
  iterations.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\nIterDescriptionB}[1]
{
  \xmlNode{n\_iter}, \xmlDesc{int, optional field}, specifies the number of
  passes over the training data (aka epochs).
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\tolDescriptionA}[1]
{
  \xmlNode{tol}, \xmlDesc{float, optional field}, stop the algorithm if the convergence error felt below the tolerance specified here. 
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\tolDescriptionB}[1]
{
  \xmlNode{tol}, \xmlDesc{float, optional field}, specifies the tolerance for
  the optimization: if the updates are smaller than tol, the optimization code
  checks the dual gap for optimality and continues until it is smaller than tol.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\tolDescriptionC}[1]
{
  \xmlNode{tol}, \xmlDesc{float, optional field}, specifies the tolerance for
  stopping criteria.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\fitInterceptDescription}[1]
{
  \xmlNode{fit\_intercept}, \xmlDesc{boolean, optional field}, determines
  whether to calculate the intercept for this model.
  %
  If set to False, no intercept will be used in the calculations (e.g. data is
  expected to be already centered).
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\normalizeDescription}[1]
{
  \xmlNode{normalize}, \xmlDesc{boolean, optional field}, if True, the
  regressors X will be normalized before regression.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\verDescriptionA}[1]
{
  \xmlNode{verbose}, \xmlDesc{boolean, optional field}, use verbose mode
  when fitting the model.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\verDescriptionB}[1]
{
  \xmlNode{verbose}, \xmlDesc{boolean or integer, optional field}, use verbose
  mode when fitting the model.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\maxIterDescription}[1]
{
\xmlNode{max\_iter}, \xmlDesc{integer, optional field}, specifies the
  maximum number of iterations.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\warmStartDescription}[1]
{
  \xmlNode{warm\_start}, \xmlDesc{boolean, optional field}, when set to
  True, the model will reuse the solution of the previous call to fit as
  initialization, otherwise, it will just erase the previous solution.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\positiveDescription}[1]
{
  \xmlNode{positive}, \xmlDesc{float, optional field}, when set to True, this
  forces the coefficients to be positive.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\precomputeDescription}[1]
{
  \xmlNode{precompute}, \xmlDesc{boolean or string, optional field}, determines
  whether to use a precomputed Gram matrix to speed up calculations.
  %
  If set to `auto,' RAVEN will decide.
  %
  The Gram matrix can also be passed as an argument.
  %
  Available options are [True | False | `auto' | array-like].
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\nAlphasDescription}[1]
{
  \xmlNode{max\_n\_alphas}, \xmlDesc{integer, optional field}, specifies the
  maximum number of points on the path used to compute the residuals in the
  cross-validation.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\shuffleDescription}[1]
{
  \xmlNode{shuffle}, \xmlDesc{boolean, optional field}, specifies whether
  or not the training data should be shuffled after each epoch.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\randomStateDescription}[1]
{
  \xmlNode{random\_state}, \xmlDesc{int seed, RandomState instance, or None},
  sets the seed of the pseudo random number generator to use when shuffling the
  data.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\solverDescription}%[1]
{
  \xmlNode{solver}, \xmlDesc{\{`auto', `svd', `cholesky', `lsqr', 
  `sparse\_cg'\}}, specifies the solver to use in the computational routines:
  \begin{itemize}
    \item `auto' chooses the solver automatically based on the type of data.
    %
    \item `svd' uses a singular value decomposition of X to compute the ridge
    coefficients.
    %
    More stable for singular matrices than `cholesky.'
    %
    \item `cholesky' uses the standard scipy.linalg.solve function to obtain a
    closed-form solution.
    %
    \item `sparse\_cg' uses the conjugate gradient solver as found in
    scipy.sparse.linalg.cg.
    %
    As an iterative algorithm, this solver is more appropriate than
    `cholesky' for large-scale data (possibility to set tol and max\_iter).
    %
    \item `lsqr' uses the dedicated regularized least-squares routine
    scipy.sparse.linalg.lsqr.
    %
    It is the fatest but may not be available in old scipy versions.
    %
    It also uses an iterative procedure.
    %
  \end{itemize}
  All three solvers support both dense and sparse data.
  %
  %\ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

%%%%%%%%%%%%%%%%%%%%%%%%% Common Regression Parameters %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% End Common Regression Parameters %%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Common SVM Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\CSVMDescription}[1]
{
  \xmlNode{C}, \xmlDesc{boolean, optional field}, sets the penalty parameter C
  of the error term.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
  %
}

\newcommand{\kernelDescription}[1]
{
  \xmlNode{kernel}, \xmlDesc{string, optional}, specifies the kernel type
  to be used in the algorithm.
  %
  It must be one of:
  \begin{itemize}
    \item `linear'
    \item `poly'
    \item `rbf'
    \item `sigmoid'
    \item `precomputed'
    \item a callable object
  \end{itemize}
  %
  If a callable is given it is used to precompute the kernel matrix.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
  %
}

\newcommand{\degreeDescription}[1]
{
  \xmlNode{degree}, \xmlDesc{int, optional field}, determines the degree
  of the polynomial kernel function (`poly').
  %
  Ignored by all other kernels.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\gammaDescription}[1]
{
  \xmlNode{gamma}, \xmlDesc{float, optional field}, sets the
  kernel coefficient for the kernels `rbf,' `poly,' and `sigmoid.'
  %
  If gamma is 0.0 then 1/n\_features will be used instead.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\coefZeroDescription}[1]
{
  \xmlNode{coef0}, \xmlDesc{float, optional field}, is an independent term in
  kernel function.
  %
  It is only significant in `poly' and `sigmoid.'
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\probabilityDescription}[1]
{
  \xmlNode{probability}, \xmlDesc{boolean, optional field}, determines whether
  or not to enable probability estimates.
  %
  This must be enabled prior to calling fit, and will slow down that method.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\shrinkingDescription}[1]
{
  \xmlNode{shrinking}, \xmlDesc{boolean, optional field}, determines whether or
  not to use the shrinking heuristic.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\cacheSizeDescription}[1]
{
  \xmlNode{cache\_size}, \xmlDesc{float, optional field}, specifies the
  size of the kernel cache (in MB).
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\classWeightDescription}[1]
{
  \xmlNode{class\_weight}, \xmlDesc{{dict, `auto'}, optional}, sets the
  parameter C of class i to class\_weight[i]*C for SVC.
  %
  If not given, all classes are assumed to have weight one.
  %
  The `auto' mode uses the values of y to automatically adjust weights inversely
  proportional to class frequencies.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\tolSVMDescription}[1]
{
  \tolDescriptionC{#1}
}

\newcommand{\verSVMDescription}[1]
{
  \verDescriptionA{False}
  %
  \nb This setting takes advantage of a per-process runtime setting in libsvm
  that, if enabled, may not work properly in a multithreaded context.
}

\newcommand{\randomStateSVMDescription}[1]
{
  \xmlNode{random\_state}, \xmlDesc{int seed, RandomState instance, or
  None}, represents the seed of the pseudo random number generator to use when
  shuffling the data for probability estimation.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%% End Common SVM Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% Common Multi-Class Parameters %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\estimatorDescription}[1]
{
  \xmlNode{estimator}, \xmlDesc{boolean, required field},
  %
  An estimator object implementing fit and one of decision\_function or
  predict\_proba.
  %
  This XML node needs to contain the following attribute:
  \vspace{-5mm}
  \begin{itemize}
    \itemsep0em
    \item \xmlAttr{estimatorType}, \xmlDesc{required string attribute}, this
    attribute is another reduced order mode type that needs to be used for the
    construction of the multi-class algorithms.
    %
    Each sub-sequential node depends on the chosen ROM.
  \end{itemize}
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}
%%%%%%%%%%%%%%%%%%%%%%% End Common Multi-Class Parameters %%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Common Bayesian Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\alphaBayesDescription}[1]
{
  \xmlNode{alpha}, \xmlDesc{float, optional field}, specifies an additive
  (Laplace/Lidstone) smoothing parameter (0 for no smoothing).
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\fitPriorDescription}[1]
{
  \xmlNode{fit\_prior}, \xmlDesc{boolean, required field}, determines whether to
  learn class prior probabilities or not.
  %
  If false, a uniform prior will be used.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\classPriorDescription}[1]
{
  \xmlNode{class\_prior}, \xmlDesc{array-like float (n\_classes), optional
  field}, specifies prior probabilities of the classes.
  %
  If specified, the priors are not adjusted according to the data.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

%%%%%%%%%%%%%%%%%%%%%%%% End Common Bayesian Parameters %%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Common Neighbor Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\nNeighborsDescription}[1]
{
  \xmlNode{n\_neighbors}, \xmlDesc{integer, optional field}, specifies the
  number of neighbors to use by default for `k\_neighbors' queries.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\radiusDescription}[1]
{
  \xmlNode{radius}, \xmlDesc{float, optional field}, specifies the range of
  parameter space to use by default for `radius\_neighbors' queries.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\weightsDescription}[1]
{
  \xmlNode{weights}, \xmlDesc{string, optional field}, specifies the weight
  function used in prediction.
  %
  Possible values:
  \begin{itemize}
    \item \textit{uniform} : uniform weights.
    %
    All points in each neighborhood are weighted equally;
    \item \textit{distance} : weight points by the inverse of their distance.
    %
    In this case, closer neighbors of a query point will have a greater
    influence than neighbors which are further away.
    %
  \end{itemize}
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\metricDescription}[1]
{
  \xmlNode{metric}, \xmlDesc{string, optional field}, sets the distance metric
  to use for the tree.
  %
  The Minkowski metric with p=2 is equivalent to the standard Euclidean metric.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\algorithmDescription}[1]
{
  \xmlNode{algorithm}, \xmlDesc{string, optional field}, specifies the algorithm
  used to compute the nearest neighbors:
  \begin{itemize}
    \item \textit{ball\_tree} will use BallTree.
    \item \textit{kd\_tree} will use KDtree.
    \item \textit{brute} will use a brute-force search.
    \item \textit{auto} will attempt to decide the most appropriate algorithm
    based on the values passed to fit method.
    %
  \end{itemize}
  \nb Fitting on sparse input will override the setting of this parameter, using
  brute force.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\leafSizeDescription}[1]
{
  \xmlNode{leaf\_size}, \xmlDesc{integer, optional field}, sets the leaf size
  passed to the BallTree or KDTree.
  %
  This can affect the speed of the construction and query, as well as the memory
  required to store the tree.
  %
  The optimal value depends on the nature of the problem.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\pDescription}[1]
{
  \xmlNode{p}, \xmlDesc{integer, optional field}, is a parameter for the
  Minkowski metric.
  %
  When $p = 1$, this is equivalent to using manhattan distance (L1), and
  euclidean distance (L2) for $p = 2$.
  %
  For arbitrary p, minkowski distance (L\_p) is used.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\outlierLabelDescription}[1]
{
  \xmlNode{outlier\_label}, \xmlDesc{integer, optional field}, is a label, which
  is given for outlier samples (samples with no neighbors on a given radius).
  %
  If set to None, ValueError is raised, when an outlier is detected.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

%%%%%%%%%%%%%%%%%%%%%%%% End Common Neighbor Parameters %%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Common Tree Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\criterionDescription}[1]
{
  \xmlNode{criterion}, \xmlDesc{string, optional field}, specifies the function
  used to measure the quality of a split.
  %
  Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the
  information gain.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\splitterDescription}[1]
{
  \xmlNode{splitter}, \xmlDesc{string, optional field}, specifies the strategy
  used to choose the split at each node.
  %
  Supported strategies are ``best'' to choose the best split and ``random'' to
  choose the best random split.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\maxFeaturesDescription}[1]
{
  \xmlNode{max\_features}, \xmlDesc{int, float or string, optional field}, sets
  the number of features to consider when looking for the best split:
  \begin{itemize}
    \item If int, then consider max\_features features at each split.
    %
    \item If float, then max\_features is a percentage and int(max\_features *
    n\_features) features are considered at each split.
    %
    \item If ``auto,'' then max\_features=sqrt(n\_features).
    \item If ``sqrt,'' then max\_features=sqrt(n\_features).
    \item If ``log2,'' then max\_features=log2(n\_features).
    \item If None, then max\_features=n\_features.
    %
  \end{itemize}
  \nb The search for a split does not stop until at least one valid partition of
  the node samples is found, even if it requires to effectively inspect more
  than max\_features features.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\maxDepthDescription}[1]
{
  \xmlNode{max\_depth}, \xmlDesc{integer, optional field}, determines the
  maximum depth of the tree.
  %
  If None, then nodes are expanded until all leaves are pure or until all leaves
  contain less than min\_samples\_split samples.
  %
  Ignored if max\_samples\_leaf is not None.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\minSamplesSplitDescription}[1]
{
  \xmlNode{min\_samples\_split}, \xmlDesc{integer, optional field}, sets the
  minimum number of samples required to split an internal node.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\minSamplesLeafDescription}[1]
{
  \xmlNode{min\_samples\_leaf}, \xmlDesc{integer, optional field}, sets the
  minimum number of samples required to be at a leaf node.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}

\newcommand{\maxLeafNodesDescription}[1]
{
  \xmlNode{max\_leaf\_nodes}, \xmlDesc{integer, optional field}, grow a tree
  with max\_leaf\_nodes in best-first fashion.
  %
  Best nodes are defined by relative reduction in impurity.
  %
  If None then unlimited number of leaf nodes.
  %
  If not None then max\_depth will be ignored.
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}
%%%%%%%%%%%%%%%%%%%%%%%%%% End Common Tree Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%% Common Gaussian Process Parameters %%%%%%%%%%%%%%%%%%%%%%
\newcommand{\blankbDescription}[1]
{
  %
  \ifthenelse{\equal{#1}{}}{}{\default{#1}}
}
%%%%%%%%%%%%%%%%%%%% End Common Gaussian Process Parameters %%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In RAVEN, \textbf{Models} are crucial entities.
%
A model is an object that employs a mathematical representation of a
phenomenon, either of a physical or other nature (e.g. statistical operators,
etc.).
%
From a practical point of view, it can be seen, as a ``black box'' that, given
an input, returns an output.
%

RAVEN has a strict classification of the different types of models.
%
Each ``class'' of models is represented by the definition reported above, but it
can be further classified based on its particular functionalities:
\begin{itemize}
  \item \xmlNode{Code} represents an external system code that employs a high
  fidelity physical model.
  \item \xmlNode{Dummy} acts as ``transfer'' tool.
  %
  The only action it performs is transferring the the information in the input
  space (inputs) into the output space (outputs).
  %
  For example, it can be used to check the effect of a sampling strategy, since
  its outputs are the sampled parameters' values (input space) and a counter
  that keeps track of the number of times an evaluation has been requested.
  %
  \item \xmlNode{ROM}, or reduced order model, is a mathematical model trained
  to predict a response of interest of a physical system.
  %
  Typically, ROMs trade speed for accuracy representing a faster, rough estimate
  of the underlying phenomenon.
  %
  The ``training'' process is performed by sampling the response of a physical
  model with respect to variation of its parameters subject to probabilistic
  behavior.
  %
  The results (outcomes of the physical model) of the sampling are fed into
  the algorithm representing the ROM that tunes itself to replicate those
  results.
  \item \xmlNode{ExternalModel}, as its name suggests, is an entity existing 
  outside the RAVEN framework that is embedded in the RAVEN code at run time.
  %
  This object allows the user to create a Python module that will be treated as
  a predefined internal model object.
%\item [Projector:] generic data manipulator
  \item \xmlNode{PostProcessor} is a container of all the actions that can
  manipulate and process a data object in order to extract key information,
  such as statistical quantities, clustering, etc.
  %
\end{itemize}
Before analyzing each model in detail, it is important to mention that each
type needs to be contained in the main XML node \xmlNode{Models}, as reported
below:

\textbf{Example:}
\begin{lstlisting}[style=XML]
<Simulation>
  ...
  <Models>
    ...
    <WhatEverModel name='whatever'>
      ...
    </WhatEverModel>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
In the following sub-sections each \textbf{Model} type is fully analyzed and
described.
%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  Code  Model   %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%<Code name='MyRAVEN' subType='RAVEN'><executable>%FRAMEWORK_DIR%/../RAVEN-%METHOD%</executable></Code>
%<alias variable='internal_variable_name'>Material|Fuel|thermal_conductivity</alias>
\subsection{Code}
\label{subsec:models_code}
As already mentioned, the \textbf{Code} model represents an external system
software employing a high fidelity physical model.
%
The link between RAVEN and the driven code is performed at run-time, through
coded interfaces that are the responsible for transferring information from the
code to RAVEN and vice versa.
%
In Section~\ref{sec:existingInterface}, all of the available interfaces are
reported and, for advanced users, Section~\ref{sec:newCodeCoupling} explains how
to couple a new code.


\specBlock{a}{Code}
%
\attrsIntro
%
\vspace{-5mm}
\begin{itemize}
  \itemsep0em
  \item \nameDescription
  \item \xmlAttr{subType}, \xmlDesc{required string attribute}, specifies the
  code that needs to be associated to this Model.
  %
  \nb See Section~\ref{sec:existingInterface} for a list of currently supported
  codes.
  %
\end{itemize}
\vspace{-5mm}

\subnodesIntro
%
\begin{itemize}
  \item \xmlNode{executable} \xmlDesc{string, required field} specifies the path
  of the executable to be used.
  %
  \nb Either an absolute or relative path can be used.
  \item \xmlNode{alias} \xmlDesc{string, optional field} specifies aliases for
  any variables of interest coming from the code to which this model refers.
  %
  These aliases can be used anywhere in the RAVEN input to refer to the code
  variables.
  %
  In the body of this node the user specifies the name of the variable that
  RAVEN will look for in the output files of the code.
  %
  The actual alias, usable throughout the input, is instead defined in the
  \xmlAttr{variable} attribute of this tag.
  %
  \nb The user can specify as many aliases as needed.
  %
  \default{None}
  %
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={subType,name,variable}]
<Simulation>
  ...
  <Models>
    ...
    <Code name='aUserDefinedName' subType='RAVEN_Driven_code'>
      <executable>path_to_executable</executable>
      <alias variable='internal_variable_name1'>
         External_Code_Variable_Name_1
      </alias>
      <alias variable='internal_variable_name2'>
         External_Code_Variable_Name_2
      </alias>
    </Code>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Dummy Model  %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dummy}
\label{subsec:models_dummy}
The \textbf{Dummy} model is an object that acts as a pass-through tool.
%
The only action it performs is transferring the information in the input
space (inputs) to the output space (outputs).
%
For example, it can be used to check the effect of a particular sampling
strategy, since its outputs are the sampled parameters' values (input space) and
a counter that keeps track of the number of times an evaluation has been
requested.
%

\specBlock{a}{Dummy}.
%
\attrsIntro
%
\vspace{-5mm}
\begin{itemize}
  \itemsep0em
  \item \nameDescription
  %
  \item \xmlAttr{subType}, \xmlDesc{required string attribute}, this attribute
  must be kept empty.
\end{itemize}
\vspace{-5mm}
Given a particular \textit{Step} using this model, if this model is linked to
a \textit{Data} with the role of \textbf{Output}, it expects one of the output
parameters will be identified by the keyword ``OutputPlaceHolder'' (see 
Section~\ref{sec:steps}).

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={subType}]
<Simulation>
  ...
  <Models>
    ...
    <Dummy name='aUserDefinedName' subType=''/>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%
%%%%% ROM Model  %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{ROM}
\label{subsec:models_ROM}
A Reduced Order Model (ROM) is a mathematical model consisting of a fast
solution trained to predict a response of interest of a physical system.
%
The ``training'' process is performed by sampling the response of a physical
model with respect to variations of its parameters subject, for example, to
probabilistic behavior.
%
The results (outcomes of the physical model) of the sampling are fed into the
algorithm representing the ROM that tunes itself to replicate those results.
%
RAVEN supports several different types of ROMs, both internally developed and
imported through an external library called ``scikit-learn''~\cite{SciKitLearn}.

Currently in RAVEN, the ROMs are classified into 4 sub-types that, once chosen,
provide access to several different algorithms.
%
These sub-types are specified in the \xmlAttr{subType} attribute and should be
one of the following:
\begin{itemize}
  \item \xmlString{NDspline}
  \item \xmlString{NDinvDistWeigth}
  \item \xmlString{microSphere}
  \item \xmlString{SciKitLearn}
\end{itemize}

\specBlock{a}{ROM}
%
\attrsIntro
%
\vspace{-5mm}
\begin{itemize}
  \itemsep0em
  \item \nameDescription
  \item \xmlAttr{subType}, \xmlDesc{required string attribute}, defines which of
  the 4 sub-types should be used, choosing among the previously reported
  types.
  %
  This choice conditions the subsequent the required and/or optional 
  \xmlNode{ROM} sub-nodes.
  %
\end{itemize}
\vspace{-5mm}

In the \xmlNode{ROM} input block, the following XML sub-nodes are required,
independent of the \xmlAttr{subType} specified:
%
\begin{itemize}
<<<<<<< HEAD
  \item \xmlNode{Features}, \xmlDesc{comma separated string, required field},
  specifies the names of the features of this ROM.
  %
  \nb These parameters will be requested for the training of this object (see
  Section~\ref{subsec:stepRomTrainer}.
  \item \xmlNode{Target}, \xmlDesc{comma separated string, required field},
  contains a comma separated list of the targets of this ROM.
  %
  The parameters listed here represent the figures of merit this ROM is supposed
  to predict.
  %
  \nb These parameters are going to be requested for the training of this object
  (see Section~\ref{subsec:stepRomTrainer}).
  %
=======
   \item $<Features>$ \textbf{\textit{, comma separated string, required field.}}. In this node, the user needs to specify the names of the features of this ROM. NB. These parameters are going to be requested for the training of this object (see section \ref{subsec:stepTraining};
    \item $<Target>$ \textbf{\textit{, comma separated string, required field.}}. This XML node contains a comma separated list of the targets of this ROM. By Instance, these parameters are the Figure of Merits this ROM is supposed to predict.
    \\NB. These parameters are going to be requested for the training of this object (see section \ref{subsec:stepTraining}.
    \item $<NormalizeData>$ \textbf{\textit{, boolean, optional field.}}. This XML node controls the normalization of data used for training the ROM. If True, the Training set is going to be normalized with respect to Average and Standard Deviation of the data set; no normalization is applied if this field is False. \textit{Default =  True}.
>>>>>>> efbd7a6e48d875b1f32bac1e9000830ae9397acb
\end{itemize}

The types and meaning of the remaining sub-nodes depend on the sub-type
specified in the attribute \xmlAttr{subType}.
%
In the sections that follow, the specifications of each type are reported.
%
%%%%% ROM Model - NDspline  %%%%%%%
\subsubsection{NDspline}
\label{subsubsec:NDspline}
The NDspline sub-type contains a single ROM type, based on an $N$-dimensional
spline interpolation/extrapolation scheme.
%
In spline interpolation, the interpolant is a special type of piecewise
polynomial called a spline.
%
The interpolation error can be made small even when using low degree polynomials
for the spline.
%
Spline interpolation avoids the problem of Runge's phenomenon, in which
oscillation can occur between points when interpolating using higher degree
polynomials.
%

In order to use this ROM, the \xmlNode{ROM} attribute \xmlAttr{subType} needs to
be \xmlString{NDspline} (see the example below).
%
No further XML sub-nodes are required.
%
\nb This ROM type must be trained from a regular Cartesian grid.
%
Thus, it can only be trained from the outcomes of a grid sampling strategy.

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
    <ROM name='aUserDefinedName' subType='NDspline'>
       <Features>var1,var2,var3</Features>
       <Target>result1,result2</Target>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
%%%%% ROM Model - NDinvDistWeigth  %%%%%%%
\subsubsection{NDinvDistWeigth}
\label{subsubsec:NDinvDistWeigth}
The NDinvDistWeigth sub-type contains a single ROM type, based on an
$N$-dimensional inverse distance weighting formulation.
%
Inverse distance weighting (IDW) is a type of deterministic method for
multivariate interpolation with a known scattered set of points.
%
The assigned values to unknown points are calculated via a weighted average of
the values available at the known points.
%

In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be xmlString{NDinvDistWeigth} (see the example 
below).
%
\subnodeIntro

\begin{itemize}
  \item \xmlNode{p}, \xmlDesc{integer, required field}, must be greater than
  zero and represents the ``power parameter''.
  %
  For the choice of value for \xmlNode{p},it is necessary to consider the degree
  of smoothing desired in the interpolation/extrapolation, the density and
  distribution of samples being interpolated, and the maximum distance over
  which an individual sample is allowed to influence the surrounding ones (lower
  $p$ means greater importance for points far away).
  %
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
    <ROM name='aUserDefinedName' subType='NDinvDistWeigth'>
      <Features>var1,var2,var3</Features>
      <Target>result1,result2</Target>
      <p>3</p>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
%%%%% ROM Model - MicroSphere  %%%%%%%
\subsubsection{MicroSphere}
\label{subsubsec:microSphere}
Not yet functional.
%
Its validity for prediction still needs to be assessed.
%
%%%%% ROM Model - SciKitLearn  %%%%%%%
\subsubsection{SciKitLearn}
\label{subsubsec:SciKitLearn}
The SciKitLearn sub-type represents the container of several ROMs available in
RAVEN through the external library scikit-learn~\cite{SciKitLearn}.
%

In order to use this Reduced Order Model, the \xmlNode{ROM} attribute
\xmlAttr{subType} needs to be \\ \xmlString{SciKitLearn} (i.e. 
\xmlAttr{subType}\textbf{\texttt{=}}\xmlString{SciKitLearn}).
%
The specifications of a \xmlString{SciKitLearn} ROM depend on the value assumed
by the following sub-node within the main \xmlNode{ROM} XML node:
\begin{itemize}
  \item \xmlNode{SKLtype}, \xmlDesc{vertical bar (\texttt{|}) separated string,
  required field}, contains a string that represents the ROM type to be used.
  %
  As mentioned, its format is:\\
  \xmlNode{SKLtype}\texttt{mainSKLclass|algorithm}\xmlNode{/SKLtype} where the
  first word (before the ``\texttt{|}'' symbol) represents the main class of
  algorithms, and the second word (after the ``\texttt{|}'' symbol) represents
  the specific algorithm.
  %
\end{itemize}
Based on the \xmlNode{SKLtype} several different algorithms are available.
%
In the following paragraphs a brief explanation and the input requirements are
reported for each of them.
%
%%%%% ROM Model - SciKitLearn: Linear Models %%%%%%%
\paragraph{Linear Models}
\label{LinearModels}
The LinearModels' algorithms implement generalized linear models.
%
They include Ridge regression, Bayesian regression, lasso, and elastic net
estimators computed with least angle regression and coordinate descent.
%
This class also implements stochastic gradient descent related algorithms.
%
In the following, all of the linear models available in RAVEN are reported.
%
\subparagraph{Linear Model: Automatic Relevance Determination Regression}
\mbox{}
\\The \textit{Automatic Relevance Determination} (ARD) regressor is a
hierarchical Bayesian approach where hyperparameters explicitly represent the
relevance of different input features.
%
These relevance hyperparameters determine the range of variation for the
parameters relating to a particular input, usually by modelling the width of a
zero-mean Gaussian prior on those parameters.
%
If the width of the Gaussian is zero, then those parameters are constrained to
be zero, and the corresponding input cannot have any effect on the predictions,
therefore making it irrelevant.
%
ARD optimizes these hyperparameters to discover which inputs are relevant.
%
\skltype{Automatic Relevance Determination regressor}{linear\_model|ARDRegression}.
\begin{itemize}
  \item \nIterDescriptionA{300}
  \item \tolDescriptionA{1.e-3}
  \item \xmlNode{alpha\_1}, \xmlDesc{float, optional field}, is a shape
  hyperparameter for the Gamma distribution prior over the $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{alpha\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_1}, \xmlDesc{float, optional field}, shape
  hyperparameter for the Gamma distribution prior over the $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{compute\_score}, \xmlDesc{boolean, optional field}, if True,
  compute the objective function at each step of the model.
  \default{False}
  %
  \item \xmlNode{threshold\_lambda}, \xmlDesc{float, optional field}, specifies
  the threshold for removing (pruning) weights with high precision from the
  computation.
  \default{ 1.e+4}
  %
  \item \fitInterceptDescription{True}
  %
  \item \normalizeDescription{False}
  %
  \item \verDescriptionA{False}
\end{itemize}
%%%%%%%%
\subparagraph{Linear Model: Bayesian ridge regression}
\mbox{}

The \textit{Bayesian ridge regression} estimates a probabilistic model of the
regression problem as described above.
%
The prior for the parameter $w$ is given by a spherical Gaussian:
\begin{equation}
p(w|\lambda) =\mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})
\end{equation}
The priors over $\alpha$ and $\lambda$ are chosen to be gamma distributions, the
conjugate prior for the precision of the Gaussian.
%
The resulting model is called Bayesian ridge regression, and is similar to the
classical ridge regression.
%
The parameters $w$, $\alpha$, and $\lambda$ are estimated jointly during the fit
of the model.
%
The remaining hyperparameters are the parameters of the gamma priors over
$\alpha$ and $\lambda$.
%
These are usually chosen to be non-informative.
%
The parameters are estimated by maximizing the marginal log likelihood.
%
\skltype{Bayesian ridge regressor}{linear\_model|BayesianRidge}
\begin{itemize}
  \item \nIterDescriptionA{300}
  \item \tolDescriptionA{1.e-3}
  \item \xmlNode{alpha\_1}, \xmlDesc{float, optional field}, is a shape
  hyperparameter for the Gamma distribution prior over the $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{alpha\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\alpha$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_1}, \xmlDesc{float, optional field}, shape
  hyperparameter for the Gamma distribution prior over the $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{lambda\_2}, \xmlDesc{float, optional field}, inverse scale
  hyperparameter (rate parameter) for the Gamma distribution prior over the
  $\lambda$ parameter.
  \default{ 1.e-6}
  %
  \item \xmlNode{compute\_score}, \xmlDesc{boolean, optional field}, if True,
  compute the objective function at each step of the model.
  \default{False}
  %
  \item \xmlNode{threshold\_lambda}, \xmlDesc{float, optional field}, specifies
  the threshold for removing (pruning) weights with high precision from the
  computation.
  \default{ 1.e+4}
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \verDescriptionA{False}
\end{itemize}
%%%%%%%
\subparagraph{Linear Model: Elastic Net}
\mbox{}
\\The \textit{Elastic Net} is a linear regression technique with combined L1 and
L2 priors as regularizers.
%
It minimizes the objective function:
\begin{equation}
1/(2*n_samples) *||y - Xw||^2_2+alpha*l1_ratio*||w||_1 + 0.5 *alpha*(1 - l1_ratio)*||w||^2_2
\end{equation}

\skltype{Elastic Net regressor}{linear\_model|ElasticNet}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, specifies a constant
  that multiplies the penalty terms.
  %
  $alpha = 0$ is equivalent to an ordinary least square, solved by the
  \textbf{LinearRegression} object.
  \default{1.0}
  %
  \item \xmlNode{l1\_ratio}, \xmlDesc{float, optional field}, specifies the
  ElasticNet mixing parameter, with $0 <= l1\_ratio <= 1$.
  %
  For $l1\_ratio = 0$ the penalty is an L2 penalty.
  %
  For $l1\_ratio = 1$ it is an L1 penalty.
  %
  For $0 < l1\_ratio < 1$, the penalty is a combination of L1 and L2.
  %
  \default{0.5}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{300}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{False}
  \item \positiveDescription{False}
  %
\end{itemize}
%%%%%%%%
\subparagraph{Linear Model: Elastic Net CV}
\mbox{}
\\The \textit{Elastic Net CV} is a linear regression similar to the Elastic Net
model but with an iterative fitting along a regularization path.
%
The best model is selected by cross-validation.
%

\skltype{Elastic Net CV regressor}{linear\_model|ElasticNetCV}
\begin{itemize}
  \item \xmlNode{l1\_ratio}, \xmlDesc{float, optional field},
  %
  Float flag between 0 and 1 passed to ElasticNet (scaling between l1 and l2
  penalties).
  %
  For $l1\_ratio = 0$ the penalty is an L2 penalty.
  %
  For $l1\_ratio = 1$ it is an L1 penalty.
  %
  For $0 < l1\_ratio < 1$, the penalty is a combination of L1 and L2 This
  parameter can be a list, in which case the different values are tested by
  cross-validation and the one giving the best prediction score is used.
  %
  Note that a good choice of list of values for $l1\_ratio$ is often to put more
  values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1,
  .5, .7, .9, .95, .99, 1].
  %
  \default{0.5}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, specifies the length of 
  the path.
  %
  eps=1e-3 means that $alpha\_min / alpha\_max = 1e-3$.
  %
  \default{0.001}
  \item \xmlNode{n\_alphas}, \xmlDesc{integer, optional field}, is the number of
  alphas along the regularization path used for each $l1\_ratio$.
  %
  \default{100}
  \item \precomputeDescription{1.0}
  \item \maxIterDescription{300}
  \item \tolDescriptionB{1.e-4}
  %
  \item \positiveDescription{False}
  %
\end{itemize}
%%%%%%
\subparagraph{Linear Model: Least Angle Regression model}
\mbox{}
\\The \textit{Least Angle Regression model} (LARS) is a regression algorithm for
high-dimensional data.
%
The LARS algorithm provides a means of producing an estimate of which variables
to include, as well as their coefficients, when a response variable is
determined by a linear combination of a subset of potential covariates.
%

\skltype{Least Angle Regression model}{linear\_model|Lars}
\begin{itemize}
  \item \xmlNode{n\_nonzero\_coef}, \xmlDesc{integer, optional field}, 
  represents the target number of non-zero coefficients.
  %
  \default{500}
  \item \fitInterceptDescription{True}
  \item \verDescriptionA{False}
  \item \precomputeDescription{1.0}
  \item \normalizeDescription{False}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the \xmlNode{tol} parameter in some iterative optimization-based
  algorithms, this parameter does not control the tolerance of the optimization.
  %
  \default{2.22e-16}
  \item \xmlNode{fit\_path}, \xmlDesc{boolean, optional field}, if True the
  full path is stored in the coef\_path\_attribute.
  %
  If you compute the solution for a large problem or many targets, setting
  fit\_path to False will lead to a speedup, especially with a small alpha.
  %
  \default{True}
  %
\end{itemize}
%%%%%%
\subparagraph{Linear Model: Cross-validated Least Angle Regression model}
\mbox{}
\\The \textit{Cross-validated Least Angle Regression model} is a regression
algorithm for high-dimensional data.
%
It is similar to the LARS method, but the best model is selected by
cross-validation.
%
\skltype{Cross-validated Least Angle Regression model}{linear\_model|LarsCV}
\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \verDescriptionA{False}
  \item \normalizeDescription{False}
  \item \precomputeDescription{1.0}
  \item \maxIterDescription{300}
  \item \nAlphasDescription{1000}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the
  machine-precision regularization in the computation of the Cholesky diagonal
  factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the \textit{tol} parameter in some iterative optimization-based
  algorithms, this parameter does not control the tolerance of the optimization.
  %
  \default{2.22e-16}
\end{itemize}
\subparagraph{Linear Model trained with L1 prior as regularizer (aka the Lasso)}
\mbox{}
\\The \textit{Linear Model trained with L1 prior as regularizer (Lasso)} is a
shrinkage and selection method for linear regression.
%
It minimizes the usual sum of squared errors, with a bound on the sum of the
absolute values of the coefficients.
%
\skltype{Linear Model trained with L1 prior as regularizer
  (Lasso)}{linear\_model|Lasso}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, sets a constant
  multiplier for the L1 term.
  %
  \default{1.0}
  %
  alpha = 0 is equivalent to an ordinary least square, solved by the
  LinearRegression object.
  %
  For numerical reasons, using alpha = 0 with the Lasso object is not advised
  and you should instead use the LinearRegression object.
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \precomputeDescription{}
  \nb For sparse input this option is always True to preserve sparsity.
  \item \maxIterDescription{}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{}
  \item \positiveDescription{}
\end{itemize}

\subparagraph{Lasso linear model with iterative fitting along a regularization
  path}
\mbox{}

The \textit{Lasso linear model with iterative fitting along a regularization
path} is an algorithm of the Lasso family, that computes the linear regressor weights, 
identifying the regularization path in an iterative fitting (see http://www.jstatsoft.org/v33/i01/paper)

\skltype{Lasso linear model with iterative fitting along a regularization path
regressor}{linear\_model|LassoCV}
\begin{itemize}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the length of
  the path.
  %
  eps=1e-3 means that alpha\_min / alpha\_max = 1e-3.
  %
  \item \xmlNode{n\_alphas}, \xmlDesc{int, optional field}, sets the number of
  alphas along the regularization path.
  \item \xmlNode{alphas}, \xmlDesc{numpy array, optional field}, lists the
  locations of the alphas used to compute the models.
  %
  If None, alphas are set automatically.
  \item \precomputeDescription{}
  \item \maxIterDescription{}
  \item \tolDescriptionB{1.e-4}
  \item \verDescriptionB{}
  \item \positiveDescription{}
\end{itemize}

\subparagraph{Lasso model fit with Least Angle Regression}
\mbox{}

The \textit{Lasso model fit with Least Angle Regression} is a cross-validated
least angle regression model.
%
\skltype{Cross-validated Least Angle Regression model
regressor}{linear\_model|LarsCV}

\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{}
  \item \normalizeDescription{False}
  \item \precomputeDescription{}
  \item \maxIterDescription{}
  \item \nAlphasDescription{}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, sets the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
\end{itemize}

\subparagraph{Cross-validated Lasso, using the LARS algorithm}
\mbox{}

The \textit{Cross-validated Lasso, using the LARS algorithm} is a
cross-validated Lasso, using the LARS algorithm. 

\skltype{Cross-validated Lasso, using the LARS algorithm
regressor}{linear\_model|LassoLarsCV}

\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{}
  \item \normalizeDescription{False}
  \item \precomputeDescription{}
  \item \maxIterDescription{}
  \item \nAlphasDescription{}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, specifies the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
\end{itemize}

\subparagraph{Lasso model fit with Lars using BIC or AIC for model selection}
\mbox{}

The \textit{Lasso model fit with Lars using BIC or AIC for model selection} is
a Lasso model fit with Lars using BIC or AIC for model selection.
\maljdan{redundant}
The optimization objective for Lasso is:
$(1 / (2 * n\_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
AIC is the Akaike information criterion and BIC is the Bayes information
criterion.
%
Such criteria are useful in selecting the value of the regularization parameter
by making a trade-off between the goodness of fit and the complexity of the
model.
%
A good model explains the data well while maintaining simplicity.
%
\skltype{Lasso model fit with Lars using BIC or AIC for
  model selection regressor}{linear\_model|LassoLarsIC}
\begin{itemize}
  \item \xmlNode{criterion}, \xmlDesc{`bic' | `aic' }, specifies the type of
  criterion to use.
  %
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{}
  \item \normalizeDescription{False}
  \item \precomputeDescription{}
  \item \maxIterDescription{}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, represents the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the tol parameter in some iterative optimization-based algorithms, this
  parameter does not control the tolerance of the optimization.
  %
\end{itemize}

\subparagraph{Ordinary least squares Linear Regression}
\mbox{}

The \textit{Ordinary least squares Linear Regression} is a method for
estimating the unknown parameters in a linear regression model, with the goal of
minimizing the differences between the observed responses in some arbitrary
dataset and the responses predicted by the linear approximation of the data.
%
\skltype{Ordinary least squares Linear
Regressor}{linear\_model|LinearRegression}

\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
\end{itemize}

\subparagraph{Logistic Regression}
\mbox{}
\\The \textit{Logistic Regression} implements L1 and L2 regularized logistic
regression using the liblinear library.
%
It can handle both dense and sparse input.
%
This regressor uses C-ordered arrays or CSR matrices containing 64-bit floats
for optimal performance; any other input format will be converted (and copied).
%
\skltype{Logistic Regressor}{linear\_model|LogisticRegression}
\begin{itemize}
  \item \xmlNode{penalty}, \xmlDesc{string, `l1' or `l2'}, specifies the norm
  used in the penalization.
  %
  \item \xmlNode{dual}, \xmlDesc{boolean}, specifies the dual or primal
  formulation.
  %
  Dual formulation is only implemented for the l2 penalty.
  %
  Prefer dual=False when n\_samples > n\_features.
  %
  \item \xmlNode{C}, \xmlDesc{float, optional field}, is the inverse of the
  regularization strength; must be a positive float.
  %
  Like in support vector machines, smaller values specify stronger
  regularization.
  %
  \default{1.0}
  \item \xmlNode{fit\_intercept}, \xmlDesc{boolean}, specifies if a constant
  (a.k.a. bias or intercept) should be added to the decision function.
  %
  \default{True}
  \item \xmlNode{intercept\_scaling}, \xmlDesc{float, optional field}, when
  self.fit\_intercept is True, instance vector x becomes [x,
  self.intercept\_scaling], i.e. a ``synthetic'' feature with constant value
  equal to intercept\_scaling is appended to the instance vector.
  %
  The intercept becomes intercept\_scaling * synthetic feature
  weight.
  \nb The synthetic feature weight is subject to l1/l2 regularization as are all
  other features.
  %
  To lessen the effect of regularization on synthetic feature weight (and
  therefore on the intercept) intercept\_scaling has to be increased.
  \default{1.0}
  \item \xmlNode{class\_weight}, \xmlDesc{\{dict, `auto'\}, optional},
  over-/undersamples the samples of each class according to the given weights.
  %
  If not given, all classes are supposed to have weight one.
  %
  The `auto' mode selects weights inversely proportional to class frequencies
  in the training set.
  %
  \item \randomStateDescription{None}
  \item \tolDescriptionC{}
\end{itemize}

\subparagraph{Multi-task Lasso model trained with L1/L2 mixed-norm as
  regularizer}
\mbox{}
\\The \textit{Multi-task Lasso model trained with L1/L2 mixed-norm as
  regularizer} is a regressor where the optimization objective for Lasso is:
$(1 / (2 * n\_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21$
Where:
$||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
%
\skltype{Multi-task Lasso model trained with L1/L2
  mixed-norm as regularizer regressor}{linear\_model|MultiTaskLasso}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, sets the constant 
  multiplier for the L1/L2 term.
  %
  \default{1.0}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{}
\end{itemize}

\subparagraph{Multi-task Elastic Net model trained with L1/L2 mixed-norm as
  regularizer}
\mbox{}

The \textit{Multi-task Elastic Net model trained with L1/L2 mixed-norm as
  regularizer} is a regressor where the optimization objective for
MultiTaskElasticNet is:
$(1 / (2 * n\_samples)) * ||Y - XW||^Fro_2
+ alpha * l1\_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1\_ratio) * ||W||_Fro^2$
Where:
$||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
%
\skltype{Multi-task ElasticNet model trained with L1/L2
  mixed-norm as regularizer regressor}{linear\_model|MultiTaskElasticNet}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, optional field}, represents a constant
  multiplier for the L1/L2 term.
  %
  \default{1.0}
  \item \xmlNode{l1\_ratio}, \xmlDesc{float}, represents the Elastic Net mixing
  parameter, with $0 < l1\_ratio \leq 1$.
  %
  For $l1\_ratio = 0$ the penalty is an L1/L2 penalty.
  %
  For $l1\_ratio = 1$ it is an L1 penalty.
  %
  For $0 < l1\_ratio < 1$, the penalty is a combination of L1/L2
  and L2.
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{}
  \item \tolDescriptionB{1.e-4}
  \item \warmStartDescription{}
\end{itemize}

\subparagraph{Orthogonal Mathching Pursuit model (OMP)}
\mbox{}

The \textit{Orthogonal Mathching Pursuit model (OMP)} is a type of sparse
approximation which involves finding the ``best matching'' projections of
multidimensional data onto an over-complete dictionary, $D$.
%
\skltype{Orthogonal Mathching Pursuit model (OMP)
regressor}{linear\_model|OrthogonalMatchingPursuit}
\begin{itemize}
  \item \xmlNode{n\_nonzero\_coefs}, \xmlDesc{int, optional field}, represents
  the desired number of non-zero entries in the solution.
  %
  If None, this value is set to 10\% of n\_features.
  %
  \default{None}
  \item \xmlNode{tol}, \xmlDesc{float, optional field}, specifies the maximum
  norm of the residual.
  %
  If not None, overrides n\_nonzero\_coefs.
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \xmlNode{precompute}, \xmlDesc{\{True, False, `auto'\}}, specifies
  whether to use a precomputed Gram and Xy matrix to speed up calculations.
  %
  Improves performance when n\_targets or n\_samples is very large.
  %
  \nb If you already have such matrices, you can pass them directly to the
  fit method.
  %
  \default{`auto'}
\end{itemize}

\subparagraph{Cross-validated Orthogonal Mathching Pursuit model (OMP)}
\mbox{}

The \textit{Cross-validated Orthogonal Mathching Pursuit model (OMP)} is a
regressor similar to OMP which has good performance in sparse recovery.
%
\skltype{Cross-validated Orthogonal Mathching Pursuit model (OMP)
regressor}{linear\_model|OrthogonalMatchingPursuitCV}
\begin{itemize}
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \maxIterDescription{}
  %
  Maximum number of iterations to perform, therefore maximum features to
  include 10\% of n\_features but at least 5 if available.
  %
  \item \xmlNode{cv}, \xmlDesc{cross-validation generator, optional},
  %
  see sklearn.cross\_validation.
  %
  \default{5-fold strategy}
  \item \verDescriptionB{}
\end{itemize}

\subparagraph{Passive Aggressive Classifier}
\mbox{}
\\The \textit{Passive Aggressive Classifier} is a principled approach to linear
classification that advocates minimal weight updates i.e., the least required 
to correctly classify the current training instance.
%
\skltype{Passive Aggressive
Classifier}{linear\_model|PassiveAggressiveClassifier}
\begin{itemize}
  \item \xmlNode{C}, \xmlDesc{float}, specifies the maximum step size
  (regularization).
  %
  \default{1.0}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{False}
  \item \randomStateDescription{None}
  \item \verDescriptionB{}
  \item \xmlNode{loss}, \xmlDesc{string, optional field}, the loss function to
  be used:
  \begin{itemize}
    \item hinge: equivalent to PA-I (http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)
    \item squared\_hinge: equivalent to PA-II (http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)
  \end{itemize}
 
  %
  \item \warmStartDescription{}
\end{itemize}

\subparagraph{Passive Aggressive Regressor}
\mbox{}
\\The \textit{Passive Aggressive Regressor} is similar to the Perceptron in that
it does not require a learning rate.
%
However, contrary to the Perceptron, this regressor includes a regularization
parameter, $C$.

\skltype{Passive Aggressive Regressor}{linear\_model|PassiveAggressiveRegressor}
\begin{itemize}
  \item \xmlNode{C}, \xmlDesc{float}, sets the maximum step size
  (regularization).
  %
  \default{1.0}
  %
  \item \xmlNode{epsilon}, \xmlDesc{float}, if the difference between the
  current prediction and the correct label is below this threshold, the model is
  not updated.
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{False}
  \item \randomStateDescription{None}
  \item \verDescriptionB{}
  \item \xmlNode{loss}, \xmlDesc{string, optional field}, specifies the loss
  function to be used: 
  \begin{itemize}
    \item epsilon\_insensitive: equivalent to PA-I in the reference paper (http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf).
    \item squared\_epsilon\_insensitive: equivalent to PA-II in the reference paper (http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf).
  \end{itemize}
  %
  \item \warmStartDescription{}
\end{itemize}

\subparagraph{Perceptron}
\mbox{}

The \textit{Perceptron} method is an algorithm for supervised classification of
an input into one of several possible non-binary outputs.
%
It is a type of linear classifier, i.e. a classification algorithm that makes
its predictions based on a linear predictor function combining a set of weights
with the feature vector.
%
The algorithm allows for online learning, in that it processes elements in the
training set one at a time.
%
\skltype{Perceptron classifier}{linear\_model|Perceptron}
\begin{itemize}
  \item \xmlNode{penalty}, \xmlDesc{None, `l2' or `l1' or `elasticnet'}, defines
  the penalty (aka regularization term) to be used.
  %
  \default{None}
  %
  \item \xmlNode{alpha}, \xmlDesc{float}, sets the constant multiplier for the
  regularization term if regularization is used.
  %
  \default{0.0001}
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{False}
  \item \randomStateDescription{None}
  \item \verDescriptionB{}
  \item \xmlNode{eta0}, \xmlDesc{double, optional field}, defines the constant
  multiplier for the updates.
  %
  \default{1.0}
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dict, {class\_label}}, specifies the
  preset for the class\_weight fit parameter.
  %
  Weights associated with classes.
  %
  If not given, all classes are supposed to have weight one.
  %
  The ``auto'' mode uses the values of y to automatically adjust weights
  inversely proportional to class frequencies.
  %
  \item \warmStartDescription{}
\end{itemize}

\subparagraph{Randomized Lasso}
\mbox{}
\\The \textit{Randomized Lasso} works by resampling the training data and
computing a Lasso on each resampling.
%
In short, the features selected more often are good features.
%
It is also known as stability selection.
%
\skltype{Randomized Lasso regressor}{linear\_model|RandomizedLasso}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, `aic', or `bic', optional}, defines the
  regularization parameter $\alpha$ in the Lasso.
  %
  %
  \item \xmlNode{scaling}, \xmlDesc{float, optional field}, specifies the alpha
  parameter used to randomly scale the
  %
  Should be between 0 and 1.
  %
  \item \xmlNode{sample\_fraction}, \xmlDesc{float, optional field}, defines
  the fraction of samples to be used in each randomized design.
  %
  Should be between 0 and 1.
  %
  If 1, all samples are used.
  %
  \item \xmlNode{n\_resampling}, \xmlDesc{int, optional field}, is the number of
  randomized models used.
  %
  \item \xmlNode{selection\_threshold}, \xmlDesc{float, optional field}, sets
  the score above for which features should be selected.
  %
  \item \fitInterceptDescription{True}
  \item \verDescriptionB{}
  \item \normalizeDescription{True}
  \item \precomputeDescription{}
  \item \maxIterDescription{}
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, defines the machine
  precision regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  Unlike the `tol' parameter in some iterative optimization-based algorithms,
  this parameter does not control the tolerance of the optimization.
  %
  \item \xmlNode{random\_state}, \xmlDesc{int, RandomState instance or None,
  optional}, if int, random\_state is the seed used by the random number
  generator; If RandomState instance, random\_state is the random
  number generator; If None, the random number generator is the RandomState
  instance used by np.random.
  %
  \default{None}
\end{itemize}

\subparagraph{Randomized Logistic Regression}
\mbox{}

The \textit{Randomized Logistic Regression} works by resampling the training
data and computing a logistic regression on each resampling.
%
In short, the features selected more often are good features.
%
It is also known as stability selection.
%
\skltype{Randomized Logistic
Regressor}{linear\_model|RandomizedLogisticRegression}
\begin{itemize}
  \item \xmlNode{C}, \xmlDesc{float, optional field}, specifies the
  regularization parameter $C$ in the LogisticRegression.
  %
  \default{1}
  %
  \item \xmlNode{scaling}, \xmlDesc{float, optional field (default=0.5)}, sets
  the alpha parameter used to randomly scale
  the features. 
  %
  Should be between 0 and 1.
  %
  \item \xmlNode{sample\_fraction}, \xmlDesc{float, optional field}, is the
  fraction of samples to be used in each randomized design.
  %
  Should be between 0 and 1.
  %
  If 1, all samples are used.
  %
  \default{0.75}
  %
  \item \xmlNode{n\_resampling}, \xmlDesc{int, optional field}, sets the number
  of randomized models.
  %
  \default{200}
  %
  \item \xmlNode{selection\_threshold}, \xmlDesc{float, optional field}, sets
  the score above which features should be selected.
  %
  \default{0.25}
  %
  \item \fitInterceptDescription{True}
  %
  \item \verDescriptionB{}
  \item \normalizeDescription{False}
  \item \tolDescriptionC{}
  \default{1=e-3}
  %
  \item \xmlNode{random\_state}, \xmlDesc{int, RandomState instance or None,
  optional}, if int, random\_state is the seed used by the random number
  generator; If RandomState instance, random\_state is the random
  number generator; If None, the random number generator is the RandomState
  instance used by np.random.
  %
  \default{None}
  %
\end{itemize}

\subparagraph{Linear least squares with l2 regularization}
\mbox{}
\\The \textit{Linear least squares with l2 regularization} solves a regression
model where the loss function is the linear least squares function and the
regularization is given by the l2-norm.
%
Also known as Ridge Regression or Tikhonov regularization.
%
This estimator has built-in support for multivariate regression (i.e., when y
is a 2d-array of shape [n\_samples, n\_targets]).
%
\skltype{Linear least squares with l2 regularization}{linear\_model|Ridge}
\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float, array-like},
  %
  shape = [n\_targets] Small positive values of alpha improve the
  conditioning of the problem and reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^-1$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  If an array is passed, penalties are assumed to be specific to the targets.
  %
  Hence they must correspond in number.
  %
  \item \fitInterceptDescription{True}
  \item \maxIterDescription{determined by scipy.sparse.linalg.}
  \item \normalizeDescription{True}
  \item \solverDescription
\end{itemize}

\subparagraph{Classifier using Ridge regression}
\mbox{}

The \textit{Classifier using Ridge regression} is a classifier based on linear
least squares with l2 regularization.
\skltype{Classifier using Ridge regression}{linear\_model|RidgeClassifier}

\begin{itemize}
  \item \xmlNode{alpha}, \xmlDesc{float}, small positive values of alpha improve
  the conditioning of the problem and reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^-1$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dict, optional field}, specifies
  weights associated with classes in the form {class\_label: weight}.
  %
  If not given, all classes are assumed to have weight one.
  %
  \item \fitInterceptDescription{True}
  \item \maxIterDescription{determined by scipy.sparse.linalg.}
  \item \normalizeDescription{False}
  \item \solverDescription
  \item \xmlNode{tol}, \xmlDesc{float}, defines the required precision of the
  solution.
\end{itemize}

\subparagraph{Ridge classifier with built-in cross-validation}
\mbox{}
\\The \textit{Ridge classifier with built-in cross-validation} performs
Generalized Cross-Validation, which is a form of efficient leave-one-out
cross-validation.
%
Currently, only the n\_features > n\_samples case is handled efficiently.
%
\skltype{Ridge classifier with built-in cross-validation
classifier}{linear\_model|RidgeClassifierCV}
\begin{itemize}
  \item \xmlNode{alphas}, \xmlDesc{numpy array of shape [n\_alphas]}, is an
  array of alpha values to try.
  %
  Small positive values of alpha improve the conditioning of the problem and
  reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^{-1}$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \xmlNode{scoring}, \xmlDesc{string, callable or None, optional}, is a
  string (see model evaluation documentation) or a scorer callable object /
  function with signature scorer(estimator, X, y).
  %
  \default{None}
  \item \xmlNode{cv}, \xmlDesc{cross-validation generator, optional},
  %
  If None, Generalized Cross-Validation (efficient leave-one-out) will be used.
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dic, optional field}, specifies 
  weights associated with classes in the form {class\_label:weight}.
  %
  If not given, all classes are supposed to have weight one.
  %
\end{itemize}

\subparagraph{Ridge regression with built-in cross-validation}
\mbox{}

The \textit{Ridge regression with built-in cross-validation} performs
Generalized Cross-Validation, which is a form of efficient leave-one-out
cross-validation.
%
\skltype{Ridge regression with built-in cross-validation regressor}{linear\_model|RidgeCV}
\begin{itemize}
  \item \xmlNode{alphas}, \xmlDesc{numpy array of shape [n\_alphas]}, specifies
  an array of alpha values to try.
  %
  Small positive values of alpha improve the conditioning of the problem and
  reduce the variance of the estimates.
  %
  Alpha corresponds to $(2*C)^{-1}$ in other linear models such as
  LogisticRegression or LinearSVC.
  %
  \item \fitInterceptDescription{True}
  \item \normalizeDescription{False}
  \item \xmlNode{scoring}, \xmlDesc{string, callable or None, optional}, is a
  string (see model evaluation documentation) or a scorer callable object /
  function with signature scorer(estimator, X, y).
  %
  \default{None}
  %
  \item \xmlNode{cv}, \xmlDesc{cross-validation generator, optional field}, if
  None, Generalized Cross-Validation (efficient leave-one-out) will be used.
  %
  \item \xmlNode{gcv\_mode}, \xmlDesc{\{None, `auto,' `svd,' `eigen'\}, optional
  field}, is a flag indicating which strategy to use when performing Generalized
  Cross-Validation.
  %
  Options are:
	\begin{itemize}
    \item `auto:' use svd if n\_samples > n\_features or when X is a
    sparse matrix, otherwise use eigen
  	\item `svd:' force computation via singular value decomposition of $X$
    (does not work for sparse matrices)
	  \item `eigen:' force computation via eigendecomposition of $X^T X$
	\end{itemize}
	The `auto' mode is the default and is intended to pick the cheaper
  option of the two depending upon the shape and format of the training data.
  %
  \default{`auto'}
  \item \xmlNode{store\_cv\_values}, \xmlDesc{boolean}, is a flag indicating if
  the cross-validation values corresponding to each alpha should be stored in
  the cv\_values\_attribute (see below).
  %
  This flag is only compatible with cv=None (i.e. using Generalized
  Cross-Validation).
  %
  \default{False}
\end{itemize}

\subparagraph{Linear classifiers (SVM, logistic regression, a.o.) with SGD
training}
\mbox{}

The \textit{Linear classifiers (SVM, logistic regression, a.o.) with SGD
training} implements regularized linear models with stochastic gradient
descent (SGD) learning: the gradient of the loss is estimated for each sample at
a time and the model is updated along the way with a decreasing strength
schedule (aka learning rate).
%
SGD allows minibatch (online/out-of-core) learning, see the partial\_fit method.
%
This implementation works with data represented as dense or sparse arrays of
floating point values for the features.
%
The model it fits can be controlled with the loss parameter; by default, it fits
a linear support vector machine (SVM).
%
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared Euclidean norm L2 or
the absolute norm L1 or a combination of both (Elastic Net).
%
If the parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieves
online feature selection.
%
\skltype{Linear classifiers (SVM, logistic regression, a.o.) with SGD
training}{linear\_model|SGDClassifier}
\begin{itemize}
  \item \xmlNode{loss}, \xmlDesc{str, `hinge,' `log,' `modified\_huber,'
  `squared\_hinge,' `perceptron,' or a regression loss: `squared\_loss,'
  `huber,' `epsilon\_insensitive,' or `squared\_epsilon\_insensitive'},
  %
  dictates the loss function to be used.
  %
  The available options are:
  \begin{itemize}
    \item `hinge' gives a linear SVM.
    \item `log' loss gives logistic regression, a probabilistic classifier.
    \item `modified\_huber' is another smooth loss that brings tolerance to
    outliers as well as probability estimates.
    \item `squared\_hinge' is like hinge but is quadratically penalized.
    \item `perceptron' is the linear loss used by the perceptron algorithm.
  \end{itemize}
  The other losses are designed for regression but can be useful in
  classification as well; see SGDRegressor for a description.
  %
  \default{`hinge'}
  %
  \item \xmlNode{penalty}, \xmlDesc{str, `l2' or `l1' or `elasticnet'}, defines
  the penalty (aka regularization term) to be used.
  %
  `l2' is the standard regularizer for linear SVM models.
  %
  `l1' and `elasticnet' might bring sparsity to the model (feature
  selection) not achievable with `l2.'
  %
  \default{`l2'}
  \item \xmlNode{alpha}, \xmlDesc{float}, is the constant multiplier for the
  regularization term.
  %
  \default{0.0001}
  \item \xmlNode{l1\_ratio}, \xmlDesc{float}, is the Elastic Net mixing
  parameter, with 0 <= l1\_ratio <= 1.
  %
  l1\_ratio=0 corresponds to L2 penalty, l1\_ratio=1 to L1.
  %
  \default{0.15}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{False}
  \item \randomStateSVMDescription{None}
  \item \verDescriptionB{}
  \item \xmlNode{epsilon}, \xmlDesc{int, optional field}, is the number of CPUs
  to use to do the OVA (One Versus All, for multi-class problems) computation.
  %
  -1 means `all CPUs.'
  %
  \default{1}
  %
  \item \xmlNode{learning\_rate}, \xmlDesc{string, optional field}, specifies
  the learning rate:
  \begin{itemize}
    \item `constant:' eta = eta0 
    \item `optimal:' eta = 1.0 / (t + t0)
    \item `invscaling:' eta = eta0 / pow(t, power\_t)
  \end{itemize}
  \default{invscaling}
  %
  \item \xmlNode{eta0}, \xmlDesc{double}, specifies the initial learning rate
  for the `constant' or `invscaling' schedules.
  %
  The default value is 0.0 as eta0 is not used by the default schedule
  `optimal.'
  %
  \default{0.0}
  %
  \item \xmlNode{power\_t}, \xmlDesc{double}, represents the exponent for
  the inverse scaling learning rate.
  %
  \default{0.5}
  %
  \item \xmlNode{class\_weight}, \xmlDesc{dict, {class\_label}}, is the preset
  for the class\_weight fit parameter.
  %
  Weights associated with classes.
  %
  If not given, all classes are assumed to have weight one.
  %
  The ``auto'' mode uses the values of y to automatically adjust weights
  inversely proportional to class frequencies.
  %
  \item \warmStartDescription{}
  %
\end{itemize}

\subparagraph{Linear model fitted by minimizing a regularized empirical loss
with SGD}
\mbox{}
\\The \textit{Linear model fitted by minimizing a regularized empirical loss
with SGD} is a model where SGD stands for Stochastic Gradient Descent: the
gradient of the loss is estimated each sample at a time and the model is updated
along the way with a decreasing strength schedule (aka learning rate).
%
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm L2 or
the absolute norm L1 or a combination of both (Elastic Net).
%
If the parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieving
online feature selection.
%
This implementation works with data represented as dense numpy arrays of
floating point values for the features.
%
\skltype{Linear model fitted by minimizing a regularized empirical loss with SGD}{linear\_model|FW\_NAME}
\begin{itemize}
  \item \xmlNode{loss}, \xmlDesc{str, `squared\_loss,' `huber,'
  `epsilon\_insensitive,' or `squared\_epsilon\_insensitive'}, specifies the
  loss function to be used.
  %
  Defaults to `squared\_loss' which refers to the ordinary least squares fit.
  %
  `huber' modifies `squared\_loss' to focus less on getting outliers correct by
  switching from squared to linear loss past a distance of epsilon.
  %
  `epsilon\_insensitive' ignores errors less than epsilon and is linear past
  that; this is the loss function used in SVR.
  %
  `squared\_epsilon\_insensitive' is the same but becomes squared loss past a
  tolerance of epsilon.
  %
  \default{`squared\_loss'}
  \item \xmlNode{penalty}, \xmlDesc{str, `l2' or `l1' or `elasticnet'}, sets
  the penalty (aka regularization term) to be used.
  %
  Defaults to `l2' which is the standard regularizer for linear SVM models.
  %
  `l1' and `elasticnet' might bring sparsity to the model (feature
  selection) not achievable with `l2'.
  %
  \default{`l2'}
  %
  \item \xmlNode{alpha}, \xmlDesc{float},
  %
  Constant that multiplies the regularization term.
  %
  Defaults to 0.0001
  \item \xmlNode{l1\_ratio}, \xmlDesc{float}, is the Elastic Net mixing
  parameter, with $0 \leq l1\_ratio \leq 1$.
  %
  l1\_ratio=0 corresponds to L2 penalty, l1\_ratio=1 to L1.
  %
  \default{0.15}
  %
  \item \fitInterceptDescription{True}
  \item \nIterDescriptionB{5}
  \item \shuffleDescription{False}
  \item \randomStateDescription{None}
  \item \verDescriptionB{}
  %
  \item \xmlNode{epsilon}, \xmlDesc{float}, sets the epsilon in the
  epsilon-insensitive loss functions; only if loss is `huber,'
  `epsilon\_insensitive,' or `squared\_epsilon\_insensitive.'
  %
  For `huber', determines the threshold at which it becomes less important
  to get the prediction exactly right.
  %
  For epsilon-insensitive, any differences between the current prediction and
  the correct label are ignored if they are less than this threshold.
  %
  \item \xmlNode{learning\_rate}, \xmlDesc{string, optional field}, 
  Learning rate:
  \begin{itemize}
    \item constant: eta = eta0
    \item optimal: eta = 1.0/(t+t0)
    \item invscaling: eta= eta0 / pow(t, power\_t)
  \end{itemize}
  \default{invscaling}
  \item \xmlNode{eta0}, \xmlDesc{double}, specifies the initial learning rate.
  %
  \default{0.01}
  %
  \item \xmlNode{power\_t}, \xmlDesc{double, optional field}, specifies the
  exponent for inverse scaling learning rate.
  %
  \default{0.25}
  %
  \item \warmStartDescription{}
  %
\end{itemize}

\subparagraph{Compute Least Angle Regression or Lasso path using LARS algorithm}
\mbox{}
\\The \textit{Compute Least Angle Regression or Lasso path using LARS algorithm}
is a regressor where the optimization objective for the case method=`lasso'
is:
$(1 / (2 * n\_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
in the case of method='lars', the objective function is only known in the
form of an implicit equation.
%
\skltype{Compute Least Angle Regression or Lasso path using LARS algorithm
regressor}{linear\_model|lars\_path}
\begin{itemize}
  \item \xmlNode{X}, \xmlDesc{array, shape: (n\_samples,n\_features)}, is the
  input data.
  %
  \item \xmlNode{y}, \xmlDesc{array, shape: (n\_samples)}, is the set of input
  targets.
  %
  \item \maxIterDescription{500}
  %
  \item \xmlNode{gram}, \xmlDesc{None, `auto', array, shape:
      (n\_features, n\_features), optional},
  %
  Precomputed Gram matrix (X' * X), if `auto', the Gram matrix is precomputed
  from the given X, if there are more samples than features.
  %
  \item \xmlNode{alpha\_min}, \xmlDesc{float, optional field}, sets the minimum
  correlation along the path.
  %
  It corresponds to the regularization parameter alpha parameter in the Lasso.
  %
  \default{0}
  %
  \item \xmlNode{method}, \xmlDesc{{`lar', `lasso'}, optional}, specifies the 
  returned model.
  %
  Select `lar' for Least Angle Regression, `lasso' for the Lasso.
  %
  \default{`lar'}
  %
  \item \xmlNode{eps}, \xmlDesc{float, optional}, sets the machine-precision
  regularization in the computation of the Cholesky diagonal factors.
  %
  Increase this for very ill-conditioned systems.
  %
  \default{``np.finfo(np.float).eps''}
  %
  \item \verDescriptionB{}
\end{itemize}

\subparagraph{Compute Lasso path with coordinate descent}
\mbox{}
\\The \textit{Compute Lasso path with coordinate descent} is a regressor where
the Lasso optimization function varies for mono and multi-outputs.
%
For mono-output tasks it is:
$(1 / (2 * n\_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
For multi-output tasks it is:
$(1 / (2 * n\_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21$
Where:
$||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
%
\skltype{Compute Lasso path with coordinate descent regressor}{linear\_model|lasso\_path}
\begin{itemize}
  \item \xmlNode{X}, \xmlDesc{{array-like, sparse matrix}, shape (n\_samples,
  n\_features)}, represents the training data.
  %
  Pass directly as Fortran-contiguous data to avoid unnecessary memory
  duplication.
  %
  If y is mono-output then X can be sparse.
  %
  \item \xmlNode{y}, \xmlDesc{ndarray, shape = (n\_samples,), or (n\_samples,
  n\_outputs)}, represents the target values.
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, specifies the length of
  the path.
  %
  eps=1e-3 means that alpha\_min / alpha\_max = 1e-3.
  \item \xmlNode{n\_alphas}, \xmlDesc{int, optional field}, is the number of
  alphas along the regularization path.
  \item \xmlNode{alphas}, \xmlDesc{ndarray, optional field}, lists the alphas
  where the models will be computed.
  %
  If None, alphas are set automatically.
  \item \xmlNode{precompute}, \xmlDesc{True | False | `auto' | array-like},
  decides whether to use a precomputed Gram matrix to speed up calculations or
  not.
  %
  If set to `auto,' RAVEN will decide.
  %
  The Gram matrix can also be passed as argument.
  %
  \item \xmlNode{Xy}, \xmlDesc{array-like, optional field},
  %
  Xy = np.dot(X.T, y) that can be precomputed.
  %
  It is useful only when the Gram matrix is precomputed.
  %
  \item \xmlNode{copy\_X}, \xmlDesc{boolean, optional field}, if True, X will be
  copied; else, it may be overwritten.
  %
  \default{True}
  %
  \item \xmlNode{coef\_init}, \xmlDesc{array, shape (n\_features, ) | None}, is
  the initial set of values of the coefficients.
  %
  \item \verDescriptionB{}
\end{itemize}

\subparagraph{Stability path based on randomized Lasso estimates}
\mbox{}
\skltype{Stability path based on randomized Lasso
  estimates regressor}{linear\_model|lasso\_stability\_path}
\begin{itemize}
  \item \xmlNode{X}, \xmlDesc{array-like, shape = [n\_samples,
      n\_features]}, represents the training data.
  %
  \item \xmlNode{y}, \xmlDesc{array-like, shape = [n\_samples]}, represents the
  target values.
  %
  \item \xmlNode{scaling}, \xmlDesc{float, optional, default=0.5},sets the alpha
  parameter used to randomly scale the
  features.
  %
  Should be between 0 and 1.
  %
  \item \xmlNode{random\_state}, \xmlDesc{integer or
  numpy.random.RandomState, optional}, sets the generator used to randomize the
  design.
  %
  \item \xmlNode{n\_resampling}, \xmlDesc{int, optional}, sets the number of
  randomized models.
  %
  \default{200}
  %
  \item \xmlNode{n\_grid}, \xmlDesc{int, optional}, specifies the number of grid
  points.
  %
  The path is linearly reinterpolated on a grid between 0 and 1 before computing
  the scores.
  %
  \default{100}
  %
  \item \xmlNode{sample\_fraction}, \xmlDesc{float, optional}, determines the
  fraction of samples to be used in each randomized design.
  %
  Should be between 0 and 1.
  %
  If 1, all samples are used.
  %
  \default{0.75}
  %
  \item \xmlNode{eps}, \xmlDesc{float, optional field}, is the smallest value of 
  alpha / alpha\_max considered.
  \item \verDescriptionB{}
\end{itemize}

\subparagraph{Gram Orthogonal Matching Pursuit (OMP)}
\mbox{}
\\The \textit{Gram OMP} solves n\_targets Orthogonal Matching Pursuit problems
using only the Gram matrix, X.T * X, and the product, X.T * y.
%
\skltype{Gram OMP regressor}{linear\_model|orthogonal\_mp\_gram}
\begin{itemize}
  \item \xmlNode{Gram}, \xmlDesc{array, shape (n\_features,n\_features)}, 
  contains the Gram matrix of the input data: X.T * X
  \item \xmlNode{Xy}, \xmlDesc{array, shape (n\_features,) or
  (n\_features, n\_targets)}, specifies the input targets multiplied by X:
  X.T * y.
  \item \xmlNode{n\_nonzero\_coefs}, \xmlDesc{int}, sets the desired number of
  non-zero entries in the solution.
  %
  If None this value is set to 10\% of n\_features.
  %
  \default{None}
  %
  \item \xmlNode{tol}, \xmlDesc{float}, sets the maximum norm of the residual.
  %
  If not None, overrides n\_nonzero\_coefs.
  %
  \item \xmlNode{norms\_squared}, \xmlDesc{array-like, shape(n\_targets)},
  \maljdan{needs description}
  %
\end{itemize}

%%%%% ROM Model - SciKitLearn: Support Vector Machines %%%%%%%
\paragraph{Support Vector Machines}
\label{SVM}
In machine learning, \textbf{Support Vector Machines} (SVMs, also support vector
networks) are supervised learning models with associated learning algorithms
that analyze data and recognize patterns, used for classification and regression
analysis.
%
Given a set of training examples, each marked as belonging to one of two
categories, an SVM training algorithm builds a model that assigns new examples
into one category or the other, making it a non-probabilistic binary linear
classifier.
%
An SVM model is a representation of the examples as points in space, mapped so
that the examples of the separate categories are divided by a clear gap that is
as wide as possible.
%
New examples are then mapped into that same space and predicted to belong to a
category based on which side of the gap they fall on.
%
In addition to performing linear classification, SVMs can efficiently perform a
non-linear classification using what is called the kernel trick, implicitly
mapping their inputs into high-dimensional feature spaces.
%
In the following, all the SVM models available in RAVEN are reported.

\subparagraph{Linear Support Vector Classifier}
\mbox{}
\\The \textit{Linear Support Vector Classifier} is similar to SVC with parameter
kernel=`linear', but implemented in terms of liblinear rather than libsvm,
so it has more flexibility in the choice of penalties and loss functions and
should scale better (to large numbers of samples).
%
This class supports both dense and sparse input and the multiclass support is
handled according to a one-vs-the-rest scheme.
%
\skltype{Linear Support Vector Classifier}{svm|LinearSVC}
\begin{itemize}
  \item \CSVMDescription{1.0}
  \item \xmlNode{loss}, \xmlDesc{string, `l1' or `l2'}, specifies the loss
  function.
  %
  `l1' is the hinge loss (standard SVM) while `l2' is the squared hinge
  loss.
  %
  \default{`l2'}
  %
  \item \xmlNode{penalty}, \xmlDesc{string, `l1' or `l2'}, specifies the norm
  used in the penalization.
  %
  The `l2' penalty is the standard used in SVC.
  %
  The `l1' leads to coef\_vectors that are sparse.
  %
  \default{`l2'}
  %
  \item \xmlNode{dual}, \xmlDesc{boolean}, selects the algorithm to either solve
  the dual or primal optimization problem.
  %
  Prefer dual=False when n\_samples > n\_features.
  %
  \default{True}
  %
  \item \tolSVMDescription{}
  %
  \item \xmlNode{multi\_class}, \xmlDesc{string, `ovr' or `crammer\_singer'},
  %
  Determines the multi-class strategy if y contains more than two classes.
  %
  ovr trains n\_classes one-vs-rest classifiers, while
  crammer\_singer optimizes a joint objective over all classes.
  %
  While crammer\_singer is interesting from a theoretical perspective as it is
  consistent, it is seldom used in practice and rarely leads to better accuracy
  and is more expensive to compute.
  %
  If crammer\_singer is chosen, the options loss, penalty and dual
  will be ignored.
  %
  \default{`ovr'}
  %
  \item \fitInterceptDescription{True}
  %
  \item \xmlNode{intercept\_scaling}, \xmlDesc{float, optional field}, when
  True, the instance vector x becomes [x,self.intercept\_scaling], i.e. a 
  ``synthetic'' feature with constant value equals to intercept\_scaling is
  appended to the instance vector.
  %
  The intercept becomes intercept\_scaling * synthetic feature
  weight.
  \nb The synthetic feature weight is subject to l1/l2 regularization as are all
  other features.
  %
  To lessen the effect of regularization on the synthetic feature weight (and
  therefore on the intercept) intercept\_scaling has to be increased.
  %
  \default{1}
  %
  \item \classWeightDescription{}
  \item \verDescriptionB{}
  %
  \nb This setting takes advantage of a per-process runtime setting in liblinear 
  that, if enabled, may not work properly in a multithreaded context.
  %
  \item \randomStateSVMDescription{None}
\end{itemize}

\subparagraph{C-Support Vector Classification}
\mbox{}
\\The \textit{C-Support Vector Classification} is a based on libsvm.
%
The fit time complexity is more than quadratic with the number of samples which
makes it hard to scale to datasets with more than a couple of 10000 samples.
%
The multiclass support is handled according to a one-vs-one scheme.
%
\skltype{C-Support Vector Classifier}{svm|SVC}
\begin{itemize}
  \item \CSVMDescription{1.0}
  \item \kernelDescription{`rbf'}
  \item \degreeDescription{3.0}
  \item \gammaDescription{0.0}
  \item \coefZeroDescription{0.0}
  \item \probabilityDescription{False}
  \item \shrinkingDescription{True}
  \item \tolSVMDescription{}
  \item \cacheSizeDescription{}
  \item \classWeightDescription{}
  \item \verSVMDescription{False}
  \item \maxIterDescription{-1}
  \item \randomStateSVMDescription{None}
  %
\end{itemize}

\subparagraph{Nu-Support Vector Classification}
\mbox{}

The \textit{Nu-Support Vector Classification} is similar to SVC but uses a
parameter to control the number of support vectors.
%
The implementation is based on libsvm.
%
\skltype{Nu-Support Vector Classifier}{svm|NuSVC}
\begin{itemize}
  \item \xmlNode{nu}, \xmlDesc{float, optional field}, is an upper bound on the
  fraction of training errors and a lower bound of the fraction of support
  vectors.
  %
  Should be in the interval (0, 1].
  %
  \default{0.5}
  %
  \item \kernelDescription{`rbf'}
  \item \degreeDescription{3.0}
  \item \gammaDescription{0.0}
  \item \coefZeroDescription{0.0}
  \item \probabilityDescription{False}
  \item \shrinkingDescription{True}
  \item \tolSVMDescription{}
  \item \cacheSizeDescription{}
  \item \verSVMDescription{False}
  \item \maxIterDescription{-1}
  \item \randomStateSVMDescription{None}
  %
\end{itemize}

\subparagraph{Support Vector Regression}
\mbox{}
\\The \textit{Support Vector Regression} is an epsilon-Support Vector
Regression.
%
The free parameters in this model are C and epsilon.
%
The implementations is a based on libsvm.
%
\skltype{Support Vector Regressor}{svm|SVR}
\begin{itemize}
  \item \CSVMDescription{1.0}
  \item \xmlNode{epsilon}, \xmlDesc{float, optional field}, specifies the
  epsilon-tube within which no penalty is associated in the training loss
  function with points predicted within a distance epsilon from the actual
  value.
  %
  \default{0.1}
  %
  \item \kernelDescription{`rbf'}
  \item \degreeDescription{3.0}
  \item \gammaDescription{0.0}
  \item \coefZeroDescription{0.0}
  \item \probabilityDescription{False}
  \item \shrinkingDescription{True}
  \item \tolSVMDescription{}
  \item \cacheSizeDescription{}
  \item \verSVMDescription{False}
  \item \maxIterDescription{-1}
  \item \randomStateSVMDescription{None}
  %
\end{itemize}
 %%%%% ROM Model - SciKitLearn: MultiClass %%%%%%%
\paragraph{Multi Class}
\label{Multiclass}
Multiclass classification means a classification task with more than two
classes; e.g., classify a set of images of fruits which may be oranges, apples,
or pears.
%
Multiclass classification makes the assumption that each sample is assigned to
one and only one label: a fruit can be either an apple or a pear but not both at
the same time.
%
In the following, all the multi-class models available in RAVEN are reported.
%
%%%%%%%%%
\subparagraph{One-vs-the-rest (OvR) multiclass/multilabel strategy}
\mbox{}

The \textit{One-vs-the-rest (OvR) multiclass/multilabel strategy}, also known
as one-vs-all, consists in fitting one classifier per class.
%
For each classifier, the class is fitted against all the other classes.
%
In addition to its computational efficiency (only n\_classes classifiers are
needed), one advantage of this approach is its interpretability.
%
Since each class is represented by one and one classifier only, it is possible
to gain knowledge about the class by inspecting its corresponding classifier.
%
This is the most commonly used strategy and is a fair default choice.

\skltype{One-vs-the-rest (OvR) multiclass/multilabel classifier}{multiClass|OneVsRestClassifier}
\begin{itemize}
  \item \estimatorDescription{}
\end{itemize}

%%%%%%%%%%%%
\subparagraph{One-vs-one multiclass strategy}
\mbox{}

The \textit{One-vs-one multiclass strategy} consists in fitting one classifier
per class pair.
%
At prediction time, the class which received the most votes is selected.
%
Since it requires to fit n\_classes * (n\_classes - 1) / 2 classifiers, this
method is usually slower than one-vs-the-rest, due to its O(n\_classes\^2)
complexity.
%
However, this method may be advantageous for algorithms such as kernel
algorithms which do not scale well with n\_samples.
%
This is because each individual learning problem only involves a small subset of
the data whereas, with one-vs-the-rest, the complete dataset is used n\_classes
times.

\skltype{One-vs-one multiclass classifier}{multiClass|OneVsOneClassifier}
\begin{itemize}
  \item \estimatorDescription{}
\end{itemize}

%%%%%%%%%%%%%
\subparagraph{Error-Correcting Output-Code multiclass strategy}
\mbox{}
\\The \textit{Error-Correcting Output-Code multiclass strategy} consists in
representing each class with a binary code (an array of 0s and 1s).
%
At fitting time, one binary classifier per bit in the code book is fitted.
%
At prediction time, the classifiers are used to project new points in the class
space and the class closest to the points is chosen.
%
The main advantage of these strategies is that the number of classifiers used
can be controlled by the user, either for compressing the model ($0 < code_size
< 1$) or for making the model more robust to errors ($code_size > 1$).

\skltype{Error-Correcting Output-Code multiclass classifier}{multiClass|OutputCodeClassifier}
\begin{itemize}
  \item \estimatorDescription{}
  \item \xmlNode{code\_size}, \xmlDesc{float, required field}, represents the
  percentage of the number of classes to be used to create the code book.
  %
  A number between 0 and 1 will require fewer classifiers than one-vs-the-rest.
  %
  A number greater than 1 will require more classifiers than one-vs-the-rest.
  %
\end{itemize}
%%%%%%%%%%%%%
%\subparagraph{fit a one-vs-the-rest strategy}
%pass
%\subparagraph{Make predictions using the one-vs-the-rest strategy}
%pass
%\subparagraph{ Fit a one-vs-one strategy}
%pass
%\subparagraph{Make predictions using the one-vs-one strategy}
%pass
%\subparagraph{Fit an error-correcting output-code strategy}
%pass
%\subparagraph{Make predictions using the error-correcting output-code strategy}
%pass

 %%%%% ROM Model - SciKitLearn: naiveBayes %%%%%%%
\paragraph{Naive Bayes}
\label{naiveBayes}
Naive Bayes methods are a set of supervised learning algorithms based on
applying Bayes' theorem with the ``naive'' assumption of independence between
every pair of features.
%
Given a class variable y and a dependent feature vector x\_1 through x\_n,
Bayes' theorem states the following relationship:
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}
{P(x_1, \dots, x_n)}
\end{equation}
Using the naive independence assumption that
\begin{equation}
P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),
\end{equation}
for all i, this relationship is simplified to
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
{P(x_1, \dots, x_n)}
\end{equation}
Since $P(x_1, \dots, x_n)$ is constant given the input, we can use the following
classification rule:
\begin{equation}
P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)
\Downarrow
\end{equation}
\begin{equation}
\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),
\end{equation}
and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and
$P(x_i \mid y)$; the former is then the relative frequency of class $y$ in the
training set.
%
The different naive Bayes classifiers differ mainly by the assumptions they make
regarding the distribution of $P(x_i \mid y)$.

In spite of their apparently over-simplified assumptions, naive Bayes
classifiers have worked quite well in many real-world situations, famously
document classification and spam filtering.
%
They require a small amount of training data to estimate the necessary
parameters.
%
(For theoretical reasons why naive Bayes works well, and on which types of data
it does, see the references below.)
Naive Bayes learners and classifiers can be extremely fast compared to more
sophisticated methods.
%
The decoupling of the class conditional feature distributions means that each
distribution can be independently estimated as a one dimensional distribution.
%
This in turn helps to alleviate problems stemming from the curse of
dimensionality.

On the flip side, although naive Bayes is known as a decent classifier, it is
known to be a bad estimator, so the probability outputs from predict\_proba are
not to be taken too seriously.
%
In the following, all the Naive Bayes available in RAVEN are reported.
%
%%%%%%%
\subparagraph{Gaussian Naive Bayes}
\mbox{}
\\The \textit{Gaussian Naive Bayes strategy} implements the Gaussian Naive Bayes
algorithm for classification.
%
The likelihood of the features is assumed to be Gaussian:
\begin{equation}
P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i -
  \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}
The parameters $\sigma_y$ and $\mu_y$ are estimated using maximum likelihood.

In order to use the \textit{Gaussian Naive Bayes strategy}, the user needs to
set the sub-node:

\xmlNode{SKLtype}\texttt{naiveBayes|GaussianNB}\xmlNode{/SKLtype}.

There are no additional sub-nodes available for this method.
%
%%%%%%%%%%%%
\subparagraph{Multinomial Naive Bayes}
\mbox{}
\\The \textit{Multinomial Naive Bayes} implements the naive Bayes algorithm for
multinomially distributed data, and is one of the two classic naive Bayes
variants used in text classification (where the data is typically represented
as word vector counts, although tf-idf vectors are also known to work well in
practice).
%
The distribution is parametrized by vectors $\theta_y =
(\theta_{y1},\ldots,\theta_{yn})$ for each class $y$, where $n$ is the number of
features (in text classification, the size of the vocabulary) and $\theta_{yi}$
is the probability $P(x_i \mid y)$ of feature $i$ appearing in a sample
belonging to class $y$.
%
The parameters $\theta_y$ are estimated by a smoothed version of maximum
likelihood, i.e. relative frequency counting:
\begin{equation}
\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}
\end{equation}
where $N_{yi} = \sum_{x \in T} x_i$ is the number of times feature $i$ appears
in a sample of class y in the training set T, and
$N_{y} = \sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class
$y$.
%
The smoothing priors $\alpha \ge 0$ account for features not present in the
learning samples and prevents zero probabilities in further computations.
%
Setting $\alpha = 1$ is called Laplace smoothing, while $\alpha < 1$ is called
Lidstone smoothing.
%
\skltype{Multinomial Naive Bayes strategy}{naiveBayes|MultinomialNB}
\begin{itemize}
  \item \alphaBayesDescription{1.0}
  \item \fitPriorDescription{False}
  \item \classPriorDescription{None}
\end{itemize}

%%%%%%%%%%%%
\subparagraph{Bernoulli Naive Bayes}
\mbox{}
\\The \textit{Bernoulli Naive Bayes} implements the naive Bayes training and
classification algorithms for data that is distributed according to multivariate
Bernoulli distributions; i.e., there may be multiple features but each one is
assumed to be a binary-valued (Bernoulli, boolean) variable.
%
Therefore, this class requires samples to be represented as binary-valued
feature vectors; if handed any other kind of data, a \textit{Bernoulli Naive
Bayes} instance may binarize its input (depending on the binarize parameter).
%
The decision rule for Bernoulli naive Bayes is based on
\begin{equation}
P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)
\end{equation}
which differs from multinomial NB's rule in that it explicitly penalizes the
non-occurrence of a feature $i$ that is an indicator for class $y$, where the
multinomial variant would simply ignore a non-occurring feature.
%
In the case of text classification, word occurrence vectors (rather than word
count vectors) may be used to train and use this classifier.
%
\textit{Bernoulli Naive Bayes} might perform better on some datasets, especially
those with shorter documents.
%
It is advisable to evaluate both models, if time permits.
%
\skltype{Bernoulli Naive Bayes strategy}{naiveBayes|BernoulliNB}
\begin{itemize}
  \item \alphaBayesDescription{1.0}
  \item \xmlNode{binarize}, \xmlDesc{float, optional field},
  %
  Threshold for binarizing (mapping to booleans) of sample features.
  %
  If None, input is presumed to already consist of binary vectors.
  %
  \default{None}
  \item \fitPriorDescription{False}
  \item \classPriorDescription{None}
  %
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Neighbors %%%%%%%
\paragraph{Neighbors}
\label{Neighbors}

The \textit{Neighbors} class provides functionality for unsupervised and
supervised neighbor-based learning methods.
%
The unsupervised nearest neighbors method is the foundation of many other
learning methods, notably manifold learning and spectral clustering.
%
Supervised neighbors-based learning comes in two flavors: classification for
data with discrete labels, and regression for data with continuous labels.

The principle behind nearest neighbor methods is to find a predefined number of
training samples closest in distance to the new point, and predict the label
from these.
%
The number of samples can be a user-defined constant (k-nearest neighbor
learning), or vary based on the local density of points (radius-based neighbor
learning).
%
The distance can, in general, be any metric measure: standard Euclidean distance
is the most common choice.
%
Neighbor-based methods are known as non-generalizing machine learning methods,
since they simply ``remember'' all of its training data (possibly transformed
into a fast indexing structure such as a Ball Tree or KD Tree.).

In the following, all the Neighbors' models available in RAVEN are reported.
%
%%%%%%%%%%%%%%%
\subparagraph{Nearest Neighbors}
\mbox{}
\\The \textit{Nearest Neighbors} implements unsupervised nearest neighbors
learning.
%
It acts as a uniform interface to three different nearest neighbors algorithms:
BallTree, KDTree, and a brute-force algorithm.
%
%The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute']. When the default value 'auto' is passed, the algorithm attempts to determine the best approach from the training data.
\skltype{Nearest Neighbors strategy}{neighbors|NearestNeighbors}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \radiusDescription{1.0}
  \item \algorithmDescription{auto}
  \item \leafSizeDescription{30}
  \item \pDescription{2}
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Classifier}
\mbox{}
\\The \textit{K Neighbors Classifier} is a type of instance-based learning or
non-generalizing learning: it does not attempt to construct a general internal
model, but simply stores instances of the training data.
%
Classification is computed from a simple majority vote of the nearest neighbors
of each point: a query point is assigned the data class which has the most
representatives within the nearest neighbors of the point.
%
It implements learning based on the $k$ nearest neighbors of each query point,
where $k$ is an integer value specified by the user.

\skltype{K Neighbors Classifier}{neighbors|KNeighborsClassifier}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \weightsDescription{uniform}
  \item \radiusDescription{1.0}
  \item \algorithmDescription{auto}
  \item \metricDescription{minkowski}
  \item \leafSizeDescription{30}
  \item \pDescription{2}
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Classifier}
\mbox{}
\\The \textit{Radius Neighbors Classifier} is a type of instance-based learning
or non-generalizing learning: it does not attempt to construct a general
internal model, but simply stores instances of the training data.
%
Classification is computed from a simple majority vote of the nearest neighbors
of each point: a query point is assigned the data class which has the most
representatives within the nearest neighbors of the point.
%
It implements learning based on the number of neighbors within a fixed radius 
$r$ of each training point, where $r$ is a floating-point value specified by the
user.

\skltype{Radius Neighbors Classifier}{neighbors|RadiusNeighbors}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \weightsDescription{uniform}
  \item \radiusDescription{1.0}
  \item \algorithmDescription{auto}
  \item \metricDescription{minkowski}
  \item \leafSizeDescription{30}
  \item \pDescription{2}
  \item \outlierLabelDescription{None}
\end{itemize}

%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Regressor}
\mbox{}

The \textit{K Neighbors Regressor} can be used in cases where the data labels
are continuous rather than discrete variables.
%
The label assigned to a query point is computed based on the mean of the labels
of its nearest neighbors.
%
It implements learning based on the $k$ nearest neighbors of each query point,
where $k$ is an integer value specified by the user.

\skltype{K Neighbors Regressor}{neighbors|KNeighborsRegressor}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \weightsDescription{uniform}
  \item \radiusDescription{1.0}
  \item \algorithmDescription{auto}
  \item \metricDescription{minkowski}
  \item \leafSizeDescription{30}
  \item \pDescription{2}
\end{itemize}

%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Regressor}
\mbox{}

The \textit{Radius Neighbors Regressor} can be used in cases where the data
labels are continuous rather than discrete variables.
%
The label assigned to a query point is computed based on the mean of the labels
of its nearest neighbors.
%
It implements learning based on the neighbors within a fixed radius $r$ of the
query point, where $r$ is a floating-point value specified by the user.

\skltype{Radius Neighbors Regressor}{neighbors|RadiusNeighborsRegressor}
\begin{itemize}
  \item \nNeighborsDescription{5}
  \item \weightsDescription{uniform}
  \item \radiusDescription{1.0}
  \item \algorithmDescription{auto}
  \item \metricDescription{minkowski}
  \item \leafSizeDescription{30}
  \item \pDescription{2}
  \item \outlierLabelDescription{None}
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Nearest Centroid Classifier}
\mbox{}

The \textit{Nearest Centroid classifier} is a simple algorithm that represents
each class by the centroid of its members.
%
It also has no parameters to choose, making it a good baseline classifier.
%
It does, however, suffer on non-convex classes, as well as when classes have
drastically different variances, as equal variance in all dimensions is assumed.

\skltype{Nearest Centroid Classifier}{neighbors|NearestCentroid}
\begin{itemize}
  \item \xmlNode{shrink\_threshold}, \xmlDesc{float, optional field}, defines
  the threshold for shrinking centroids to remove features.
  %
  \default{None}
  %
\end{itemize}
%\subparagraph{Ball Tree}
%pass
%\subparagraph{K-D Tree}
%pass

 %%%%% ROM Model - SciKitLearn: Quadratic Discriminant Analysis %%%%%%%
\paragraph{Quadratic Discriminant Analysis}
\label{QDA}

The \textit{Quadratic Discriminant Analysis} is a classifier with a quadratic
decision boundary, generated by fitting class conditional densities to the data
and using Bayes' rule.
%
The model fits a Gaussian density to each class.

\skltype{Quadratic Discriminant Analysis Classifier}{qda|QDA}
\begin{itemize}
  \item \xmlNode{priors}, \xmlDesc{array-like (n\_classes), optional field},
  specifies the priors on the classes.
  %
  \default{None}
  \item \xmlNode{reg\_param}, \xmlDesc{float, optional field}, regularizes the
  covariance estimate as (1-reg\_param)*Sigma +
  reg\_param*Identity(n\_features).
  %
  \default{0.0}
  %
\end{itemize}

 %%%%% ROM Model - SciKitLearn: Tree %%%%%%%
\paragraph{Tree}
\label{tree}

Decision Trees (DTs) are a non-parametric supervised learning method used for
classification and regression.
%
The goal is to create a model that predicts the value of a target variable by
learning simple decision rules inferred from the data features.
%
\begin{itemize}
  \item Some advantages of decision trees are:
  \item Simple to understand and to interpret.
  %
  Trees can be visualized.
  %
  \item Requires little data preparation.
  %
  Other techniques often require data normalization, dummy variables need to be
  created and blank values to be removed.
  %
  Note however, that this module does not support missing values.
  %
  \item The cost of using the tree (i.e., predicting data) is logarithmic in the
  number of data points used to train the tree.
  %
  \item Able to handle both numerical and categorical data.
  %
  Other techniques are usually specialized in analyzing datasets that have only
  one type of variable.
  %
  \item Able to handle multi-output problems.
  %
  \item Uses a white box model.
  %
  If a given situation is observable in a model, the explanation for the
  condition is easily explained by boolean logic.
  %
  By contrast, in a black box model (e.g., in an artificial neural network),
  results may be more difficult to interpret.
  %
  \item Possible to validate a model using statistical tests.
  %
  That makes it possible to account for the reliability of the model.
  %
  \item Performs well even if its assumptions are somewhat violated by the true
  model from which the data were generated.
  %
\end{itemize}
The disadvantages of decision trees include:
\begin{itemize}
  \item Decision-tree learners can create over-complex trees that do not
  generalise the data well.
  %
  This is called overfitting.
  %
  Mechanisms such as pruning (not currently supported), setting the minimum
  number of samples required at a leaf node or setting the maximum depth of the
  tree are necessary to avoid this problem.
  %
  \item Decision trees can be unstable because small variations in the data
  might result in a completely different tree being generated.
  %
  This problem is mitigated by using decision trees within an ensemble.
  %
  \item The problem of learning an optimal decision tree is known to be
  NP-complete under several aspects of optimality and even for simple concepts.
  %
  Consequently, practical decision-tree learning algorithms are based on
  heuristic algorithms such as the greedy algorithm where locally optimal
  decisions are made at each node.
  %
  Such algorithms cannot guarantee to return the globally optimal decision tree.
  %
  This can be mitigated by training multiple trees in an ensemble learner, where
  the features and samples are randomly sampled with replacement.
  %
  \item There are concepts that are hard to learn because decision trees do not
  express them easily, such as XOR, parity or multiplexer problems.
  %
  \item Decision tree learners create biased trees if some classes dominate.
  %
  It is therefore recommended to balance the dataset prior to fitting with the
  decision tree.
  %
\end{itemize}

In the following, all the linear models available in RAVEN are reported.

%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Classifier}
\mbox{}
\\The \textit{Decision Tree Classifier} is a classifier that is based on the
decision tree logic.

\skltype{Decision Tree Classifier}{tree|DecisionTreeClassifier}
\begin{itemize}
  \item \criterionDescription{gini}
  \item \splitterDescription{best}
  \item \maxFeaturesDescription{None}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
  \item \maxLeafNodesDescription{None}
\end{itemize}

%%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Regressor}
\mbox{}
\\The \textit{Decision Tree Regressor} is a Regressor that is based on the
decision tree logic.
%
\skltype{Decision Tree Regressor}{tree|DecisionTreeRegressor}
\begin{itemize}
  \item \criterionDescription{gini}
  \item \splitterDescription{best}
  \item \maxFeaturesDescription{None}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
  \item \maxLeafNodesDescription{None}
\end{itemize}

%%%%%%%%%%%%%%%%
\subparagraph{Extra Tree Classifier}
\mbox{}
\\The \textit{Extra Tree Classifier} is an extremely randomized tree classifier.
%
Extra-trees differ from classic decision trees in the way they are built.
%
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the max\_features randomly selected
features and the best split among those is chosen.
%
When max\_features is set 1, this amounts to building a totally random decision
tree.

\skltype{Extra Tree Classifier}{tree|ExtraTreeClassifier}

\begin{itemize}
  \item \criterionDescription{gini}
  \item \splitterDescription{best}
  \item \maxFeaturesDescription{None}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
  \item \maxLeafNodesDescription{None}
  %
\end{itemize}

%%%%%%%%%%%%
\subparagraph{Extra Tree Regressor}
\mbox{}

The \textit{Extra Tree Regressor} is an extremely randomized tree regressor.
%
Extra-trees differ from classic decision trees in the way they are built.
%
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the max\_features randomly selected
features and the best split among those is chosen.
%
When max\_features is set 1, this amounts to building a totally random decision
tree.

\skltype{Extra Tree Regressor}{tree|ExtraTreeRegressor}

\begin{itemize}
  \item \criterionDescription{gini}
  \item \splitterDescription{best}
  \item \maxFeaturesDescription{None}
  \item \maxDepthDescription{None}
  \item \minSamplesSplitDescription{2}
  \item \minSamplesLeafDescription{1}
  \item \maxLeafNodesDescription{None}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Gaussian Process %%%%%%%
\paragraph{Gaussian Process}
\label{GP}
Gaussian Processes for Machine Learning (GPML) is a generic supervised learning
method primarily designed to solve regression problems.
%
The advantages of Gaussian Processes for Machine Learning are:
\begin{itemize}
  \item The prediction interpolates the observations (at least for regular
  correlation models).
  \item The prediction is probabilistic (Gaussian) so that one can compute
  empirical confidence intervals and exceedance probabilities that might be used
  to refit (online fitting, adaptive fitting) the prediction in some region of
  interest.
  \item Versatile: different linear regression models and correlation models can
  be specified.
  %
  Common models are provided, but it is also possible to specify custom models
  provided they are stationary.
  %
\end{itemize}
The disadvantages of Gaussian Processes for Machine Learning include:
\begin{itemize}
  \item It is not sparse.
  %
  It uses the whole samples/features information to perform the prediction.
  \item It loses efficiency in high dimensional spaces  namely when the
  number of features exceeds a few dozens.
  %
  It might indeed give poor performance and it loses computational efficiency.
  \item Classification is only a post-processing, meaning that one first needs
  to solve a regression problem by providing the complete scalar float precision
  output $y$ of the experiment one is attempting to model.
  %
\end{itemize}

\skltype{Gaussian Process Regressor}{GaussianProcess|GaussianProcess}

\begin{itemize}
  \item \xmlNode{regr}, \xmlDesc{string, optional field}, is a regression
  function returning an array of outputs of the linear regression functional
  basis.
  %
  The number of observations n\_samples should be greater than the size p of
  this basis.
  %
  Available built-in regression models are `constant,' `linear,' and 
  `quadratic.'
  %
  \default{constant}
  \item \xmlNode{corr}, \xmlDesc{string, optional field}, is a stationary
  autocorrelation function returning the autocorrelation between two points $x$
  and $x'$.
  %
  Default assumes a squared-exponential autocorrelation model.
  %
  Built-in correlation models are `absolute\_exponential,' 
  `squared\_exponential,' `generalized\_exponential,' `cubic,' and `linear.'
  %
  \default{squared\_exponential}
  \item \xmlNode{beta0}, \xmlDesc{float, array-like, optional field}, specifies
  the regression weight vector to perform Ordinary Kriging (OK).
  %
  \default{Universal Kriging}
  \item \xmlNode{storage\_mode}, \xmlDesc{string, optional field}, specifies
  whether the Cholesky decomposition of the correlation matrix should be stored
  in the class (storage\_mode = `full') or not (storage\_mode = `light').
  %
  \default{full}
  \item \verDescriptionA{False}
  \item \xmlNode{theta0}, \xmlDesc{float, array-like, optional field}, is an
  array with shape (n\_features, ) or (1, ).
  %
  This represents the parameters in the autocorrelation model.
  %
  If thetaL and thetaU are also specified, theta0 is considered as the starting
  point for the maximum likelihood estimation of the best set of parameters.
  %
  \default{[1e-1]}
  \item \xmlNode{thetaL}, \xmlDesc{float, array-like, optional field}, is an
  array with shape matching that defined by \xmlNode{theta0}.
  %
  Lower bound on the autocorrelation parameters for maximum likelihood
  estimation.
  %
  \default{None}
  \item \xmlNode{thetaU}, \xmlDesc{float, array-like, optional field}, is an
  array with shape matching that defined by \xmlNode{theta0}.
  %
  Upper bound on the autocorrelation parameters for maximum likelihood
  estimation.
  %
  \default{None}
  \item \xmlNode{normalize}, \xmlDesc{boolean, optional field}, if True, the
  input $X$ and observations $y$ are centered and reduced w.r.t. means and
  standard deviations estimated from the n\_samples observations provided.
  %
  \default{True}
  \item \xmlNode{nugget}, \xmlDesc{float, optional field},specifies a nugget
  effect to allow smooth predictions from noisy data.
  %
  The nugget is added to the diagonal of the assumed training covariance.
  %
  In this way it acts as a Tikhonov regularization in the problem.
  %
  In the special case of the squared exponential correlation function, the
  nugget mathematically represents the variance of the input values.
  %
  \default{10 * MACHINE\_EPSILON}
  \item \xmlNode{optimizer}, \xmlDesc{string, optional field}, specifies the
  optimization algorithm to be used.
  %
  Available optimizers are: 'fmin\_cobyla', 'Welch'.
  %
  \default{fmin\_cobyla}
  \item \xmlNode{random\_start}, \xmlDesc{integer, optional field}, sets the
  number of times the Maximum Likelihood Estimation should be performed from
  a random starting point.
  %
  The first MLE always uses the specified starting point (theta0), the next
  starting points are picked at random according to an exponential distribution
  (log-uniform on [thetaL, thetaU]).
  %
  \default{1}
  \item \xmlNode{random\_state}, \xmlDesc{integer, optional field}, is the seed
  of the internal random number generator.
  %
  \default{random}
  %
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType}]
<Simulation>
  ...
  <Models>
    ...
   <ROM name='aUserDefinedName' subType='SciKitLearn'>
     <Features>var1,var2,var3</Features>
     <Target>result1</Target>
     <SKLtype>linear_model|LinearRegression</SKLtype>
     <fit_intercept>True</fit_intercept>
     <normalize>False</normalize>
   </ROM>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  External  Model   %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{External Model}
\label{subsec:models_externalModel}
As the name suggests, an external model is an entity that is embedded in the
RAVEN code at run time.
%
This object allows the user to create a python module that is going to be
treated as a predefined internal model object.
%
In other words, the \textbf{External Model} is going to be treated by RAVEN as a
normal external Code (e.g. it is going to be called in order to compute an
arbitrary quantity based on arbitrary input).
%

The specifications of an External Model must be defined within the XML block
\xmlNode{ExternalModel}.
%
This XML node needs to contain the attributes:

\vspace{-5mm}
\begin{itemize}
  \itemsep0em
  \item \xmlAttr{name}, \xmlDesc{required string attribute}, user-defined name
  of this External Model.
  %
  \nb As with the other objects, this is the name that can be used to refer to
  this specific entity from other input blocks in the XML.
  \item \xmlAttr{subType}, \xmlDesc{required string attribute}, must be kept
  empty.
  \item \xmlAttr{ModuleToLoad}, \xmlDesc{required string attribute}, file name
  with its absolute or relative path.
  %
  \nb If a relative path is specified, it must be relative with respect to where
  the user runs the code.
  %
\end{itemize}
\vspace{-5mm}

In order to make the RAVEN code aware of the variables the user is going to
manipulate/use in her/his own python Module, the variables need to be specified
in the \xmlNode{ExternalModel} input block.
%
The user needs to input, within this block, only the variables that RAVEN needs
to be aware of (i.e. the variables are going to directly be used by the code)
and not the local variables that the user does not want to, for example, store
in a RAVEN internal object.
%
These variables are specified within consecutive \xmlNode{variable} blocks:
\begin{itemize}
  \item \xmlNode{variable}, \xmlDesc{string, required parameter}.
  %
  In the body of this XML node, the user needs to specify the name of the
  variable.
  %
  This variable needs to match a variable used/defined in the external python
  model.
  %
\end{itemize}

When the external function variables are defined, at run time, RAVEN initializes
them and tracks their values during the simulation.
%
Each variable defined in the \xmlNode{ExternalModel} block is available in the
module (each method implemented) as a python ``self.''
%

In the External Python module, the user can implement all the methods that are
needed for the functionality of the model, but only the following methods, if
present, are called by the framework:
\begin{itemize}
  \item \texttt{\textbf{def \_readMoreXML}}, \xmlDesc{OPTIONAL METHOD}, can be
  implemented by the user if the XML input that belongs to this External Model
  needs to be extended to contain other information.
  %
  The information read needs to be stored in ``self'' in order to be available
  to all the other methods (e.g. if the user needs to add a couple of newer XML
  nodes with information needed by the algorithm implemented in the ``run''
  method).
  \item \texttt{\textbf{def initialize}}, \xmlDesc{OPTIONAL METHOD}, can
  implement all the actions need to be performed at the initialization stage.
  \item \texttt{\textbf{def createNewInput}}, \xmlDesc{OPTIONAL METHOD}, creates
  a new input with the information coming from the RAVEN framework.
  %
  In this function the user can retrieve the information coming from the RAVEN
  framework, during the employment of a calculation flow, and use them to
  construct a new input that is going to be transferred to the ``run'' method.
  \item \texttt{\textbf{def run}}, \xmlDesc{REQUIRED METHOD}, is the actual
  location where the user needs to implement the model action (e.g. resolution
  of a set of equations, etc.).
  %
  This function is going to receive the Input (or Inputs) generated either by
  the External Model ``createNewInput'' method or the internal RAVEN one.
\end{itemize}

In the following sub-sections, all the methods are going to be analyzed in
detail.

\subsubsection{Method: \texttt{def \_readMoreXML}}
\label{subsubsec:externalReadMoreXML}
As already mentioned, the \textbf{readMoreXML} method can be implemented by the
user if the XML input that belongs to this External Model needs to be extended
to contain other information.
%
The information read needs to be stored in ``self'' in order to be available to
all the other methods (e.g. if the user needs to add a couple of newer XML nodes
with information needed by the algorithm implemented in the ``run'' method).
%
If this method is implemented in the \textbf{External Model}, RAVEN is going to
call it when the node \xmlNode{ExternalModel} is found parsing the XML input
file.
%
The method receives from RAVEN an attribute of type ``xml.etree.ElementTree'',
containing all the sub-nodes and attribute of the XML block \xmlNode{ExternalModel}.
%

Example XML:
\begin{lstlisting}[style=XML,morekeywords={subType,ModuleToLoad}]
<Simulation>
  ...
  <Models>
     ...
    <ExternalModel name='AnExtModule' subType='' ModuleToLoad='path_to_external_module'>
       <variable>sigma</variable>
       <variable>rho</variable>
       <variable>outcome</variable>
       <!--
          here we define other XML nodes RAVEN does not read automatically.
          We need to implement, in the external module ``AnExtModule'' the readMoreXML method
        -->
        <newNodeWeNeedToRead>
            whatNeedsToBeRead
        </newNodeWeNeedToRead>
    </ExternalModel>
     ...
  </Models>
  ...
</Simulation>
\end{lstlisting}

Corresponding Python function:
\begin{lstlisting}[language=python]
def _readMoreXML(self,xmlNode):
  # the xmlNode is passed in by RAVEN framework
  # <newNodeWeNeedToRead> is unknown (in the RAVEN framework)
  # we have to read it on our own
  # get the node
  ourNode = xmlNode.find(``newNodeWeNeedToRead'')
  # get the information in the node
  self.ourNewVariable = ourNode.text
  # end function
\end{lstlisting}
\subsubsection{Method: \texttt{def initialize}}
\label{subsubsec:externalInitialize}
The \textbf{initialize} method can be implemented in the \textbf{External Model}
in order to initialize some variables needed by it.
%
For example, it can be used to compute a quantity needed by the ``run'' method
before performing the actual calculation).
%
If this method is implemented in the \textbf{External Model}, RAVEN is going to
call it at the initialization stage of each ``Step'' (see section
\ref{sec:steps}.
%
RAVEN will communicate, thorough a set of method attributes, all the information
that are generally needed to perform a initialization:
\begin{itemize}
  \item runInfo, a dictionary containing information regarding how the
  calculation is set up (e.g. number of processors, etc.).
  %
  It contains the following attributes:
  \begin{itemize}
    \item \texttt{DefaultInputFile} -- default input file to use
    \item \texttt{SimulationFiles} -- the xml input file
    \item \texttt{ScriptDir} -- the location of the pbs script interfaces
    \item \texttt{FrameworkDir} -- the directory where the framework is located
    \item \texttt{WorkingDir} -- the directory where the framework should be
    running
    \item \texttt{TempWorkingDir} -- the temporary directory where a simulation
    step is run
    \item \texttt{NumMPI} -- the number of mpi process by run
    \item \texttt{NumThreads} -- number of threads by run
    \item \texttt{numProcByRun} -- total number of core used by one run (number 
    of threads by number of mpi)
    \item \texttt{batchSize} -- number of contemporaneous runs
    \item \texttt{ParallelCommand} -- the command that should be used to submit
    jobs in parallel (mpi)
    \item \texttt{numNode} -- number of nodes
    \item \texttt{procByNode} -- number of processors by node
    \item \texttt{totalNumCoresUsed} -- total number of cores used by driver
    \item \texttt{queueingSoftware} -- queueing software name
    \item \texttt{stepName} -- the name of the step currently running
    \item \texttt{precommand} -- added to the front of the command that is run
    \item \texttt{postcommand} -- added after the command that is run
    \item \texttt{delSucLogFiles} -- if a simulation (code run) has not failed,
    delete the relative log file (if True)
    \item \texttt{deleteOutExtension} -- if a simulation (code run) has not
    failed, delete the relative output files with the listed extension (comma
    separated list, for example: `e,r,txt')
    \item \texttt{mode} -- running mode, curently the only modes supported are
    pbsdsh and mpi
    \item \textit{expectedTime} -- how long the complete input is expected to
    run
    \item \textit{logfileBuffer} -- logfile buffer size in bytes
  \end{itemize}
  \item inputs, a list of all the inputs that have been specified in the
  ``Step'' using this model.
  %
\end{itemize}
In the following an example is reported:
\begin{lstlisting}[language=python]
def initialize(self,runInfo,inputs):
 # Let's suppose we just need to initialize some variables
  self.sigma = 10.0
  self.rho   = 28.0
  # end function
\end{lstlisting}
\subsubsection{Method: \texttt{def createNewInput}}
\label{subsubsec:externalcreateNewInput}

The \textbf{createNewInput} method can be implemented by the user to create a
new input with the information coming from the RAVEN framework.
%
In this function, the user can retrieve the information coming from the RAVEN
framework, during the employment of a calculation flow, and use them to
construct a new input that is going to be transferred to the ``run'' method.
%
The new input created needs to be returned to RAVEN (i.e. ``return NewInput'').
%
RAVEN communicates, thorough a set of method attributes, all the information
that are generally needed to create a new input:
%myInput,samplerType,**Kwargs
\begin{itemize}
  \item \texttt{inputs}, \xmlDesc{python list}, a list of all the inputs that
  have been defined in the ``Step'' using this model.
  \item \texttt{samplerType}, \xmlDesc{string}, the type of Sampler, if a
  sampling strategy is employed; will be None otherwise.
  \item \texttt{Kwargs}, \xmlDesc{dictionary}, a dictionary containing several
  pieces of information (that can change based on the ``Step'' type).
  %
  If a sampling strategy is employed, this dictionary contains another
  dictionary identified by the keyword ``SampledVars'', in which the variables
  perturbed by the sampler are reported.
\end{itemize}
\nb If the ``Step'' that is using this Model has as input(s) an object of main
class type ``Datas'' (see section \ref{datas}), the internal ``createNewInput''
method is going to convert it in a dictionary of values.
%

Here we present an example:
\begin{lstlisting}[language=python]
def createNewInput(self,inputs,samplerType,**Kwargs):
  # in here the actual createNewInput of the
  # model is implemented
  if samplerType == ``MonteCarlo'':
    avariable = inputs[``something'']*inputs[``something2'']
  else:
    avariable = inputs[``something'']/inputs[``something2'']
  return avariable*Kwargs[``SampledVars''][``aSampledVar'']
\end{lstlisting}

\subsubsection{Method: \texttt{def run}}
\label{subsubsec:externalRun}
As stated previously, the only method that \emph{must} be present in an
External Module is the \textbf{run} function.
%
In this function, the user needs to implement the algorithm that RAVEN will
execute.
%
The \texttt{\textbf{run}} method is generally called after having inquired the
``createNewInput'' method (either the internal or the user-implemented one).
%
The only attribute this method is going to receive is a Python list of inputs
(the inputs coming from the \texttt{createNewInput} method).
%
If the user wants RAVEN to collect the results of this method, the outcomes of
interest need to be stored in ``self.''
%
\nb RAVEN is trying to collect the values of the variables listed only in the
\xmlNode{ExternalModel} XML block.
%

In the following an example is reported:
\begin{lstlisting}[language=python]
def run(self,Input):
  # in here the actual run of the
  # model is implemented
  input = Input[0]
  self.outcome = self.sigma*self.rho*input[``whatEver'']
\end{lstlisting}

%\subsection{Projector}
%\label{sec:models_projector}
%
%Description

%Summary

%Example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%         PostProcessor         %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PostProcessor}
\label{sec:models_postProcessor}
A Post-Processor (PP) can be considered as an action performed on a set of data
or other type of objects.
%
Most of the post-processors contained in RAVEN, employ a mathematical operation
on the data given as ``input''.
%
RAVEN supports several different types of PPs.

Currently, the following PPs are available in RAVEN:
\begin{itemize}
  \itemsep0em
  \item \textbf{BasicStatistics}
  \item \textbf{ComparisonStatistics}
  \item \textbf{SafestPoint}
  \item \textbf{LimitSurface}
  \item \textbf{External}
  \item \textbf{TopologicalDecomposition}
  \item \textbf{PrintCSV}
  \item \textbf{LoadCsvIntoInternalObject}
\end{itemize}

The specifications of these PPs must be defined within the XML block
\xmlNode{PostProcessor}.
%
This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
  \itemsep0em
  \item \xmlAttr{name}, \xmlDesc{required string attribute}, user-defined
  identifier of this post-processor.
  %
  \nb As with other objects, this is the name that can be used to refer to this
  specific entity from other input XML blocks.
  \item \xmlAttr{subType}, \xmlDesc{required string attribute}, defines which of
  the post-processors needs to be used, choosing among the previously reported 
  types.
  %
  This choice conditions the subsequent required and/or optional
  \xmlNode{PostProcessor} sub nodes.
  %
\end{itemize}
\vspace{-5mm}

As already mentioned, all the types and meaning of the remaining sub-nodes
depend on the post-processor type specified in the attribute \xmlAttr{subType}.
%
In the following sections the specifications of each type are reported.

%%%%% PP BasicStatistics %%%%%%%
\paragraph{BasicStatistics}
\label{BasicStatistics}
The \textbf{BasicStatistics} post-processor is the container of the algorithms
to compute many of the most important statistical quantities.
%
\ppType{BasicStatistics post-processor}{BasicStatistics}
\begin{itemize}
  \item \xmlNode{what}, \xmlDesc{comma separated string, required field},
  %
  List of quantities to be computed.
  %
  Currently the quantities available are:
  \begin{itemize}
    \item \textbf{covariance matrix} -- covariance matrix
    \item \textbf{NormalizedSensitivity} -- matrix of normalized sensitivity
    coefficients
    \item \textbf{sensitivity} -- matrix of sensitivity coefficients
    \item \textbf{pearson} -- matrix of sensitivity coefficients
    \item \textbf{expectedValue} -- expected value or mean
    \item \textbf{sigma} -- standard deviation
    \item \textbf{variationCoefficient} -- coefficient of variation
    (sigma/expected value)
    \item \textbf{variance} --  variance
    \item \textbf{skewness} -- skewness
    \item \textbf{kurtois} -- kurtois
    \item \textbf{median} -- median
    \item \textbf{percentile} -- 95 percentile
    %
  \end{itemize}
  If all the quantities need to be computed, the user can input in the body of
  \xmlNode{what} the string ``all.''
  %
  \item \xmlNode{parameters}, \xmlDesc{comma separated string, required field},
  lists the parameters on which the previous operations need to be applied
  (e.g., massFlow, Temperature)
  \item \xmlNode{methodsToRun}, \xmlDesc{comma separated string, optional
  field}, specifies the method names of an external Function that need to be run
  before computing any of the predefined quantities.
  %
  If this XML node is specified, the \xmlNode{Function} node must be present.
  %
  \default{None}
% Assembler Objects
  \item \textbf{Assembler Objects} These objects are either required or optional
  depending on the functionality of the Adaptive Sampler.
  %
  The objects must be listed with a rigorous syntax that, except for the xml
  node tag, is common among all the objects.
  %
  Each of these nodes must contain 2 attributes that are used to map those
  within the simulation framework:
  \begin{itemize}
    \item \xmlAttr{class}, \xmlDesc{required string attribute}, it is the main
    ``class'' the listed object is from;
    \item \xmlAttr{type}, \xmlDesc{required string attribute}, it is the object
    identifier or sub-type.
    %
  \end{itemize}
  The \textbf{BasicStatistics} post-processor approach optionally accepts the
  following object type:
  \begin{itemize}
    \item \xmlNode{Function}, \xmlDesc{string, required field}, The body of
    this xml block needs to contain the name of an External Function defined
    within the \xmlNode{Functions} main block (see section \ref{sec:functions}).
    %
    This object needs to contain the methods listed in the node
    \xmlNode{methodsToRun}.
  \end{itemize}
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType,debug}]
<Simulation>
  ...
  <Models>
    ...
    <PostProcessor name='aUserDefinedName' subType='BasicStatistics'  debug='true'>
      <!-- Here you can specify what type of figure of merit
          you need to compute: expectedValue, sigma, variance, kurtosis, pearson, covariance, etc.
       -->
      <what>expectedValue</what>
      <parameters>x01,x02</parameters>
      <methodsToRun>failureProbability</methodsToRun>
    </PostProcessor>
    ...
  <Models>
  ...
<Simulation>
\end{lstlisting}

%%%%% PP ComparisonStatistics %%%%%%%
\paragraph{ComparisonStatistics}
\label{ComparisonStatistics}
\TODO{To be finalized}

%%%%% PP SafestPoint %%%%%%%
\paragraph{SafestPoint}
\label{SafestPoint}
The \textbf{SafestPoint} post-processor provides the coordinates of the farthest
point from the limit surface that is given as an input.
%
The safest point coordinates are expected values of the coordinates of the
farthest points from the limit surface in the space of the ``controllable''
variables based on the probability distributions of the ``non-controllable''
variables.

The term ``controllable'' identifies those variables that are under control
during the system operation, while the ``non-controllable'' variables are
stochastic parameters affecting the system behaviour randomly.

The ``SafestPoint'' post-processor requires the set of points belonging to the
limit surface, which must be given as an input.
%
The probability distributions as ``Assembler Objects'' are required in the
``Distribution'' section for both ``controllable'' and ``non-controllable''
variables.

The sampling method used by the ``SafestPoint'' is a ``value'' or ``CDF'' grid.
%
At present only the ``equal'' grid type is available.

\ppType{Safest Point}{SafestPoint}

\begin{itemize}
  \item \xmlNode{Distribution}, \xmlDesc{Required}, represents the probability
  distributions of the ``controllable'' and ''non-controllable'' variables.
  %
  These are \textbf{Assembler Objects}, each of these nodes must contain 2
  attributes that are used to map those within the simulation framework:
	\begin{itemize}
    \item \xmlAttr{class}, \xmlDesc{required string attribute}, is the main
    ``class'' the listed object is from.
 		\item \xmlAttr{type}, \xmlDesc{required string attribute}, is the object
    identifier or sub-type.
	\end{itemize}
	\item \xmlNode{controllable} lists the controllable variables.
  %
  Each variable is associated with its name and the two items below:
	\begin{itemize}
		\item \xmlNode{distribution} names the probability distribution associated
    with the controllable variable.
    %
		\item \xmlNode{grid} specifies the \xmlAttr{type}, \xmlAttr{steps}, and
    tolerance of the sampling grid.
    %
	\end{itemize}
	\item \xmlNode{non-controllable} lists the non-controllable variables.
  %
  Each variable is associated with its name and the two items below:
	\begin{itemize}
		\item \xmlNode{distribution} names the probability distribution associated
    with the non-controllable variable.
    %
		\item \xmlNode{grid} specifies the \xmlAttr{type}, \xmlAttr{steps}, and
    tolerance of the sampling grid.
    %
		\end{itemize}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType,class,type,steps}]
<Simulation>
  ...
    <Models>
    ...
    <PostProcessor name='SP' subType='SafestPoint'>
      <Distribution  class='Distributions'  type='Normal'>x1_dst</Distribution>
      <Distribution  class='Distributions'  type='Normal'>x2_dst</Distribution>
      <Distribution  class='Distributions'  type='Normal'>gammay_dst</Distribution>
      <controllable>
        <variable name='x1'>
          <distribution>x1_dst</distribution>
          <grid type='value' steps='20'>1</grid>
        </variable>
        <variable name='x2'>
          <distribution>x2_dst</distribution>
          <grid type='value' steps='20'>1</grid>
        </variable>
      </controllable>
      <non-controllable>
        <variable name='gammay'>
          <distribution>gammay_dst</distribution>
          <grid type='value' steps='20'>2</grid>
        </variable>
      </non-controllable>
    </PostProcessor>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
%%%%% PP LimitSurface %%%%%%%
\paragraph{LimitSurface}
\label{LimitSurface}
The \textbf{LimitSurface} post-processor is aimed to identify the transition
zones that determine a change in the status of the system (Limit Surface).

\ppType{LimitSurface}{LimitSurface}

\begin{itemize}
  \item \xmlNode{parameters}, \xmlDesc{comma separated string, required field},
  lists the parameters that define the uncertain domain and from which the LS
  needs to be computed.
  \item \xmlNode{tolerance}, \xmlDesc{float, optional field}, sets the absolute
  value of the convergence tolerance.
  %
  This value defines the coarseness of the evaluation grid.
  %
  \textit{Default= 1.e-4}
  % Assembler Objects
  \item \textbf{Assembler Objects} These objects are either required or optional
  depending on the functionality of the Adaptive Sampler.
  %
  The objects must be listed with a rigorous syntax that, except for the xml
  node tag, is common among all the objects.
  %
  Each of these nodes must contain 2 attributes that are used to map those
  within the simulation framework:
   \begin{itemize}
    \item \xmlAttr{class}, \xmlDesc{required string attribute}, is the main
    ``class'' of the listed object.
    %
    For example, it can be ``Models,'' ``Functions,'' etc.
    \item \xmlAttr{type}, \xmlDesc{required string attribute}, is the object
    identifier or sub-type.
    %
    For example, it can be ``ROM,'' ``External,'' etc.
    %
  \end{itemize}
  The \textbf{LimitSurface} post-processor requires or optionally accepts the
  following objects' types:
   \begin{itemize}
    \item \xmlNode{ROM}, \xmlDesc{string, optional field}, body of this xml
    node must contain the name of a ROM defined in the \xmlNode{Models} block
    (see section \ref{subsec:models_ROM}).
    \item \xmlNode{Function}, \xmlDesc{string, required field}, the body of
    this xml block needs to contain the name of an External Function defined
    within the \xmlNode{Functions} main block (see section \ref{sec:functions}).
    %
    This object represents the boolean function that defines the transition
    boundaries.
    %
    This function must implement a method called
    \textit{\_\_residuumSign(self)}, that returns either -1 or 1, depending on
    the system conditions (see section \ref{sec:functions}).
    %
    \end{itemize}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={name,subType,debug,class,type}]
<Simulation>
 ...
 <Models>
  ...
    <PostProcessor name="computeLimitSurface" subType='LimitSurface' debug='True'>
      <what>all</what>
      <parameters>x0,y0</parameters>
      <parameters>x0,y0</parameters>
      <ROM class='Models' type='ROM'>Acc</ROM>
      <!-- Here, you can add a ROM defined in Models block. 
           If it is not Present, a nearest neighbor algorithm
           will be used.
       -->
      <Function class='Functions' type='External'>
        goalFunctionForLimitSurface
      </Function>
    </PostProcessor>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
%%%%% PP External %%%%%%%
\paragraph{External}
\label{External}
The \textbf{External} post-processor will execute an arbitrary python function
defined externally using the \textit{Functions} interface (see 
Section~\ref{sec:functions} for more details).
%

\ppType{External}{External}

\begin{itemize}
  \item \xmlNode{method}, \xmlAttr{comma separated string, required field},
  lists the method names of an external Function that will be computed.
  \item \xmlNode{Assembler}, \xmlAttr{xml node, required field}, contains a 
  ``list'' of Functions where the \textit{methods} listed above are defined.
  %
  The objects must be listed with a rigorous syntax that, except for the xml
  node tag, is common among all the objects.
  %
  Each of these sub-nodes must contain 2 attributes that are used to map them
  within the simulation framework:

   \begin{itemize}
     \item \xmlAttr{class}, \textit{required string attribute}, is the main
     ``class'' the listed object is from, the only acceptable class for
     this post-processor is \textit{Functions};
     \item \xmlAttr{type}, \textit{required string attribute}, is the object
     identifier or sub-type, the only acceptable type for this post-processor is
     \textit{External}.
  \end{itemize}
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={subType,debug,name,class,type}]
<Simulation>
  ...
  <Models>
    ...
    <PostProcessor name="externalPP" subType='External' debug='True'>
      <method>Delta,Sum</method>
      <Assembler>
        <Function class='Functions' type='External'>operators</Function> 
          <!--Here, you can add a Function defined in the
              Functions block. This should be present or
              else RAVEN will not know where to find the
              defined methods.
            -->
      </Assembler>
    </PostProcessor>
    ...
  </Models>
  ...
</Simulation>
\end{lstlisting}
%%%%% PP TopologicalDecomposition %%%%%%%
\paragraph{TopologicalDecomposition}
\label{TopologicalDecomposition}
The \textbf{TopologicalDecomposition} post-processor will compute an 
approximated hierarchical Morse-Smale complex and perform linear regression on
each component. 
%

In order to use the \textbf{TopologicalDecomposition} post-processor, the user
needs to set the attribute \xmlAttr{subType}: 
\xmlNode{PostProcessor \xmlAttr{subType}=\xmlString{TopologicalDecomposition}}. 
The following is a list of acceptable sub-nodes:
\begin{itemize}
  \item \xmlNode{graph} \xmlDesc{, string, optional field}, specifies the type
  of neighborhood graph used in the algorithm, available options are:
  \begin{itemize}
    \item Beta Skeleton
    \item Relaxed Beta Skeleton
    \item Approximate KNN
    \item Delaunay \textit{(disabled)}
  \end{itemize}
  \default{Beta Skeleton}
  \item \xmlNode{gradient}, \xmlDesc{string, optional field}, specifies the 
  method used for estimating the gradient, available options are:
  \begin{itemize}
    \item Steepest
    \item MaxFlow \textit{(disabled)}
  \end{itemize}
  \default{Steepest}
  \item \xmlNode{beta}, \xmlDesc{float, optional field}, is only used when the
  \texttt{graph} is set to \textit{Beta Skeleton} or \textit{Relaxed Beta
  Skeleton}.
  \default{1.0}
  \item \xmlNode{knn}, \xmlDesc{integer, optional field}, is the number of
  neighbors when using the \textit{Approximate KNN} for the \texttt{graph}
  sub-node and used to speed up the computation of other graphs by using the
  approximate knn graph as a starting point for pruning. -1 means use a fully
  connected graph.
  \default{-1}
  \item \xmlNode{persistence}, \xmlDesc{float, optional field}, specifies the
  amount of noise reduction to apply before returning labels. 
  \default{0}
  \item \xmlNode{parameters}, \xmlDesc{comma separated string, required field},
  lists the parameters defining the input space.
  \item \xmlNode{response}, \xmlDesc{string, required field}, is a single
  variable name defining the scalar output space.
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML,morekeywords={subType}]
<Simulation>
  ...
  <Models>
    ...
    <PostProcessor name="***" subType='TopologicalDecomposition'>
      <graph>Beta Skeleton</graph>
      <gradient>Steepest</gradient>
      <beta>1</beta>
      <knn>8</knn>
      <parameters>X,Y</parameters>
      <response>Z</response>
    </PostProcessor>
    ...
  <Models>
  ...
<Simulation>
\end{lstlisting}
%%%%% PP External %%%%%%%
\paragraph{External.}
\label{External}
The \textbf{External} post-processor will execute an arbitrary python function
defined externally using the \textit{Functions} interface (see 
Section~\ref{sec:functions} for more details).
\\In order to use the \textit{External} post-processor, the user needs to set 
the \textit{subType} attribute to be ``External'': 
\textit{$<$PostProcessor subType=``External''$>$}. 
The following sub-nodes need to be specified:
\begin{itemize}
\item $<method>$ \textbf{\textit{, comma separated string, required field.}}.
List of method names of an external Function that will be computed.
\item $<Assembler>$\textbf{\textit{, xml node, required field.}} This xml node
contains a ``list'' of Functions where the \textit{methods} listed above are
defined. The objects must be listed with a rigorous syntax that, except for the
xml node tag, is common among all the objects. 
Each of these sub-nodes must contain 2 attributes that are used to map them
within the simulation framework:
   \begin{itemize}
     \item \textbf{class}, \textit{required string attribute}, it is the main
           ``class'' the listed object is from, the only acceptable class for
           this post-processor is \textit{Functions};
     \item \textbf{type},  \textit{required string attribute}, it is the object
           identifier or sub-type, the only acceptable type for this 
           post-processor is \textit{External}.
    \end{itemize}
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML]
----------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <PostProcessor name="externalPP" subType='External' debug='True'>
      <method>Delta,Sum</method>
      <Assembler>
        <Function class='Functions' type='External'>operators</Function> 
          <!--Here, you can add a Function defined in the
              Functions block. This should be present or
              else RAVEN will not know where to find the
              defined methods.
            -->
      </Assembler>
    </PostProcessor>
    ...
  </Models>
  ...
</Simulation>
----------------------------------------------
\end{lstlisting}
%%%%% PP PrintCSV %%%%%%%
\paragraph{PrintCSV}
\label{PrintCSV}
TO BE MOVED TO STEP ``IOSTEP''
%%%%% PP LoadCsvIntoInternalObject %%%%%%%
\paragraph{LoadCsvIntoInternalObject}
\label{LoadCsvIntoInternalObject}
TO BE MOVED TO STEP ``IOSTEP''
